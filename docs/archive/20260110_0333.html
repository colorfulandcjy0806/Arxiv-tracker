<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-10 03:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260110_0333</div>
    <div class="row"><div class="card">
<div class="title">Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</div>
<div class="meta-line">Authors: William Rudman, Michal Golovanevsky, Dana Arad, Yonatan Belinkov, Ritambhara Singh, Carsten Eickhoff, Kyle Mahowald</div>
<div class="meta-line">First: 2026-01-08T18:23:03+00:00 · Latest: 2026-01-08T18:23:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05201v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05201v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中提示诱发幻觉的机制</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）功能强大，但经常倾向于根据文本提示而非视觉证据进行幻觉。我们在一个受控的对象计数设置中研究了这种失败模式，其中提示会夸大图像中的对象数量（例如，要求模型描述四朵水仙花，而实际上只有三朵）。在对象数量较低时，模型通常会纠正这种高估，但随着对象数量的增加，它们越来越倾向于遵循提示，无视差异。通过对三种VLMs的机制分析，我们发现一小组注意力头的消除可以显著减少提示诱发幻觉（PIH），至少降低40%且无需额外训练。在不同模型中，PIH头以特定方式介导提示复制。我们描述了这些差异，并表明PIH消除增加了向视觉证据的纠正。我们的研究结果揭示了驱动提示诱发幻觉的内部机制，揭示了这些行为在不同模型中的特定差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how large vision-language models (VLMs) hallucinate based on textual prompts rather than visual evidence. By manipulating object counts in images, the researchers found that as the number of objects increases, models increasingly conform to the prompt. Through analysis of three VLMs, they identified specific attention heads that, when removed, significantly reduced prompt-induced hallucinations by at least 40% without additional training. The findings highlight model-specific differences in how these behaviors are implemented and suggest that targeting these heads can improve the models&#x27; reliance on visual evidence.</div>
<div class="mono" style="margin-top:8px">研究探讨了大型视觉-语言模型（VLMs）如何基于文本提示而非视觉证据产生幻觉。通过改变图像中的物体数量，研究人员发现，随着物体数量的增加，模型越来越倾向于遵循提示。通过消除特定的注意力头，可以在不同模型中至少减少40%的提示诱导幻觉，而无需额外训练。研究结果揭示了这些行为在不同模型中的特定机制，并表明针对这些头可以减轻幻觉。</div>
</details>
</div>
<div class="card">
<div class="title">CoV: Chain-of-View Prompting for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang</div>
<div class="meta-line">First: 2026-01-08T17:59:42+00:00 · Latest: 2026-01-08T17:59:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoV：空间推理的链式视角提示</div>
<div class="mono" style="margin-top:8px">在3D环境中的嵌入式问题回答（EQA）通常需要收集分布在多个视角且部分被遮挡的上下文。然而，大多数最新的视觉-语言模型（VLMs）仅限于固定且有限的输入视角集，这限制了它们在推理时获取与问题相关上下文的能力，并阻碍了复杂的空间推理。我们提出了一种名为Chain-of-View（CoV）的提示方法，这是一种无需训练、在测试时进行推理的框架，通过从粗到细的探索过程将VLM转换为积极的视角推理者。CoV首先使用视图选择代理筛选冗余帧并识别与问题对齐的锚视图，然后通过交替进行迭代推理和离散相机动作进行细粒度的视图调整，从底层3D场景表示中获取新观察，直到收集到足够多的上下文或达到步骤预算。
我们在OpenEQA上对CoV进行了评估，跨四个主流VLMs获得了平均+11.56%的LLM-Match改进，最大增益为Qwen3-VL-Flash上的+13.62%。CoV还表现出测试时的扩展性：增加最小动作预算可额外获得平均+2.51%的改进，峰值为Gemini-2.5-Flash上的+3.73%。在ScanQA和SQA3D上，CoV表现出强大的性能（例如，ScanQA上的116 CIDEr / 31.9 EM@1和SQA3D上的51.1 EM@1）。总体而言，这些结果表明，与问题对齐的视图选择结合开放视图搜索是提高3D EQA中空间推理能力的有效、模型无关的策略，无需额外训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of vision-language models in embodied question answering (EQA) by proposing Chain-of-View (CoV) prompting, which enhances spatial reasoning by dynamically selecting and adjusting viewpoints. CoV uses a View Selection agent to filter redundant views and identify relevant anchor views, followed by iterative reasoning and camera actions to gather necessary context. The method significantly improves performance across various VLMs, achieving an average improvement of 11.56% in LLM-Match, with up to 13.62% on Qwen3-VL-Flash. Additionally, increasing the action budget further enhances performance, with an average improvement of 2.51%, peaking at 3.73% on Gemini-2.5-Flash. CoV also shows strong performance on ScanQA and SQA3D, indicating its effectiveness in improving spatial reasoning in 3D EQA without additional training.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决固定输入视图限制，提升3D环境中的体感问答（EQA）能力。提出的Chain-of-View（CoV）提示方法采用粗到细的探索过程，使用视图选择代理筛选冗余帧并识别相关锚视图，随后通过迭代推理和相机动作进行精细视图调整。CoV在多种视觉语言模型中显著提升了性能，平均提高了+11.56%的LLM-Match得分，最高达到+13.62%的Qwen3-VL-Flash。此外，它还展示了测试时的可扩展性，随着最小动作预算的增加，性能进一步提升。</div>
</details>
</div>
<div class="card">
<div class="title">MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging</div>
<div class="meta-line">Authors: Siyi Chen, Kai Wang, Weicong Pang, Ruiming Yang, Ziru Chen, Renjun Gao, Alexis Kai Hon Lau, Dasa Gu, Chenchen Zhang, Cheng Li</div>
<div class="meta-line">First: 2025-09-23T06:23:56+00:00 · Latest: 2026-01-08T17:56:05+00:00</div>
<div class="meta-line">Comments: The project is available at https://charlescsyyy.github.io/MVT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18693v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.18693v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://charlescsyyy.github.io/MVT">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MVT：基于掩码的视觉-语言模型在分类学对齐的土地覆盖标签化中的应用</div>
<div class="mono" style="margin-top:8px">遥感中的土地覆盖理解越来越多地需要能够在不同数据集之间泛化的同时保持空间精确性和可解释性的类无差别系统。我们研究了在领域转移下的几何优先发现与解释设置，其中候选区域以类无差别的方式划定，监督避免使用类名的词汇标识符。除了开放集识别和开放世界学习，我们专注于将类无差别掩码证据与分类学导向的场景解释相结合，而不是未知拒绝或持续类扩展。我们提出了MVT，一个三阶段框架，(i) 使用SAM2结合领域适应提取边界忠实的区域掩码，(ii) 通过双步骤LoRA微调多模态LLM进行掩码导向的语义标签和场景描述生成，(iii) 使用LLM作为裁判评分，通过分层专家评分校准输出评估。在跨数据集分割迁移（在OpenEarthMap上训练，在LoveDA上评估）中，领域适应的SAM2提高了掩码质量；同时，双步骤多模态LLM微调产生了更准确的分类学对齐标签和更具有信息性的掩码导向场景描述。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop class-agnostic systems for land-cover understanding in remote sensing that can generalize across datasets while maintaining spatial precision and interpretability. The method involves a three-stage framework: (i) extracting boundary-faithful region masks using SAM2 with domain adaptation, (ii) performing mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluating outputs with LLM-as-judge scoring calibrated by expert ratings. The key findings show that domain-adapted SAM2 improves mask quality, and dual-step MLLM fine-tuning leads to more accurate taxonomy-aligned tags and more informative scene descriptions on cross-dataset segmentation transfer.</div>
<div class="mono" style="margin-top:8px">研究旨在开发能够在不同数据集间泛化的遥感系统，同时保持空间精度和可解释性。方法包括三个阶段：(i) 使用SAM2进行领域适应以提取边界忠实的区域掩码，(ii) 通过双重步骤的LoRA微调多模态LLM进行掩码导向的语义标签和场景描述生成，(iii) 使用LLM作为评判者进行输出评估，并通过分层专家评分进行校准。研究显示，领域适应的SAM2提高了掩码质量，而双重步骤的LLM微调则产生了更准确的分类学对齐标签和更具信息量的场景描述。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</div>
<div class="meta-line">Authors: Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu</div>
<div class="meta-line">First: 2026-01-08T17:49:13+00:00 · Latest: 2026-01-08T17:49:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05159v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言内省：通过可解释的双因归向引导减轻MLLM中的过度自信幻觉</div>
<div class="mono" style="margin-top:8px">物体幻觉严重削弱了多模态大型语言模型的可靠性，通常源于认知内省的基本失败，模型盲目信任语言先验而非具体的视觉证据。现有缓解措施仍有限：对比解码方法表面化地操作而未能纠正内部语义错位，而当前的潜在引导方法依赖于静态向量，缺乏实例特定的精确性。我们引入了视觉-语言内省（VLI），这是一种无需训练的推理框架，模拟了元认知的自我纠正过程。VLI 首先进行属性内省，通过概率冲突检测诊断幻觉风险并定位因果视觉锚点。然后使用可解释的双因归向引导主动调节推理过程，动态隔离视觉证据与背景噪声，并通过自适应校准消除盲目的自信。VLI 在先进模型上实现了最先进的性能，在MMHal-Bench 上将物体幻觉率降低了12.67%，在POPE 上提高了5.8%的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs</div>
<div class="meta-line">Authors: Carlos Plou, Cesar Borja, Ruben Martinez-Cantin, Ana C. Murillo</div>
<div class="meta-line">First: 2025-03-25T17:17:19+00:00 · Latest: 2026-01-08T17:17:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.19850v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.19850v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM&#x27;s answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FALCONEye：使用多模态LLM在一小时视频中查找答案并定位内容</div>
<div class="mono" style="margin-top:8px">在小时长视频中查找信息对顶级视觉语言模型（VLMs）来说也是一个具有挑战性的任务，因为编码视觉内容会迅速超出可用的上下文窗口。为了解决这一挑战，我们提出了FALCONEye，这是一种基于训练无损、模型无关元架构的新型视频代理，该架构由VLM和大型语言模型（LLM）组成。FALCONEye 使用由VLM答案校准置信度引导的基于探索的搜索算法来回答开放式问题。我们还引入了FALCON-Bench基准测试，将问答问题扩展到视频答案搜索，要求模型返回一小时长视频中开放式问题的答案及其支持的时间窗口。仅使用一个7B VLM和一个轻量级LLM，FALCONEye 在FALCON-Bench中得分超过了所有开源7B VLMs和可比代理。此外，FALCONEye 还在MLVU基准测试中展示了其泛化能力，处理较短视频和不同任务时，超越了GPT-4o，在单一细节任务上将推理成本降低了大约一个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FALCONEye is a video agent that uses a VLM and an LLM to answer open-ended questions in one-hour-long videos. It employs an exploration-based search algorithm guided by the VLM&#x27;s calibrated confidence. FALCONEye outperforms all open-source 7B VLMs and comparable agents in the FALCON-Bench and shows strong generalization in the MLVU benchmark, surpassing GPT-4o on single-detail tasks while reducing inference cost significantly.</div>
<div class="mono" style="margin-top:8px">FALCONEye 是一种新型视频代理，利用 VLM 和 LLM 来回答一小时长视频中的开放性问题。它采用了一种由 VLM 的校准置信度引导的探索式搜索算法。FALCONEye 在 FALCON-Bench 中优于所有开源的 7B VLM 及其同类代理，并在 MLVU 基准测试中展示了强大的泛化能力，超越了 GPT-4o 在单一细节任务上的表现，同时大幅降低了推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding</div>
<div class="meta-line">Authors: Ignacio de Rodrigo, Alvaro J. Lopez-Lopez, Jaime Boal</div>
<div class="meta-line">First: 2026-01-08T17:15:15+00:00 · Latest: 2026-01-08T17:15:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05125v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05125v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VERSE：视觉嵌入缩减与空间探索。基于聚类指导的视觉丰富文档理解训练数据增强方法</div>
<div class="mono" style="margin-top:8px">本文介绍了VERSE，一种用于分析和改进应用于视觉丰富文档理解的视觉语言模型的方法，通过探索其视觉嵌入空间。VERSE使潜在表示的可视化成为可能，支持模型可行性的评估。它还使识别问题区域并指导生成合成数据以在这些聚类中增强性能成为可能。我们通过在合成MERIT数据集上进行训练并在其现实世界对应物MERIT Secret上进行评估来验证该方法。结果表明，VERSE有助于揭示与错误频发聚类相关的视觉特征，并且使用包含这些特征的样本重新训练显著提高了F1性能，而不会损害泛化能力。此外，我们证明了使用VERSE优化的本地模型（如Donut和Idefics2）在性能上可以与甚至超越像GPT-4和Pixtral这样的SaaS解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VERSE is a methodology that explores the visual embedding space of Vision-Language Models to improve their performance in Visually-rich Document Understanding. It visualizes latent representations to identify problematic regions and guides the generation of synthetic data to enhance model performance. Experiments show that VERSE helps uncover visual features associated with error-prone clusters and retraining with these features significantly improves F1 performance without degrading generalization.</div>
<div class="mono" style="margin-top:8px">VERSE 是一种方法，通过探索 Vision-Language 模型的视觉嵌入空间来提高其在视觉丰富文档理解中的性能。它可视化潜在表示以识别问题区域，并指导生成合成数据以增强模型性能。实验表明，VERSE 帮助发现了与错误多发簇相关的视觉特征，并通过这些特征重新训练显著提高了 F1 表现，同时没有损害泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">$π_0$: A Vision-Language-Action Flow Model for General Robot Control</div>
<div class="meta-line">Authors: Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky</div>
<div class="meta-line">Venue: RSS 2025</div>
<div class="meta-line">First: 2024-10-31T17:22:30+00:00 · Latest: 2026-01-08T17:01:05+00:00</div>
<div class="meta-line">Comments: See project website for videos: https://physicalintelligence.company/blog/pi0 Published in RSS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.24164v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.24164v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$π_0$: 一种视觉-语言-行动流模型用于通用机器人控制</div>
<div class="mono" style="margin-top:8px">机器人学习在解锁灵活、通用和灵巧的机器人系统潜力以及解决人工智能领域的一些最深层次问题方面具有巨大的前景。然而，将机器人学习提升到有效现实系统所需的通用性水平面临数据、泛化和鲁棒性方面的重大障碍。在本文中，我们讨论了通用机器人策略（即机器人基础模型）如何应对这些挑战，以及如何设计有效的通用机器人策略以应对复杂和高度灵巧的任务。我们提出了一种基于预训练视觉-语言模型（VLM）的新颖流匹配架构，以继承互联网规模的语义知识。然后，我们讨论了如何使用多种灵巧机器人平台的大规模和多样化数据集对该模型进行训练，包括单臂机器人、双臂机器人和移动操作器。我们从预训练后执行任务的能力、遵循人类和高级VLM策略的语言指令以及通过微调获取新技能等方面评估了该模型。我们的结果涵盖了各种任务，如衣物折叠、桌面清洁和组装盒子。</div>
</details>
</div>
<div class="card">
<div class="title">POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering</div>
<div class="meta-line">Authors: Yichen Xu, Liangyu Chen, Liang Zhang, Jianzhe Ma, Wenxuan Wang, Qin Jin</div>
<div class="meta-line">First: 2025-07-16T06:09:02+00:00 · Latest: 2026-01-08T17:00:25+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.11939v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.11939v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POLYCHARTQA：使用多语言图表问答评估大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">图表是数据交流的普遍采用媒介，但现有的图表理解基准主要以英语为中心，限制了其对全球受众的适用性和相关性。为解决这一局限，我们引入了PolyChartQA，这是首个大规模多语言图表问答基准，包含22,606张图表和26,151个问答对，覆盖10种不同语言。PolyChartQA通过可扩展的流水线构建，利用数据翻译和代码重用实现高效的多语言图表生成，由基于LLM的翻译和严格的质量控制支持。我们系统地使用PolyChartQA对最先进的LVLM进行多语言图表理解评估，揭示了英语与其他语言之间，尤其是低资源语言之间存在显著的性能差距。此外，我们还引入了PolyChartQA-Train多语言图表问答训练集，基于此对LVLM进行微调可显著提高多语言图表理解能力，适用于各种模型规模和架构。我们的基准为开发能够跨多种语言环境理解图表的全球包容性视觉语言模型奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PolyChartQA is a new multilingual benchmark for chart question answering, addressing the lack of diversity in existing benchmarks. It includes 22,606 charts and 26,151 QA pairs in 10 languages and evaluates large vision-language models, revealing a significant performance gap between English and other languages. Fine-tuning on PolyChartQA-Train improves multilingual chart understanding across different model sizes and architectures.</div>
<div class="mono" style="margin-top:8px">PolyChartQA 是一个新的多语言图表问答基准，旨在解决现有基准的多样性不足问题。它包含 22,606 个图表和 26,151 个问答对，覆盖 10 种语言，并评估了大型视觉语言模型，结果显示英语和其他语言之间的性能差距显著。通过 PolyChartQA-Train 进行微调可以提高不同模型大小和架构下的多语言图表理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts</div>
<div class="meta-line">Authors: Wenhao Zeng, Xuteng Zhang, Yuling Shi, Chao Hu, Yuting Chen, Beijun Shen, Xiaodong Gu</div>
<div class="meta-line">First: 2026-01-08T16:58:07+00:00 · Latest: 2026-01-08T16:58:07+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/Zengwh02/GlimpRouter</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05110v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05110v1">PDF</a> · <a href="https://github.com/Zengwh02/GlimpRouter">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the &quot;Aha Moment&quot; phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GlimpRouter：通过瞥见一个思维令牌实现高效的协作推理</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过显式生成多步思维链实现显著性能，但这种能力会带来显著的推理延迟和计算成本。协作推理通过在轻量级和大型模型之间选择性分配工作提供了有希望的解决方案，但一个基本挑战仍然存在：确定推理步骤需要大型模型的容量还是小型模型的效率。现有路由策略要么依赖于局部令牌概率，要么进行事后验证，引入了显著的推理开销。在本文中，我们提出了一种新的步骤协作视角：推理步骤的难度可以从其第一个令牌中推断出来。受LRMs中的“顿悟时刻”现象启发，我们表明初始令牌的熵是步骤难度的强预测器。基于这一洞察，我们引入了GlimpRouter，这是一种无需训练的步骤协作框架。GlimpRouter使用轻量级模型生成每个推理步骤的第一个令牌，并仅当初始令牌的熵超过阈值时才将步骤路由到更大的模型。在多个基准上的实验表明，我们的方法在显著减少推理延迟的同时保持了准确性。例如，与单独的大型模型相比，GlimpRouter在AIME25上的准确率提高了10.7%，推理延迟减少了25.9%。这些结果表明，一种简单而有效的推理机制是：根据思维的一瞥而不是完整步骤的评估来分配计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GlimpRouter proposes a novel approach to efficient collaborative inference by leveraging the entropy of the first token generated during the reasoning process. This method reduces inference latency and computational cost without significant loss of accuracy. Experiments show that GlimpRouter improves accuracy by 10.7% and reduces inference latency by 25.9% compared to a standalone large model on AIME25.</div>
<div class="mono" style="margin-top:8px">GlimpRouter通过利用推理过程中生成的第一个令牌的熵来实现协作推理，无需进行完整的步骤评估，轻量级模型可以处理更简单的推理步骤，只有在必要时才使用大型模型。实验表明，与AIME25上的单一大型模型相比，GlimpRouter可以提高10.7%的准确率并减少25.9%的推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact</div>
<div class="meta-line">Authors: Hyunji Lee, Seunghyun Yoon, Yunjae Won, Hanseok Oh, Geewook Kim, Trung Bui, Franck Dernoncourt, Elias Stengel-Eskin, Mohit Bansal, Minjoon Seo</div>
<div class="meta-line">First: 2025-06-18T14:13:56+00:00 · Latest: 2026-01-08T16:32:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15480v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.15480v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带有和不带有上下文的指令调优：行为变化及下游影响</div>
<div class="mono" style="margin-top:8px">指令调优是广泛用于提高大型语言模型（LLMs）指令遵循能力的一种方法。指令调优数据集通常包含上下文增强和无上下文示例的混合，但先前的工作大多将这些数据类型结合起来而没有考察它们的独特影响。在本文中，我们研究了在有或没有上下文的情况下训练LLMs如何影响模型行为和下游性能。首先，在文本领域，我们展示了使用上下文训练的LLMs更强烈地关注提供的知识，从而实现更好的定位。我们还观察到，上下文增强的训练改变了LLMs使用知识的方式：模型存储和利用的参数化知识较少，而是更多地依赖提供的上下文。其次，我们观察到，使用基于上下文增强数据训练的LLMs作为视觉-语言模型的骨干可以减少幻觉并改善视觉领域的定位。最后，我们探讨了在上下文可用性变化的现实世界部署中实用的策略。我们展示了保持上下文增强和无上下文模型的分离，并在它们之间路由输入，比训练单一混合模型能获得更稳健的整体性能，因为它更好地保留了它们互补的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the impact of training large language models with or without context on their instruction-following ability and downstream performance. It finds that context-augmented training improves grounding and shifts model reliance from parametric knowledge to provided context, reducing hallucination in vision-language models. The study also suggests maintaining separate models for context-augmented and context-free data to achieve better overall performance.</div>
<div class="mono" style="margin-top:8px">研究探讨了在有或无上下文的情况下训练大型语言模型（LLMs）对其指令遵循能力和下游性能的影响。研究发现，带有上下文的训练增强了模型的语境关联性，并使其对参数知识的依赖转向了提供的上下文。此外，使用带有上下文的LLM作为视觉-语言模型的骨干可以减少幻觉并提高视觉关联性。研究还建议，在不同上下文可用性场景中维护单独的带有上下文和不带上下文的模型以获得更稳健的整体性能，因为这样可以更好地保留它们的互补优势。</div>
</details>
</div>
<div class="card">
<div class="title">From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</div>
<div class="meta-line">Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar</div>
<div class="meta-line">First: 2026-01-08T16:02:56+00:00 · Latest: 2026-01-08T16:02:56+00:00</div>
<div class="meta-line">Comments: Contributed original research to top tier conference in VLM; currently undergoing peer review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05059v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05059v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut &amp; Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从理解到参与：通过视觉语言模型（VLMs）个性化制药视频片段</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）有望通过使智能、可扩展和自动化的多模态内容处理成为可能，来革新制药行业的数字化转型。传统的异构数据模态（文本、图像、视频、音频和网页链接）的手动注释容易导致不一致、内容质量下降和内容利用效率低下。大量的长视频和音频数据进一步加剧了这些挑战（例如，长期的临床试验访谈和教育研讨会）。
在这里，我们介绍了一种针对制药领域的视频到视频片段生成框架，该框架结合了音频语言模型（ALMs）和视觉语言模型（VLMs）以生成高光片段。我们的贡献包括三个方面：（i）一种可重复的剪切与合并算法，带有淡入淡出和时间戳规范化，确保平滑过渡和音视频对齐；（ii）基于角色定义和提示注入的个性化机制，以生成定制输出（营销、培训、监管）；（iii）一种成本效益高的端到端管道策略，平衡了ALM/VLM增强处理。在Video MME基准（900）和我们14个疾病领域16,159个制药视频的专有数据集上进行的评估表明，速度提高了3到4倍，成本降低了4倍，片段质量具有竞争力。除了效率提升，我们还报告了我们的方法提高了片段连贯性评分（0.348）和信息量评分（0.721），超过了最先进的VLM基线（例如，Gemini 2.5 Pro），突显了透明、自定义提取和合规支持的视频摘要在生命科学领域的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces a domain-adapted Video to Video Clip Generation framework using Audio Language Models (ALMs) and Vision Language Models (VLMs) to automate the creation of highlight clips from long pharmacy videos. Key contributions include a reproducible Cut &amp; Merge algorithm, a personalization mechanism based on role definitions, and an efficient end-to-end pipeline. Evaluations show a 3 to 4 times speedup, 4 times cost reduction, and improved clip quality scores compared to state-of-the-art VLMs.</div>
<div class="mono" style="margin-top:8px">该研究引入了一种基于音频和视觉语言模型的领域适配视频到视频剪辑生成框架，以自动化生成长药学视频的亮点剪辑。该框架包括可重复的剪辑与合并算法、个性化机制和高效的端到端流程。评估结果显示，在基准数据集和自有数据集上的表现分别提高了3到4倍的速度、4倍的成本效率，并且剪辑连贯性和信息量评分优于最先进的VLM基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform</div>
<div class="meta-line">Authors: Suyash Mishra, Qiang Li, Srikanth Patil, Satyanarayan Pati, Baddu Narendra</div>
<div class="meta-line">First: 2026-01-08T12:42:17+00:00 · Latest: 2026-01-08T12:42:17+00:00</div>
<div class="meta-line">Comments: Submitted to the Industry Track of Top Tier Conference; currently under peer review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04891v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new &quot;A+B&quot; model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在工业级GenAI平台上扩展视觉语言模型以处理制药长格式视频推理</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在多模态推理任务中表现出强大的性能，但大多数评估集中在短视频上，并假设不受限制的计算资源。在制药内容理解等工业环境中，从业人员必须在严格的GPU、延迟和成本约束下处理长格式视频，而许多现有方法无法扩展。在本研究中，我们提出了一种工业级GenAI框架，处理了超过200,000个PDF文件、25,326个不同格式（如MP4、M4V等）的视频以及888个多语言音频文件，涉及20多种语言。我们的研究做出了三项贡献：（i）制药领域的大规模多模态推理工业架构；（ii）在两个领先基准（Video-MME和MMBench）和包含25,326个视频的自有数据集（涵盖14个疾病领域）上对超过40个VLMs的实证分析；（iii）关于长格式视频推理的四项发现：多模态的作用、注意力机制权衡、时间推理限制以及在GPU约束下的视频分割挑战。结果表明，与普通GPU相比，SDPA注意力机制可提高3-8倍的效率，多模态在8/12任务领域（尤其是长度依赖任务）上可提高性能，开放源和闭源VLMs在时间对齐和关键帧检测方面存在明显瓶颈。本文并未提出新的“A+B”模型，而是对在现实部署约束下当前VLMs的实用极限、权衡和失败模式进行了描述，并为研究人员和从业者设计可扩展的多模态系统提供了实用指导，以用于工业领域的长格式视频理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the scalability challenges of Vision Language Models (VLMs) in industrial settings, particularly for long-form pharmaceutical videos. The study presents an industrial GenAI framework that processes over 200,000 documents and 25,326 videos across 14 disease areas. Key findings include efficiency gains with SDPA attention, improved performance in length-dependent tasks through multimodality, and identified bottlenecks in temporal alignment and keyframe detection. The research provides practical insights and actionable guidance for designing scalable multimodal systems under realistic constraints.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决在严格计算资源限制下，Vision Language Models (VLMs) 对制药领域长视频内容理解的可扩展性问题。研究在工业规模的数据集和专有基准上评估了超过40种VLMs，强调了多模态性、注意力机制和时间推理的重要性。关键发现包括使用SDPA注意力机制的效率提升、在长度依赖任务中的性能改进，以及在时间对齐和关键帧检测方面的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models</div>
<div class="meta-line">Authors: Oriol Rabasseda, Zenjie Li, Kamal Nasrollahi, Sergio Escalera</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2026-01-08T10:58:59+00:00 · Latest: 2026-01-08T10:58:59+00:00</div>
<div class="meta-line">Comments: This work has been accepted at Real World Surveillance: Applications and Challenges, 6th (in WACV Workshops)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04824v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SOVABench：一种针对多模态大型语言模型的车辆监控动作检索基准</div>
<div class="mono" style="margin-top:8px">自动识别事件和重复行为分析是视频监控的关键。然而，现有的大多数基于内容的视频检索基准主要关注场景相似性，而不评估监控所需的动作区分能力。为了解决这一差距，我们引入了SOVABench（Surveillance Opposite Vehicle Actions Benchmark），这是一个基于监控视频构建的现实世界检索基准，专注于车辆相关动作。SOVABench 定义了两种评估协议（跨动作对和同动作对），以评估跨动作区分能力和时间方向理解能力。尽管动作区分对于人类观察者来说通常是直观的，但我们的实验表明，对于最先进的视觉和多模态模型来说，它们仍然是具有挑战性的。
 利用多模态大型语言模型（MLLMs）的视觉推理和指令跟随能力，我们提出了一种无需训练的框架，用于从MLLM生成的描述中生成可解释的嵌入，适用于图像和视频。该框架在SOVABench以及几个对比视觉-语言模型经常失败的空间和计数基准上都取得了良好的性能。基准的代码、注释和构建说明已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SOVABench is a new benchmark for vehicle surveillance action retrieval, addressing the gap in existing benchmarks that do not evaluate action discrimination. It includes two evaluation protocols to assess cross-action discrimination and temporal direction understanding. The benchmark demonstrates that state-of-the-art vision and multimodal models struggle with these tasks. A training-free framework using MLLM-generated descriptions produces interpretable embeddings and achieves strong performance on SOVABench and other benchmarks where contrastive Vision-Language Models often fail.</div>
<div class="mono" style="margin-top:8px">SOVABench 是一个新的车辆 surveillance 行动检索基准，解决了现有基准不评估动作区分的问题。它包含两个评估协议，用于评估跨动作区分和时间方向理解。基准表明，最先进的视觉和多模态模型在这些任务上仍然存在挑战。一个无需训练的框架使用 MLLM 生成的描述产生了可解释的嵌入，并在 SOVABench 和其他对比视觉-语言模型经常失败的空间和计数基准上取得了良好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Retoucher for Text-To-Image Generation</div>
<div class="meta-line">Authors: Shaocheng Shen, Jianfeng Liang, Chunlei Cai, Cong Geng, Huiyu Duan, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</div>
<div class="meta-line">First: 2026-01-05T12:06:43+00:00 · Latest: 2026-01-08T10:57:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02046v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理修图师：用于文本到图像生成</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）扩散模型如SDXL和FLUX已经实现了令人印象深刻的写实效果，但在肢体、面部、文本等方面仍然普遍存在小规模失真。现有的精修方法要么进行昂贵的迭代重新生成，要么依赖于弱空间定位的视觉语言模型（VLMs），导致语义漂移和不可靠的局部编辑。为了解决这一问题，我们提出了一种层次化的决策驱动框架——代理修图师，将后生成修正重新构想为类似人类感知-推理-行动的循环。具体来说，我们设计了（1）一个感知代理，学习在文本-图像一致性线索下的细粒度失真定位的上下文显著性；（2）一个推理代理，通过逐步偏好对齐进行符合人类的推断诊断；（3）一个行动代理，根据用户偏好自适应地计划局部修复。该设计将感知证据、语言推理和可控修正整合到一个统一的、自我修正的决策过程中。为了实现精细的监督和定量评估，我们进一步构建了包含6000张T2I图像和27000个注释缺陷区域的12个类别的GenBlemish-27K数据集。广泛的实验表明，代理修图师在感知质量、失真定位和人类偏好对齐方面始终优于最先进的方法，建立了自修正和感知可靠的T2I生成的新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the small-scale distortions in text-to-image generation by proposing Agentic Retoucher, a hierarchical framework that mimics human perception, reasoning, and action. It includes a perception agent for fine-grained distortion localization, a reasoning agent for human-aligned inferential diagnosis, and an action agent for adaptive localized inpainting. The framework integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified process. Experiments show that Agentic Retoucher outperforms existing methods in perceptual quality, distortion localization, and human preference alignment, setting a new standard for self-corrective and perceptually reliable T2I generation.</div>
<div class="mono" style="margin-top:8px">研究旨在解决SDXL和FLUX等文本到图像生成模型中存在的小尺度失真问题。提出了Agentic Retoucher作为一种分层框架，将后生成修正重新构想为感知-推理-行动循环。该框架包括用于精细失真定位的感知代理、用于人类对齐的推理诊断代理以及用于适应性局部修复的动作代理。该框架将感知证据、语言推理和可控修正统一到一个过程中。实验表明，Agentic Retoucher在感知质量、失真定位和人类偏好对齐方面优于现有方法，为自我纠正和感知可靠的文本到图像生成设定了新标准。</div>
</details>
</div>
<div class="card">
<div class="title">AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding</div>
<div class="meta-line">Authors: Aleksei Kondratenko, Mussie Birhane, Houssame E. Hsain, Guido Maciocci</div>
<div class="meta-line">First: 2026-01-08T10:54:32+00:00 · Latest: 2026-01-08T10:54:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04819v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AECV-Bench：建筑和工程图纸理解的多模态模型基准测试</div>
<div class="mono" style="margin-top:8px">建筑和工程(AEC)图纸通过符号、布局规范和密集注释来编码几何和语义，但尚不清楚现代多模态和视觉-语言模型是否能可靠地解释这种图形语言。我们提出了AECV-Bench，这是一个基准测试，通过两个互补的应用场景来评估多模态和视觉-语言模型在现实AEC制品上的表现：(i) 在120份高质量的楼层平面图上进行物体计数（门、窗、卧室、厕所），(ii) 包含192个问题-答案对的图纸指导文档问答，测试文本提取（OCR）、实例计数、空间推理和对常见图纸区域的比较推理。物体计数性能使用每个字段的精确匹配准确率和MAPE结果报告，而文档问答性能使用总体准确率和按类别细分的评分管道报告，并通过LLM作为法官的评分流程和针对边缘情况的人工复核。在统一协议下评估一系列最先进的模型，我们观察到一种稳定的性能梯度；文本提取和文本为中心的文档问答表现最强（高达0.95的准确率），空间推理表现中等，而以符号为中心的图纸理解——尤其是门和窗的可靠计数——仍然无法解决（通常0.40-0.55的准确率），存在大量比例错误。这些结果表明，当前系统在文档助手方面表现良好，但在绘制图的阅读能力方面缺乏稳健性，这促使了针对特定领域的表示和工具增强的人机协作工作流程，以实现高效的AEC自动化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AECV-Bench evaluates multimodal and vision-language models on interpreting architectural and engineering drawings through object counting and drawing-grounded document QA, revealing that while OCR and text extraction are strong, spatial reasoning and reliable counting of symbols like doors and windows remain challenging. The benchmark uses a unified protocol to assess a range of state-of-the-art models, highlighting the need for domain-specific representations and human-in-the-loop workflows for AEC automation.</div>
<div class="mono" style="margin-top:8px">AECV-Bench 通过对象计数和文档问答任务评估多模态和视觉-语言模型在建筑和工程图纸上的表现。基准数据集包括120个楼层平面图用于对象计数，以及192个问题-答案对用于文档问答。结果显示，在OCR和文本提取方面表现强劲，空间推理表现一般，而在门窗计数等符号识别方面表现较差，表明需要领域特定的表示和人工在环的工作流程以实现AEC自动化。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-08T10:46:04+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用黑盒优化构建针对大型视觉-语言模型的对抗输入</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多种跨模态任务中展现了突破性的能力。然而，这些模型仍然容易受到对抗性脱管攻击的影响，攻击者通过施加微妙的扰动来绕过安全机制并触发有害输出。现有的白盒攻击方法需要完全访问模型，计算成本高且对抗性转移性不足，使其在实际的黑盒环境中不切实际。为了解决这些限制，我们提出了一种通过零阶优化使用同时扰动随机近似（ZO-SPSA）对LVLMs进行黑盒脱管攻击的方法。ZO-SPSA提供了三个关键优势：（i）无需模型知识的输入-输出交互的无梯度近似，（ii）无需代理模型的模型无关优化，（iii）降低资源需求，减少GPU内存消耗。我们在三个LVLMs上评估了ZO-SPSA，包括InstructBLIP、LLaVA和MiniGPT-4，在InstructBLIP上实现了最高的脱管攻击成功率83.0%，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗性示例在其他LVLMs上表现出强大的转移性，ASR达到64.18%。这些发现强调了黑盒脱管攻击在实际环境中的可行性，并揭示了当前LVLMs安全机制中的关键弱点</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the vulnerability of Large Vision-Language Models (LVLMs) to adversarial attacks by proposing a black-box jailbreak attack using Zeroth-Order optimization with Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA allows for gradient-free optimization without model knowledge, reducing resource requirements and enhancing transferability. The method achieves high jailbreak success rates, comparable perturbations to white-box methods, and strong transferability across different LVLMs, highlighting the need for improved safety mechanisms in these models.</div>
<div class="mono" style="margin-top:8px">该研究通过提出基于零阶优化的Simultaneous Perturbation Stochastic Approximation (ZO-SPSA)方法，针对大型视觉-语言模型（LVLMs）的黑盒破解攻击，解决了现有白盒攻击方法需要模型知识、计算成本高且缺乏对抗样本可移植性的问题。实验结果显示，在InstructBLIP、LLaVA和MiniGPT-4上的破解成功率高达83.0%，并且生成的对抗样本在MiniGPT-4上的攻击成功率达到了64.18%，显示出强大的可移植性。这些结果强调了黑盒破解攻击在现实世界中的可行性，并揭示了当前LVLMs安全机制中的关键弱点。</div>
</details>
</div>
<div class="card">
<div class="title">CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</div>
<div class="meta-line">Authors: Dahyeon Kye, Jeahun Sung, Minkyu Jeon, Jihyong Oh</div>
<div class="meta-line">First: 2025-12-08T04:39:12+00:00 · Latest: 2026-01-08T10:29:58+00:00</div>
<div class="meta-line">Comments: Please visit our project page at https://cmlab-korea.github.io/CHIMERA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07155v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.07155v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cmlab-korea.github.io/CHIMERA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHIMERA：自适应缓存注入与语义锚点提示的零样本图像形态变换及其形态导向度量</div>
<div class="mono" style="margin-top:8px">扩散模型展示了卓越的生成能力，但在实现平滑且语义一致的图像形态变换方面仍面临挑战。现有方法往往由于缺乏自适应的结构和语义对齐而产生突兀的过渡或过度饱和的外观。我们提出CHIMERA，一种基于扩散的零样本框架，将形态变换形式化为缓存反演引导的去噪过程。为处理大规模的语义和外观差异，我们提出了自适应缓存注入和语义锚点提示。自适应缓存注入(ACI)在DDIM反演过程中从两个输入中缓存下、中、上层特征，并在去噪过程中适配性地重新注入，从而在深度和时间自适应的方式下实现空间和语义对齐，并实现自然特征融合和平滑过渡。语义锚点提示(SAP)利用视觉-语言模型生成共享的锚点提示，作为语义锚点，连接不相似的输入，并引导去噪过程向一致的结果发展。最后，我们引入全局-局部一致性评分(GLCS)，这是一种形态导向度量，同时评估两个输入的全局和谐性和局部形态变换的平滑度。广泛的实验和用户研究显示，CHIMERA实现了比现有方法更平滑且更语义对齐的过渡，建立了图像形态变换的新基准。代码和项目页面将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CHIMERA is a zero-shot diffusion-based framework designed to achieve smooth and semantically consistent image morphing by formulating morphing as a cached inversion-guided denoising process. It introduces Adaptive Cache Injection (ACI) and Semantic Anchor Prompting (SAP) to handle large semantic and appearance disparities, and proposes the Global-Local Consistency Score (GLCS) as a morphing-oriented metric. Experiments demonstrate that CHIMERA outperforms existing methods in achieving smoother and more semantically aligned transitions, setting a new state of the art in image morphing.</div>
<div class="mono" style="margin-top:8px">CHIMERA 是一种基于扩散模型的零样本框架，通过将图像变形视为缓存反演引导的去噪过程来实现平滑且语义一致的图像变形。它引入了自适应缓存注入（ACI）和语义锚点提示（SAP）来处理大规模的语义和外观差异。ACI 在 DDIM 反演过程中缓存来自两个输入的特征并在去噪过程中适当地重新注入，而 SAP 使用视觉语言模型生成共享的锚点提示以实现语义对齐。实验表明，CHIMERA 在实现更平滑且更语义对齐的过渡方面优于现有方法，建立了图像变形的新基准。代码和项目页面已公开。</div>
</details>
</div>
<div class="card">
<div class="title">ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Yen-Jen Chiou, Wei-Tse Cheng, Yuan-Fu Yang</div>
<div class="meta-line">First: 2026-01-08T09:20:46+00:00 · Latest: 2026-01-08T09:20:46+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04754v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProFuse：开放词汇3D高斯点云融合的高效跨视图上下文融合框架</div>
<div class="mono" style="margin-top:8px">我们提出了ProFuse，一种基于3D高斯点云（3DGS）的开放词汇3D场景理解的高效上下文感知框架。该流水线在直接配准设置中增强跨视图一致性及掩膜内的内聚性，增加的开销极小，无需渲染监督微调。我们引入了一种密集对应关系引导的预配准阶段，该阶段以准确的几何形状初始化高斯点，同时通过跨视图聚类联合构建3D上下文提案。每个提案携带一个通过加权聚合成员嵌入获得的全局特征，并在直接配准过程中将该特征融合到高斯点上，以保持视图间的语义一致性。通过预先建立的关联，语义融合无需额外优化，且模型在不增加密度的情况下保留几何细化。ProFuse在每场景约五分钟内实现强大的开放词汇3DGS理解，比当前最佳方案快两倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProFuse is an efficient framework for 3D scene understanding using 3D Gaussian Splatting. It enhances cross-view consistency and intra-mask cohesion through a dense correspondence-guided pre-registration phase and cross-view clustering. This approach initializes Gaussians with accurate geometry and fuses global features during direct registration, maintaining semantic coherence across views. ProFuse completes semantic attachment in about five minutes per scene, which is twice as fast as the state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">ProFuse 是一种高效的上下文感知框架，使用 3D 高斯点积进行开放词汇的 3D 场景理解。它通过增强跨视图一致性和内部掩膜的凝聚力，而不增加显著的开销或需要渲染监督的微调。通过引入密集对应关系引导的预注册阶段，ProFuse 准确初始化高斯点并使用跨视图聚类构建 3D 上下文提案。这些提案在直接注册过程中与全局特征融合，保持跨视图的语义一致性。ProFuse 每个场景的语义附着大约需要五分钟，比最先进的方法快两倍。</div>
</details>
</div>
<div class="card">
<div class="title">Skeletonization-Based Adversarial Perturbations on Large Vision Language Model&#x27;s Mathematical Text Recognition</div>
<div class="meta-line">Authors: Masatomo Yoshida, Haruto Namura, Nicola Adami, Masahiro Okuda</div>
<div class="meta-line">Venue: Proc. ITC-CSCC 2025</div>
<div class="meta-line">First: 2026-01-08T09:15:27+00:00 · Latest: 2026-01-08T09:15:27+00:00</div>
<div class="meta-line">Comments: accepted to ITC-CSCC 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04752v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04752v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models&#x27; visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于骨架化的大规模视觉语言模型数学文本识别的对抗性扰动</div>
<div class="mono" style="margin-top:8px">本研究通过引入一种利用骨架化减少搜索空间的新颖对抗攻击方法，探索基础模型的视觉能力和局限性。我们的方法特别针对包含文本的图像，尤其是由于其LaTeX转换和复杂的结构，数学公式图像更具挑战性。我们详细评估了原始输出和对抗性扰动输出之间的字符和语义变化，以提供模型视觉解释和推理能力的见解。通过将其应用于ChatGPT，进一步证明了该方法的有效性及其在实际场景中的实际意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the visual recognition capabilities of large vision-language models by applying a skeletonization-based adversarial attack on mathematical text images. The method narrows down the search space and targets the more complex structure of mathematical formulas. Key findings include significant character and semantic changes in the model&#x27;s outputs, indicating limitations in visual interpretation and reasoning. The attack&#x27;s effectiveness is also shown on ChatGPT, highlighting its practical implications.</div>
<div class="mono" style="margin-top:8px">该研究通过使用骨架化基于的对抗攻击方法，探讨大型视觉语言模型的视觉能力。方法针对数学公式图像，减少搜索空间，并评估字符和语义变化。研究展示了模型在视觉解释和推理方面的局限性，并通过应用到ChatGPT展示了其实用意义。</div>
</details>
</div>
<div class="card">
<div class="title">Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning</div>
<div class="meta-line">Authors: Wentao Zhang, Lifei Wang, Lina Lu, MingKun Xu, Shangyang Li, Yanchao Yang, Tao Fang</div>
<div class="meta-line">Venue: ACL 2026 long</div>
<div class="meta-line">First: 2026-01-08T07:34:37+00:00 · Latest: 2026-01-08T07:34:37+00:00</div>
<div class="meta-line">Comments: This paper is submitted for review to ACL 2026. It is 17 pages long and includes 5 figures. The corresponding authors are Tao Fang and Lina Lu</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04672v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Agri-R1：通过强化学习增强通用农业推理能力的视觉语言模型</div>
<div class="mono" style="margin-top:8px">农业疾病诊断对VLM构成挑战，因为传统的微调需要大量标签，缺乏可解释性且泛化能力差。尽管推理可以提高模型的鲁棒性，但现有方法依赖昂贵的专家注释，很少解决农业查询的开放性和多样性。为解决这些局限性，我们提出了**Agri-R1**，一种增强的农业推理大型模型。我们的框架通过视觉语言合成和基于LLM的过滤自动生成高质量的推理数据，仅使用可用样本的19%。训练使用组相对策略优化（GRPO）和一个新颖的奖励函数，该函数结合领域特定词汇和模糊匹配来评估开放性回答的正确性和语言灵活性。在CDDMBench上评估，我们的3B参数模型在疾病识别准确性上比7B到13B参数的基线模型高出+23.2%，在农业知识问答上高出+33.3%，在跨域泛化上比标准微调高出+26.10分。消融研究证实，结构化推理数据与GRPO驱动的探索之间的协同作用是这些改进的基础，随着问题复杂性的增加，这种好处会增加。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of agricultural disease diagnosis for vision-language models (VLMs) by proposing Agri-R1, which enhances model reasoning through reinforcement learning. The method generates high-quality reasoning data using vision-language synthesis and LLM-based filtering, requiring only 19% of available samples. Training uses Group Relative Policy Optimization (GRPO) with a reward function that integrates domain-specific lexicons and fuzzy matching. The resulting 3B-parameter model outperforms baselines, achieving a +23.2% relative gain in disease recognition accuracy, +33.3% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning.</div>
<div class="mono" style="margin-top:8px">Agri-R1 通过提出一个增强推理的框架来解决传统微调在农业疾病诊断中的局限性。该框架通过视觉语言合成和基于大语言模型的过滤自动生成高质量的推理数据，并使用一种结合领域特定词汇的新颖奖励函数，该奖励函数能够评估开放性回答的正确性和语言灵活性。最终生成的3B参数模型在疾病识别准确性上提高了23.2%，在农业知识问答上提高了33.3%。</div>
</details>
</div>
<div class="card">
<div class="title">GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning</div>
<div class="meta-line">Authors: Wenshuai Li, Xiantai Xiang, Zixiao Wen, Guangyao Zhou, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuxin Hu</div>
<div class="meta-line">First: 2026-01-07T17:26:41+00:00 · Latest: 2026-01-08T06:19:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04118v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoReason: 通过逻辑一致性强化学习使遥感视觉语言模型的思考与回答保持一致</div>
<div class="mono" style="margin-top:8px">遥感视觉语言模型(RS-VLMs)的发展强调了从感知中心的识别向高级演绎推理转变的重要性，以增强复杂空间任务中的认知可靠性。然而，当前模型常常遭受逻辑幻觉的问题，即正确的答案是基于有缺陷的推理链或依赖于位置捷径而非空间逻辑。这种脱节削弱了在战略空间决策中的可靠性。为了解决这一问题，我们提出了GeoReason框架，旨在使内部思考与最终决策同步。我们首先构建了GeoReason-Bench，这是一个逻辑驱动的数据集，包含4,000条从几何原语和专家知识中合成的推理轨迹。然后我们制定了两阶段训练策略：(1) 监督知识初始化，以使模型具备推理语法和领域专业知识；(2) 一致性感知强化学习，以提高演绎可靠性。这一阶段整合了一种新颖的逻辑一致性奖励，通过选项排列策略惩罚逻辑漂移，以使决策基于可验证的推理轨迹。实验结果表明，我们的框架显著提高了RS-VLMs的认知可靠性和可解释性，达到了与其他先进方法相比的最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GeoReason is a framework designed to improve the cognitive reliability of Remote Sensing Vision-Language Models (RS-VLMs) by aligning internal reasoning with final decisions. It introduces a two-stage training strategy: supervised knowledge initialization to equip models with reasoning syntax and domain expertise, followed by consistency-aware reinforcement learning to refine deductive reliability. The framework uses a novel Logical Consistency Reward to penalize logical drift, ensuring decisions are based on verifiable reasoning. Experiments show that GeoReason significantly enhances the cognitive reliability and interpretability of RS-VLMs, outperforming other advanced methods.</div>
<div class="mono" style="margin-top:8px">GeoReason 是一个框架，旨在通过解决逻辑幻觉来提高遥感视觉语言模型（RS-VLMs）的认知可靠性和可解释性。它采用两阶段训练策略：监督知识初始化和一致性意识强化学习。后者使用逻辑一致性奖励来惩罚逻辑漂移，确保决策基于可验证的推理轨迹。实验表明，GeoReason 显著提升了 RS-VLMs 的性能和可靠性，优于其他先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models</div>
<div class="meta-line">Authors: Dianyi Wang, Siyuan Wang, Zejun Li, Yikun Wang, Yitong Li, Duyu Tang, Xiaoyu Shen, Xuanjing Huang, Zhongyu Wei</div>
<div class="meta-line">First: 2025-08-13T13:00:05+00:00 · Latest: 2026-01-08T05:44:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09779v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09779v2">PDF</a> · <a href="https://github.com/AlenjandroWang/MoIIE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across multi-modal tasks by scaling model size and training data. However, these dense LVLMs incur significant computational costs and motivate the exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve parameter efficiency, effectively applying MoE to simultaneously model modality-specific features and cross-modal associations in LVLMs remains challenging. In this work, we propose to incorporate Mixture of Intra- and Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is guided by its modality, directing tokens to their respective intra-modality experts as well as a shared pool of inter-modality experts, enabling the model to jointly learn rich intra-modal features and cross-modal interactions. We further introduce an effective and straightforward two-stage training strategy, which facilitates the direct activation of both MoE and multi-modal capabilities. Extensive experiments across different data scales and LLM backbone demonstrate the effectiveness, efficiency and generality of our approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters match or even surpass the performance of existing advanced open-source MoE-LLMs based multi-modal models that involve more activated parameters. The code is available at https://github.com/AlenjandroWang/MoIIE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoIIE：混合内模态和跨模态专家的大规模视觉语言模型</div>
<div class="mono" style="margin-top:8px">大规模多模态语言模型（LVLMs）通过扩大模型规模和训练数据，在多模态任务中表现出显著的性能。然而，这些密集的LVLMs带来了显著的计算成本，并促使人们探索稀疏专家混合（MoE）架构。虽然MoE提高了参数效率，但同时建模LVLMs中的模态特定特征和跨模态关联仍然具有挑战性。在本文中，我们提出将混合内模态和跨模态专家（MoIIE）引入LVLMs。对于每个标记，专家路由由其模态引导，将标记导向其各自的内模态专家以及共享的跨模态专家池，使模型能够同时学习丰富的内模态特征和跨模态交互。我们还引入了一种有效的两阶段训练策略，这有助于直接激活MoE和多模态能力。在不同数据规模和LLM主干的广泛实验中，证明了我们方法的有效性、效率和通用性。值得注意的是，我们的MoIIE模型在激活参数为55亿和113亿的情况下，与涉及更多激活参数的现有先进开源MoE-LLMs基于多模态模型相比，性能相当甚至更优。代码可在https://github.com/AlenjandroWang/MoIIE/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenges of applying Mixture of Experts (MoE) to large Vision-Language Models (LVLMs) by proposing MoIIE, which incorporates both intra- and inter-modality experts. The model routes tokens to either modality-specific experts or a shared pool of cross-modal experts, allowing for efficient learning of both intra-modal features and cross-modal interactions. The authors introduce a two-stage training strategy to enhance the model&#x27;s performance. Experimental results show that MoIIE models with fewer activated parameters match or outperform more complex MoE-LLMs on various tasks, demonstrating the approach&#x27;s effectiveness and efficiency. The code is available on GitHub.</div>
<div class="mono" style="margin-top:8px">该研究针对大型视觉语言模型（LVLM）的计算挑战，提出了MoIIE混合内模和跨模专家模型。该模型将令牌路由到特定模态的专家和共享的跨模专家池，以促进学习内模特征和跨模交互。实验表明，MoIIE模型在更少激活参数的情况下，能够匹配甚至超越更复杂的现有模型，展示了其在不同数据规模和LLM骨干网络上的有效性、高效性和通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</div>
<div class="meta-line">Authors: Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen</div>
<div class="meta-line">First: 2025-12-12T09:19:45+00:00 · Latest: 2026-01-08T05:10:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11399v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最少片段，最大显著性：通过关键时刻提取进行长视频摘要</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）能够处理越来越长的视频。然而，重要视觉信息很容易在整个上下文中丢失并被VLMs忽略。此外，设计能够经济有效地分析长视频内容的工具也很重要。在本文中，我们提出了一种片段选择方法，旨在选择要包含在多模态摘要中的关键视频时刻。我们将视频划分为短片段，并使用轻量级视频描述模型生成每个片段的紧凑视觉描述。然后将这些描述传递给大型语言模型（LLM），该模型选择包含最多相关视觉信息的K个片段以构建多模态摘要。我们在MovieSum数据集中的人类标注屏幕剧和摘要的参考片段上评估了我们的方法。我们进一步表明，这些参考片段（不到电影的6%）足以构建MovieSum中电影的完整多模态摘要。使用我们的片段选择方法，我们实现了与这些参考片段相当的摘要性能，同时捕获了比随机片段选择多得多的相关视频信息。重要的是，我们通过依赖轻量级描述模型保持了较低的计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of summarizing long videos by focusing on key moments. It proposes a method that divides videos into short clips and uses a lightweight video captioning model to generate visual descriptions. These descriptions are then analyzed by a large language model to select the most relevant clips for a multimodal summary. The approach is evaluated on reference clips from the MovieSum dataset, showing that a small fraction of the movie (less than 6%) is sufficient to create a complete summary. The method achieves summarization performance close to the reference clips while capturing more relevant video information than random selection, maintaining low computational cost through the use of a lightweight model.</div>
<div class="mono" style="margin-top:8px">本文旨在通过聚焦关键时刻来总结长视频。提出了一种方法，将视频分割成短片段，并使用轻量级视频描述模型生成视觉描述。这些描述随后由大型语言模型分析，以选择最相关的片段用于多模态摘要。该方法在来自MovieSum数据集的参考片段上进行了评估，显示少于6%的电影片段足以创建完整的摘要。该方法通过使用轻量级模型实现了与参考片段相近的总结性能，同时捕获了比随机选择更多相关的视频信息，且保持了较低的计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation</div>
<div class="meta-line">Authors: Amit Bin Tariqul, A N M Zahid Hossain Milkan, Sahab-Al-Chowdhury, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan</div>
<div class="meta-line">First: 2026-01-08T03:01:59+00:00 · Latest: 2026-01-08T03:01:59+00:00</div>
<div class="meta-line">Comments: Under review, 12 pages, 7 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04534v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04534v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (&gt;88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BanglaLorica：面向孟加拉语大型语言模型文本生成的鲁棒水印算法设计与评估</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在文本生成中的广泛应用，水印技术对于作者归属、知识产权保护和滥用检测变得至关重要。尽管现有的水印方法在高资源语言中表现良好，但在低资源语言中的鲁棒性仍被忽视。本研究首次系统评估了最先进的文本水印方法：KGW、指数采样（EXP）和Waterfall，针对孟加拉语LLM文本生成在跨语言往返翻译（RTT）攻击下的表现。在正常条件下，KGW和EXP的检测准确率超过88%，且几乎不影响困惑度和ROUGE值。然而，RTT攻击导致检测准确率降至9-13%，表明基于标记级别的水印方法存在根本性失败。为解决这一问题，我们提出了一种分层水印策略，结合了嵌入时和生成后水印。实验结果表明，分层水印策略在RTT攻击后的检测准确率提高了25-35%，达到40-50%，相比单层方法提高了3到4倍，但代价是可控的语义降解。我们的研究量化了多语言水印的鲁棒性-质量权衡，并将分层水印策略确立为低资源语言如孟加拉语的一种实用、无需训练的解决方案。我们的代码和数据将公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work evaluates the robustness of state-of-the-art text watermarking methods (KGW, EXP, and Waterfall) for Bangla LLM text generation under cross-lingual round-trip translation attacks. While these methods achieve high detection accuracy under benign conditions, they fail significantly under RTT attacks. To address this, a layered watermarking strategy combining embedding-time and post-generation watermarks is proposed, which improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, a 3-4 times relative improvement over single-layer methods, at the cost of controlled semantic degradation. This study highlights the robustness-quality trade-off in multilingual watermarking and establishes layered watermarking as a practical solution for low-resource languages like Bangla.</div>
<div class="mono" style="margin-top:8px">这项工作评估了KGW、EXP和Waterfall等最先进的水印方法在跨语言往返翻译攻击下的鲁棒性。虽然这些方法在良性条件下可以实现高检测准确率，但在往返翻译攻击下表现不佳。为此，作者提出了一种结合嵌入时间和生成后水印的分层水印策略，该策略在往返翻译后的检测准确率上提高了25-35%，同时引入了可控的语义降解。这项研究揭示了多语言水印的鲁棒性-质量权衡，并建议分层水印作为低资源语言如孟加拉语的实际解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Current Agents Fail to Leverage World Model as Tool for Foresight</div>
<div class="meta-line">Authors: Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-Tür, Gokhan Tur, Yunzhu Li, Heng Ji</div>
<div class="meta-line">First: 2026-01-07T13:15:23+00:00 · Latest: 2026-01-08T02:36:21+00:00</div>
<div class="meta-line">Comments: 36 Pages, 13 Figures, 17 Tables (Meta data updated)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03905v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03905v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents&#x27; capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当前代理无法利用世界模型作为前瞻工具</div>
<div class="mono" style="margin-top:8px">基于视觉-语言模型构建的代理越来越多地面临需要预测未来状态的任务，而不是依赖于短期推理。生成的世界模型提供了一种有希望的解决方案：代理可以使用它们作为外部模拟器，在行动前预见结果。本文实证研究了当前代理是否能够利用此类世界模型作为工具来增强其认知能力。在各种各样的代理和视觉问答任务中，我们观察到一些代理很少使用模拟（不到1%），频繁误用预测滚动（约15%），并且在模拟可用或强制时，经常表现出不一致甚至退化的性能（最高5%）。归因分析进一步表明，主要瓶颈在于代理决定何时模拟、如何解释预测结果以及如何将前瞻性纳入下游推理的能力。这些发现强调了需要机制来促进与世界模型的校准、战略性互动，为未来代理系统更可靠的前瞻性认知铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether current agents can effectively use generative world models to enhance their foresight in tasks requiring long-term reasoning. Across various agentic and visual question answering tasks, the study finds that some agents rarely use simulations, many misuse predicted outcomes, and some even perform worse when simulations are available. The main challenge is the agents&#x27; inability to decide when and how to simulate and integrate foresight into their reasoning. These results highlight the necessity for better mechanisms to facilitate strategic interaction with world models in future agent systems.</div>
<div class="mono" style="margin-top:8px">本文研究了当前代理是否能够有效利用生成的世界模型来增强其在需要长期推理的任务中的前瞻性。在各种代理和视觉问答任务中，研究发现许多代理要么很少或糟糕地利用这些模型，经常导致性能不一致甚至变差。主要挑战在于代理无法决定何时进行模拟、正确解释预测结果以及将前瞻性融入其决策过程。这些发现强调了在未来代理系统中更好地促进与世界模型的战略互动的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Agents for Interactive Forest Change Analysis</div>
<div class="meta-line">Authors: James Brock, Ce Zhang, Nantheera Anantrasirichai</div>
<div class="meta-line">First: 2026-01-08T02:02:36+00:00 · Latest: 2026-01-08T02:02:36+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures, Submitted to IGARSS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04497v1">PDF</a> · <a href="https://github.com/JamesBrockUoB/ForestChat">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言代理在交互式森林变化分析中的应用</div>
<div class="mono" style="margin-top:8px">现代森林监测工作流程越来越多地受益于高分辨率卫星图像的日益可用和深度学习的进展。在此背景下，准确的像素级变化检测和复杂森林动态的有意义语义变化描述是两个持续存在的挑战。虽然大型语言模型（LLMs）正在被适应用于交互式数据探索，但它们与视觉-语言模型（VLMs）在遥感图像变化解释（RSICI）中的集成仍然未被充分探索。为了解决这一差距，我们引入了一个由LLM驱动的集成森林变化分析代理，支持跨多个RSICI任务的自然语言查询。所提出系统基于多级变化解释（MCI）视觉-语言骨干，并通过LLM进行编排。为了在森林环境中促进适应和评估，我们进一步引入了森林变化数据集，该数据集包含双时相卫星图像、像素级变化掩码以及使用人类注释和基于规则的方法生成的多粒度语义变化描述。实验结果表明，所提出系统在森林变化数据集上的mIoU和BLEU-4得分为67.10%和40.17%，在LEVIR-MCI-Trees上的得分为88.13%和34.41%，LEVIR-MCI基准数据集的一个以树木为重点的子集，用于联合变化检测和描述。这些结果突显了交互式、LLM驱动的RSICI系统在提高森林变化分析的可访问性、可解释性和效率方面的潜力。所有数据和代码均可在https://github.com/JamesBrockUoB/ForestChat/上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of accurate pixel-level change detection and semantic change captioning in forest monitoring using deep learning and large language models. It introduces an LLM-driven agent that integrates a multi-level change interpretation vision-language backbone for RSICI tasks, supported by a new Forest-Change dataset. The system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset and 88.13% and 34.41% on LEVIR-MCI-Trees, demonstrating improved accessibility, interpretability, and efficiency in forest change analysis.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将大型语言模型（LLMs）与视觉语言模型（VLMs）结合，提高森林监测的准确性和语义描述能力。提出的系统使用多级变化解释骨干网络，并通过LLM进行编排，以支持自然语言查询。实验在森林变化数据集和LEVIR-MCI-Trees数据集上分别展示了67.10%的mIoU和40.17%的BLEU-4分数，表明交互式LLM驱动的遥感图像变化解释系统可以提高森林变化分析的可访问性、可解释性和效率。所有数据和代码可在https://github.com/JamesBrockUoB/ForestChat获取。</div>
</details>
</div>
<div class="card">
<div class="title">From Dataset to Real-world: General 3D Object Detection via Generalized Cross-domain Few-shot Learning</div>
<div class="meta-line">Authors: Shuangzhi Li, Junlong Shen, Lei Ma, Xingyu Li</div>
<div class="meta-line">First: 2025-03-08T17:05:21+00:00 · Latest: 2026-01-08T01:19:36+00:00</div>
<div class="meta-line">Comments: The latest version refines the few-shot setting on common classes, enforcing a stricter object-level definition</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.06282v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.06282v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LiDAR-based 3D object detection models often struggle to generalize to real-world environments due to limited object diversity in existing datasets. To tackle it, we introduce the first generalized cross-domain few-shot (GCFS) task in 3D object detection, aiming to adapt a source-pretrained model to both common and novel classes in a new domain with only few-shot annotations. We propose a unified framework that learns stable target semantics under limited supervision by bridging 2D open-set semantics with 3D spatial reasoning. Specifically, an image-guided multi-modal fusion injects transferable 2D semantic cues into the 3D pipeline via vision-language models, while a physically-aware box search enhances 2D-to-3D alignment via LiDAR priors. To capture class-specific semantics from sparse data, we further introduce contrastive-enhanced prototype learning, which encodes few-shot instances into discriminative semantic anchors and stabilizes representation learning. Extensive experiments on GCFS benchmarks demonstrate the effectiveness and generality of our approach in realistic deployment settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从数据集到现实世界：通过通用跨域少样本学习进行通用3D物体检测</div>
<div class="mono" style="margin-top:8px">基于LiDAR的3D物体检测模型往往难以在现实环境中泛化，因为现有数据集中的物体多样性有限。为解决这一问题，我们首次在3D物体检测中引入了通用跨域少样本学习（GCFS）任务，旨在仅通过少量标注将源预训练模型适应到新域中的常见和新型类。我们提出了一种统一框架，通过将2D开放集语义与3D空间推理相结合，在有限监督下学习稳定的靶域语义。具体而言，图像引导的多模态融合通过视觉语言模型将可转移的2D语义线索注入3D管道，而物理感知的框搜索通过LiDAR先验增强2D到3D对齐。为了从稀疏数据中捕获类特定语义，我们进一步引入了对比增强原型学习，将少量样本编码为判别性语义锚点，并稳定表示学习。在GCFS基准上的大量实验表明，我们的方法在现实部署场景中具有有效性和普适性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the generalization of LiDAR-based 3D object detection models to real-world environments by addressing the limited object diversity in existing datasets. It introduces a generalized cross-domain few-shot (GCFS) task and a unified framework that combines 2D open-set semantics with 3D spatial reasoning. The method includes image-guided multi-modal fusion and physically-aware box search to enhance transferability and alignment, and contrastive-enhanced prototype learning to stabilize representation learning. Experiments show the approach effectively adapts to both common and novel classes with few-shot annotations in new domains.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有数据集中的物体多样性限制，提高基于LiDAR的3D物体检测模型在真实环境中的泛化能力。引入了一种统一的跨域少量样本学习框架，该框架能够将预训练模型适应新域中的常见和新型类目，并仅使用少量标注数据。关键方法包括基于图像的多模态融合和物理感知的框搜索，这些方法增强了2D到3D的对齐和语义推理。此外，还引入了对比增强的原型学习来从稀疏数据中稳定表示学习。实验表明，该方法在现实场景中具有有效性和普适性。</div>
</details>
</div>
<div class="card">
<div class="title">UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving</div>
<div class="meta-line">Authors: Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu, Jingru Luo, Nathan Jacobs, Liu Ren</div>
<div class="meta-line">First: 2026-01-07T23:49:52+00:00 · Latest: 2026-01-07T23:49:52+00:00</div>
<div class="meta-line">Comments: Project Page: https://unidrive-wm.github.io/UniDrive-WM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://unidrive-wm.github.io/UniDrive-WM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM&#x27;s trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniDrive-WM：统一理解、规划和生成世界模型的自主驾驶</div>
<div class="mono" style="margin-top:8px">世界模型已成为自主驾驶的核心，准确的场景理解和未来预测对于安全控制至关重要。近期研究探索了使用视觉语言模型（VLMs）进行规划，但现有方法通常将感知、预测和规划视为独立模块。我们提出UniDrive-WM，这是一种基于VLM的统一世界模型，能够在单一架构中联合执行驾驶场景理解、轨迹规划和基于轨迹的未来图像生成。UniDrive-WM的轨迹规划器预测未来轨迹，条件化VLM图像生成器以生成合理的未来帧。这些预测提供了额外的监督信号，增强场景理解并迭代细化轨迹生成。我们进一步比较了离散和连续输出表示对未来图像预测的影响，分析其对下游驾驶性能的影响。在具有挑战性的Bench2Drive基准测试中，UniDrive-WM生成了高保真度的未来图像，并在L2轨迹误差和碰撞率方面分别提高了5.9%和9.2%，超过了之前的最佳方法。这些结果表明，将VLM驱动的推理、规划和生成世界建模紧密集成对于自主驾驶的优势。项目页面可在https://unidrive-wm.github.io/UniDrive-WM 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniDrive-WM is a unified VLM-based world model that integrates driving-scene understanding, trajectory planning, and future image generation. It uses a trajectory planner to predict future trajectories, which conditions a VLM to generate plausible future frames. Experiments show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate compared to the previous best method.</div>
<div class="mono" style="margin-top:8px">UniDrive-WM 是一个统一的基于 VLM 的世界模型，集成了驾驶场景理解、轨迹规划和未来图像生成。它使用轨迹规划器预测未来路径，进而条件化 VLM 生成可能的未来帧。实验表明，UniDrive-WM 在 Bench2Drive 基准上的轨迹误差 L2 减少了 5.9%，碰撞率降低了 9.2%，优于之前的最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization</div>
<div class="meta-line">Authors: Xingjian Diao, Zheyuan Liu, Chunhui Zhang, Weiyi Wu, Keyi Kong, Lin Shi, Kaize Ding, Soroush Vosoughi, Jiang Gui</div>
<div class="meta-line">First: 2026-01-07T23:05:17+00:00 · Latest: 2026-01-07T23:05:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过门控感知推理优化解决大型视觉-语言模型中的过度思考问题</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）通过生成逐步推理机制展示了强大的推理能力。然而，这种缓慢思考的方法往往会导致过度思考，模型会对简单查询产生冗长的响应，导致测试时的低效率甚至降低了准确性。先前的工作试图通过自适应推理策略来缓解这一问题，但这些方法大多忽视了一个根本瓶颈：视觉感知失败。我们认为，稳定的推理依赖于低级视觉定位，推理错误通常源自不完美的感知而非不足的思考。为了解决这一限制，我们提出了门控感知推理优化（GPRO），这是一种元推理控制器，在每一步生成中动态地在三条决策路径之间分配计算：一条轻量级的快速路径，一条缓慢的感知路径用于重新审视视觉输入，以及一条缓慢的推理路径用于内部自我反思。为了学习这种区分，我们从大约79万样本中推导出大规模的失败归因监督，使用教师模型区分感知幻觉和推理错误。然后，我们使用多目标强化学习训练控制器，在不确定性下优化任务准确性和计算成本之间的权衡。在五个基准上的实验表明，GPRO在准确性和效率上都有显著改进，优于最近的缓慢思考方法，同时生成的响应也显著更短。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of overthinking in large vision-language models (LVLMs) by proposing Gated Perception-Reasoning Optimization (GPRO), which dynamically routes computation among a fast path, a slow perception path, and a slow reasoning path. The method uses failure attribution supervision to train a meta-reasoning controller and optimizes the trade-off between accuracy and computational cost. Experiments show that GPRO improves both accuracy and efficiency, outperforming recent slow-thinking methods while producing shorter responses.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Gated Perception-Reasoning Optimization (GPRO)的方法，动态地在快速路径、感知路径和推理路径之间分配计算。该方法使用教师模型的失败归因监督来区分感知错误和推理错误，并通过多目标强化学习训练一个元推理控制器。实验表明，GPRO在提高准确性和效率方面表现出色，生成的响应比最近的慢思考方法更短。</div>
</details>
</div>
<div class="card">
<div class="title">3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation</div>
<div class="meta-line">Authors: Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, Keze Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-07T21:23:05+00:00 · Latest: 2026-01-07T21:23:05+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D-Agent：三模态多智能体协作的可扩展3D物体标注</div>
<div class="mono" style="margin-top:8px">受自主驾驶机器人和增强现实应用的驱动，3D物体标注面临着超越2D标注的挑战，包括空间复杂性、遮挡和视角不一致等问题。现有基于单一模型的方法往往难以有效解决这些问题。我们提出了一种名为Tri MARF的新框架，该框架整合了三模态输入，包括2D多视角图像、文本描述和3D点云，并在多智能体协作架构中进行增强，以提高大规模3D标注的效率。Tri MARF包括三个专门的智能体：一个视觉语言模型智能体用于生成多视角描述，一个信息聚合智能体用于选择最优描述，以及一个门控智能体用于将文本语义与3D几何对齐，以实现精细的标注。在Objaverse LVIS、Objaverse XL和ABO上的广泛实验表明，Tri MARF在CLIPScore、检索准确率和吞吐量方面显著优于现有方法，CLIPScore达到88.7，检索准确率分别为ViLT R@5的45.2和43.8，单块NVIDIA A100 GPU上的吞吐量可达每小时12000个物体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of 3D object annotation in autonomous driving and augmented reality by proposing Tri MARF, a framework that integrates 2D multi-view images, textual descriptions, and 3D point clouds. Tri MARF uses three specialized agents to generate multi-view descriptions, select optimal descriptions, and align textual semantics with 3D geometry. Experiments show that Tri MARF outperforms existing methods with a CLIPScore of 88.7, retrieval accuracy of 45.2% and 43.8% on ViLT R at 5, and a throughput of up to 12,000 objects per hour on a single NVIDIA A100 GPU.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出Tri MARF框架解决自动驾驶和增强现实中的3D对象标注挑战，该框架整合了2D多视角图像、文本描述和3D点云。Tri MARF使用三个专门的代理生成多视角描述、选择最优描述并使文本语义与3D几何对齐。实验表明，Tri MARF在CLIPScore上达到88.7，在ViLT R 5的检索准确率为45.2%和43.8%，并且在单个NVIDIA A100 GPU上的吞吐量可达每小时12000个对象。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
