<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-26 03:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251226_0330</div>
    <div class="row"><div class="card">
<div class="title">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</div>
<div class="meta-line">Authors: Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-24T18:59:54+00:00 · Latest: 2025-12-24T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://sytwu.github.io/BeyondMemo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21337v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21337v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sytwu.github.io/BeyondMemo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越记忆：多模态序数回归基准以揭示视觉语言模型中的流行度偏差</div>
<div class="mono" style="margin-top:8px">我们揭示了最先进的视觉语言模型（VLMs）中存在显著的流行度偏差，这些模型在著名建筑上的准确率比普通建筑高出34%，表明它们依赖于记忆而非可泛化的理解。为了系统地研究这一问题，我们引入了该任务上最大的公开基准数据集：YearGuessr数据集，包含来自157个国家的55,546张建筑图像，具有多模态属性，并标注了其建设年份的连续序数标签（1001-2024）、GPS数据以及页面浏览量作为流行度的代理。使用该数据集，我们将建筑年份预测任务框架化为序数回归，并引入了流行度感知的区间准确度指标来量化这种偏差。我们基准测试的30多种模型，包括我们的YearCLIP模型，证实了VLMs在流行、记忆化的项目上表现出色，但在未识别的主题上却面临巨大挑战，揭示了它们推理能力中的关键缺陷。项目页面：https://sytwu.github.io/BeyondMemo/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the significant popularity bias in state-of-the-art vision-language models (VLMs) by introducing the YearGuessr dataset, which includes 55,546 building images with multi-modal attributes and continuous ordinal labels of construction years. The authors frame the task as ordinal regression and introduce new metrics to quantify this bias, confirming that VLMs perform well on popular buildings but poorly on unrecognized subjects, highlighting their reliance on memorization rather than general understanding. Project page: https://sytwu.github.io/BeyondMemo/</div>
<div class="mono" style="margin-top:8px">论文通过引入包含55,546栋建筑物图像和多模态属性的YearGuessr数据集，探讨了最先进的视觉-语言模型（VLMs）中的显著流行度偏差问题。作者将任务定义为序数回归，并引入新的指标来量化这种偏差，证实VLMs在流行建筑上表现良好，但在未识别的主题上表现较差，突显了它们依赖记忆而非泛化理解的问题。项目页面: https://sytwu.github.io/BeyondMemo/</div>
</details>
</div>
<div class="card">
<div class="title">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</div>
<div class="meta-line">Authors: Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2025-12-24T15:36:21+00:00 · Latest: 2025-12-24T15:36:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21243v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lookplangraph.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LookPlanGraph：基于VLM图增强的体感指令跟随方法</div>
<div class="mono" style="margin-top:8px">使用大型语言模型（LLM）作为规划器的方法在体感指令跟随任务中变得普遍。为了成功完成任务，LLM 必须在机器人操作的环境中得到接地。一种解决方案是使用包含所有必要信息的场景图。现代方法依赖于预先构建的场景图，并假设在规划开始时所有任务相关信息都已可用。然而，这些方法没有考虑到在图构建和任务执行之间环境可能发生的改变。我们提出了 LookPlanGraph 方法，该方法利用由静态资产和对象先验组成的场景图。在计划执行过程中，LookPlanGraph 会通过验证现有先验或发现新实体来不断更新图，这通过使用视觉语言模型处理代理的主观摄像机视图来实现。我们在具有改变对象位置的 VirtualHome 和 OmniGibson 模拟环境中进行了实验，证明了 LookPlanGraph 在基于预定义静态场景图的方法中表现出色。为了展示我们方法的实际适用性，我们还在现实世界中进行了实验。此外，我们还引入了 GraSIF（用于指令跟随的图场景）数据集，该数据集包含自动验证框架，包括从 SayPlan Office、BEHAVIOR-1K 和 VirtualHome RobotHow 中抽取的 514 个任务。项目页面可在 https://lookplangraph.github.io 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve embodied instruction following by addressing the limitations of methods that rely on static scene graphs. LookPlanGraph uses a scene graph augmented with object priors and continuously updates it during execution by processing the agent&#x27;s egocentric view with a Vision Language Model. Experiments in simulated and real-world environments show that LookPlanGraph outperforms methods using predefined static scene graphs, especially when the environment changes.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决使用静态场景图的局限性，即无法应对环境变化，来提升基于指令的机器人任务执行能力。LookPlanGraph 使用包含物体先验信息的场景图，并在任务执行过程中通过视觉语言模型处理代理的主观视角来不断更新场景图。实验在模拟和真实环境中表明，LookPlanGraph 的表现优于依赖预定义静态场景图的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</div>
<div class="meta-line">Authors: Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen</div>
<div class="meta-line">Venue: MM</div>
<div class="meta-line">First: 2025-12-24T15:02:33+00:00 · Latest: 2025-12-24T15:02:33+00:00</div>
<div class="meta-line">Comments: System description paper for EVENTA Grand Challenge Track 2 at ACM Multimedia 2025 (MM &#x27;25). Ranked 4th place. 6 pages, 1 figure, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21221v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21221v1">PDF</a> · <a href="https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用轻量级实体提取实现可扩展的基于事件的图像检索</div>
<div class="mono" style="margin-top:8px">从自然语言描述中检索图像是一项核心任务，位于计算机视觉和自然语言处理的交叉领域，广泛应用于搜索引擎、媒体归档和数字内容管理中。然而，由于模糊或依赖上下文的查询、语言的多样性以及需要可扩展的解决方案，现实世界中的图像-文本检索仍然具有挑战性。在本文中，我们提出了一种轻量级的两阶段检索管道，利用事件中心的实体提取来结合现实世界标题中的时间与上下文信号。第一阶段使用基于显著实体的BM25高效候选过滤，第二阶段应用BEiT-3模型来捕捉深层的多模态语义并重新排序结果。在OpenEvents v1基准上评估，我们的方法达到了0.559的平均精度，显著优于先前的基线。这些结果突显了结合事件引导的过滤与长文本视觉语言建模在复杂现实场景中实现准确高效检索的有效性。我们的代码可在https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of retrieving images from natural language descriptions by proposing a lightweight two-stage retrieval pipeline. The first stage uses BM25 based on salient entities for efficient candidate filtering, and the second stage employs BEiT-3 models to capture deep multimodal semantics and rerank results. The method achieves a mean average precision of 0.559 on the OpenEvents v1 benchmark, significantly outperforming previous approaches, demonstrating the effectiveness of integrating event-guided filtering with long-text vision-language modeling for accurate and efficient image retrieval in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本文提出了一种轻量级的两阶段检索管道，以解决从自然语言描述中检索图像的挑战。第一阶段使用基于显著实体的BM25进行高效的候选过滤，第二阶段则使用BEiT-3模型捕获深度多模态语义并重新排序结果。该方法在OpenEvents v1基准测试中实现了0.559的平均精度，显著优于先前的方法，展示了结合事件引导过滤与长文本视觉语言建模在复杂现实场景中实现准确高效检索的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic</div>
<div class="meta-line">Authors: Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu</div>
<div class="meta-line">First: 2025-12-24T15:01:26+00:00 · Latest: 2025-12-24T15:01:26+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21220v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent&#x27;s multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSafe：通过可执行的安全逻辑保护具身代理</div>
<div class="mono" style="margin-top:8px">由视觉-语言模型（VLMs）驱动的具身代理越来越能够执行复杂的现实世界任务，但它们仍然容易受到可能导致不安全行为的危险指令的影响。运行时安全护栏可以在任务执行过程中拦截危险行为，提供了一种有前景的解决方案，因为它们具有灵活性。然而，现有的防御措施往往依赖于静态规则过滤或提示级控制，难以应对动态、时间依赖性和语境丰富的环境中隐含的风险。为了解决这个问题，我们提出了一种名为RoboSafe的混合推理运行时保护，通过可执行的谓词基础安全逻辑为具身代理提供保护。RoboSafe结合了在混合长短期安全记忆上的两种互补推理过程。我们首先提出了一种后向反思推理模块，该模块不断回顾短期记忆中的最近轨迹，以推断时间安全谓词，并在检测到违规行为时主动触发重新规划。然后，我们提出了一种前瞻预测推理模块，该模块通过生成基于长期安全记忆和代理的多模态观察的安全谓词来预见即将出现的风险。这些组件共同形成了一个适应性强、可验证的安全逻辑，既可解释又可作为代码执行。在多个代理的广泛实验中，RoboSafe与领先基准相比显著减少了危险行为（风险发生率降低36.8%），同时保持了接近原始的任务性能。在物理机器人手臂上的实际评估进一步证实了其实用性。代码将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSafe is a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. It integrates Backward Reflective Reasoning and Forward Predictive Reasoning on a Hybrid Long-Short Safety Memory to continuously infer and anticipate safety predicates, respectively. Experiments show that RoboSafe reduces hazardous actions by 36.8% compared to leading baselines while maintaining task performance, and real-world evaluations on robotic arms confirm its practicality.</div>
<div class="mono" style="margin-top:8px">RoboSafe 通过使用可执行的安全逻辑来保护实体代理免受危险指令的影响，结合了向后反思推理以持续监控近期行为的安全性以及前瞻预测推理以预见未来风险。实验表明，RoboSafe 将危险行为减少了 36.8%，同时保持了类似的任务性能。实际世界中的机器人手臂评估进一步证实了其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</div>
<div class="meta-line">Authors: Brigitta Malagurski Törtei, Yasser Dahou, Ngoc Dung Huynh, Wamiq Reyaz Para, Phúc H. Lê Khac, Ankit Singh, Sofian Chaybouti, Sanath Narayan</div>
<div class="meta-line">First: 2025-12-24T14:18:38+00:00 · Latest: 2025-12-24T14:18:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21194v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisRes 基准：关于评估 VLM 视觉推理能力的研究</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在视觉问答和图像描述等任务上取得了显著进展。然而，这些模型在视觉推理方面的表现与其依赖语言先验的程度仍然不清楚。为了解决这个问题，我们引入了 VisRes 基准，该基准旨在在无需上下文语言监督的自然环境中研究视觉推理。通过对三种复杂性级别的模型行为进行分析，我们发现了感知和关系视觉推理能力的明显局限性。VisRes 在其级别上隔离了不同的推理能力。第一级测试在模糊、纹理变化、遮挡和旋转等干扰下的感知完成和全局图像匹配；第二级测试单一属性（如颜色、数量、方向）的基于规则的推理；第三级则针对需要整合多个视觉属性的组合推理。在超过 19,000 张受控任务图像中，我们发现最先进的 VLMs 在微妙的感知干扰下表现接近随机，揭示了其有限的抽象能力，仅限于模式识别。最后，我们讨论了 VisRes 如何为多模态研究中的抽象视觉推理提供统一框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate the visual reasoning capabilities of Vision-Language Models (VLMs) by introducing VisRes Bench, a benchmark that tests models in naturalistic settings without language supervision. The study analyzes model behavior across three levels of complexity: perceptual completion, rule-based inference, and compositional reasoning. Key findings show that state-of-the-art VLMs struggle with subtle perceptual perturbations, indicating limited abstraction beyond pattern recognition. This work highlights the need for improving abstract visual reasoning in VLMs.</div>
<div class="mono" style="margin-top:8px">VisRes Bench 是一个基准，旨在评估 Vision-Language 模型（VLMs）在无需依赖上下文语言监督的情况下进行视觉推理的能力。该基准包括三个复杂度级别：感知完成和全局图像匹配（Level 1）、单一属性的规则推理（Level 2）和需要整合多个视觉属性的组合推理（Level 3）。在超过 19,000 张受控任务图像的研究中，发现最先进的 VLMs 在细微的感知扰动下表现不佳，显示出有限的抽象能力，仅限于模式识别，并且依赖于语言先验进行视觉推理任务。</div>
</details>
</div>
<div class="card">
<div class="title">SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</div>
<div class="meta-line">Authors: Xiaojun Guo, Runyu Zhou, Yifei Wang, Qi Zhang, Chenheng Zhang, Stefanie Jegelka, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Yisen Wang</div>
<div class="meta-line">First: 2025-10-18T09:22:40+00:00 · Latest: 2025-12-24T13:40:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16416v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.16416v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework&#x27;s generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSL4RL：重新审视自我监督学习作为视觉-语言推理内在奖励的方法</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过结合大型语言模型和视觉输入展示了显著的能力。然而，它们往往未能充分利用视觉证据，要么依赖于视觉中心任务中的语言先验，要么在推理过程中求助于文本捷径。尽管强化学习（RL）可以将模型与期望的行为对齐，但将其应用于VLMs受到了缺乏可扩展和可靠的奖励机制的阻碍。为克服这一挑战，我们提出了一种名为SSL4RL的新框架，该框架利用自我监督学习（SSL）任务作为RL基础微调的验证奖励来源。我们的方法将SSL目标，如预测图像旋转或重建遮罩片段，重新表述为密集的自动奖励信号，从而消除了对人类偏好数据或不可靠的人工智能评估者的需要。实验表明，SSL4RL在视觉中心和视觉-语言推理基准测试中显著提高了性能。此外，通过系统性的消融实验，我们确定了影响SSL4RL任务有效性的关键因素，如任务难度、模型规模和与目标领域的语义对齐，为未来的工作提供了新的设计原则。我们还通过将其应用于图学习，展示了该框架的通用性，其中它带来了显著的收益。SSL4RL建立了一种灵活且有效的范式，用于使用可验证的自我监督目标对多模态模型进行对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the performance of vision-language models by integrating self-supervised learning (SSL) tasks as intrinsic rewards in reinforcement learning (RL). The proposed SSL4RL framework converts SSL objectives into dense, automatic reward signals, improving both vision-centric and vision-language reasoning benchmarks. Key factors influencing the effectiveness of SSL4RL include task difficulty, model scale, and semantic alignment with the target domain, providing new design principles for future work.</div>
<div class="mono" style="margin-top:8px">论文提出了SSL4RL框架，利用自我监督学习(SSL)任务作为强化学习(RL)微调视觉语言模型(VLM)的内在奖励。这种方法将SSL目标重新表述为密集的自动奖励信号，提高了视觉中心和视觉语言推理基准上的性能。影响SSL4RL任务有效性的关键因素包括任务难度、模型规模和与目标领域的语义对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction</div>
<div class="meta-line">Authors: Yunheng Li, Yuxuan Li, Quansheng Zeng, Wenhai Wang, Qibin Hou, Ming-Ming Cheng</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2024-12-09T06:34:23+00:00 · Latest: 2025-12-24T13:11:11+00:00</div>
<div class="meta-line">Comments: Accepted at ICCV 2025. The code is available at https://github.com/HVision-NKU/DenseVLM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.06244v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.06244v4">PDF</a> · <a href="https://github.com/HVision-NKU/DenseVLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot recognition capability, but still underperform in dense prediction tasks. Self-distillation recently is emerging as a promising approach for fine-tuning VLMs to better adapt to local regions without requiring extensive annotations. However, previous state-of-the-art approaches often suffer from significant `foreground bias&#x27;, where models tend to wrongly identify background regions as foreground objects. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. DenseVLM leverages the pre-trained VLM to retrieve categories for unlabeled regions and then decouples the interference between foreground and background features. We show that DenseVLM can directly replace the original VLM in open-vocabulary object detection and image segmentation methods, leading to notable performance improvements. Furthermore, it exhibits promising zero-shot scalability when training on more extensive and diverse datasets. Our code is available at https://github.com/HVision-NKU/DenseVLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无偏区域-语言对齐以实现开放词汇密集预测</div>
<div class="mono" style="margin-top:8px">预训练的视觉-语言模型（VLMs），如CLIP，展示了令人印象深刻的零样本识别能力，但在密集预测任务中仍然表现不佳。最近，自我蒸馏作为一种有希望的方法正在兴起，用于微调VLM以更好地适应局部区域，而无需大量注释。然而，之前最先进的方法往往遭受显著的“前景偏差”问题，模型倾向于错误地将背景区域识别为前景对象。为了解决这一问题，我们提出了一种名为DenseVLM的框架，旨在从强大的预训练VLM表示中学习无偏的区域-语言对齐。DenseVLM利用预训练的VLM检索未标记区域的类别，然后分离前景和背景特征之间的干扰。我们展示了DenseVLM可以直接替换开放词汇目标检测和图像分割方法中的原始VLM，从而显著提高性能。此外，当在更广泛和多样化的数据集上进行训练时，它还表现出有希望的零样本扩展性。我们的代码可在https://github.com/HVision-NKU/DenseVLM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the performance of pre-trained vision-language models (VLMs) in dense prediction tasks by addressing the foreground bias issue. DenseVLM is proposed to learn unbiased region-language alignment from pre-trained VLMs, decoupling foreground and background features. The method directly replaces the original VLM in open-vocabulary object detection and image segmentation, achieving significant performance improvements and demonstrating promising zero-shot scalability with more diverse datasets.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决前景偏差问题，提高预训练视觉-语言模型（VLMs）在密集预测任务中的性能。提出了DenseVLM框架，利用自蒸馏技术学习无偏的区域-语言对齐。该方法利用预训练的VLM检索未标注区域的类别，并分离前景和背景特征，从而在开放词汇量的目标检测和图像分割任务中表现出更好的性能，无需大量标注。此外，它在更广泛的多样数据集上也显示出良好的零样本扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">ORCA: Object Recognition and Comprehension for Archiving Marine Species</div>
<div class="meta-line">Authors: Yuk-Kwan Wong, Haixin Liang, Zeyu Ma, Yiwei Chen, Ziqiang Zheng, Rinaldi Gotama, Pascal Sebastian, Lauren D. Sparks, Sai-Kit Yeung</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2025-12-24T12:36:57+00:00 · Latest: 2025-12-24T12:36:57+00:00</div>
<div class="meta-line">Comments: Accepted by The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21150v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORCA：海洋物种识别与理解以实现海洋生物存档</div>
<div class="mono" style="margin-top:8px">海洋视觉理解对于监测和保护海洋生态系统至关重要，能够实现自动化的生物调查。然而，由于训练数据有限且缺乏将特定海洋领域的挑战与明确的计算机视觉任务系统化结合的任务表述，进展受到限制，从而限制了有效模型的应用。为解决这一问题，我们提出了ORCA，一个包含14,647张图像和478个物种的多模态基准数据集，其中包含42,217个边界框注释和22,321个专家验证的实例描述。该数据集提供了细粒度的视觉和文本注释，捕捉了不同海洋物种的形态特征。为了促进方法学的进步，我们在三个任务上评估了18个最先进的模型：对象检测（封闭集和开放词汇）、实例描述和视觉定位。结果突显了关键挑战，包括物种多样性、形态重叠和专门领域的特殊需求，强调了海洋理解的难度。ORCA因此建立了一个全面的基准，以推进海洋领域的研究。项目页面：http://orca.hkustvgd.com/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve marine ecosystem monitoring through automatic and scalable biological surveys by addressing the challenges of limited training data and lack of systematic task formulation. ORCA, a multi-modal benchmark, is introduced with 14,647 images and detailed annotations for 478 marine species. The evaluation of 18 state-of-the-art models on tasks like object detection, instance captioning, and visual grounding highlights key challenges such as species diversity and morphological overlap, underscoring the complexity of marine understanding. This benchmark is expected to advance research in the marine domain by providing a comprehensive dataset and evaluation framework.</div>
<div class="mono" style="margin-top:8px">论文介绍了ORCA，这是一个多模态基准，用于海洋物种识别和理解，旨在解决海洋视觉理解中训练数据不足和系统任务缺乏的问题。该基准包含14,647张带有详细注释的图像，并对18种最先进的模型进行了对象检测、实例描述和视觉定位三项任务的评估。结果揭示了物种多样性、形态重叠等挑战，突显了海洋理解的复杂性，并强调了进一步研究的必要性。项目主页: http://orca.hkustvgd.com/</div>
</details>
</div>
<div class="card">
<div class="title">Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</div>
<div class="meta-line">Authors: Yupeng Zhang, Adam G. Dunn, Usman Naseem, Jinman Kim</div>
<div class="meta-line">First: 2025-12-17T09:47:29+00:00 · Latest: 2025-12-24T12:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15249v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15249v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model&#x27;s decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型在医学图像疾病分类中的交叉公平性</div>
<div class="mono" style="margin-top:8px">医学人工智能（AI）系统，尤其是多模态视觉语言模型（VLM），常常表现出交叉偏见，模型在诊断边缘化患者子群体时系统性地缺乏信心。这种偏见可能导致由于样本数据的种族分布偏差和诊断确定性分布差异而出现更高的误诊和漏诊率。当前的公平性干预措施往往未能解决这些差距，或者在实现子群体统计平等的同时牺牲整体诊断性能。在本研究中，我们开发了跨模态一致性匹配（CMAC-MMD）训练框架，以标准化交叉边缘化患者子群体的诊断确定性。与传统的去偏见方法不同，该方法在临床推理过程中不需要敏感的种族数据即可使模型的决策信心相等。我们使用10,015张皮肤病变图像（HAM10000）进行了评估，并通过外部验证使用了12,000张图像（BCN20000）和10,000张用于青光眼检测的视网膜图像（Harvard-FairVLMed），按交叉年龄、性别和种族属性分层评估性能。在皮肤科队列中，所提出的方法将总体交叉误诊差距（真实阳性率差异，ΔTPR）从0.50降低到0.26，同时将总体曲线下面积（AUC）从0.94提高到0.97，优于标准训练。同样，在青光眼筛查中，该方法将ΔTPR从0.41降低到0.31，实现了更好的AUC（0.72，与0.71基线相比）。这建立了一个可扩展的框架，用于开发既准确又能在不同患者子群体中公平执行的高风险临床决策支持系统，确保可靠性能而不增加隐私风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses intersectional biases in medical AI systems, particularly in vision-language models used for disease classification. It introduces Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardizes diagnostic certainty across different patient subgroups without requiring sensitive demographic data. Evaluations on skin lesion and fundus images showed that the proposed method reduced the intersectional missed diagnosis gap and improved overall diagnostic performance, achieving better Area Under the Curve (AUC) scores compared to standard training methods.</div>
<div class="mono" style="margin-top:8px">该研究针对医疗AI系统中的交集偏见问题，特别是在疾病分类的视觉-语言模型中。研究引入了跨模态一致性对齐（CMAC-MMD）的训练框架，该框架能够在不需要敏感人口统计数据的情况下，使不同患者亚组的诊断确定性标准化。在皮肤病变和视网膜图像上的评估表明，所提出的方法减少了漏诊差距，并提高了整体诊断性能，AUC分数优于标准训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">MarineEval: Assessing the Marine Intelligence of Vision-Language Models</div>
<div class="meta-line">Authors: YuK-Kwan Wong, Tuan-An To, Jipeng Zhang, Ziqiang Zheng, Sai-Kit Yeung</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2025-12-24T11:57:50+00:00 · Latest: 2025-12-24T11:57:50+00:00</div>
<div class="meta-line">Comments: Accepted by The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21126v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MarineEval：评估视觉语言模型的海洋智能</div>
<div class="mono" style="margin-top:8px">我们见证了由大型语言模型（LLMs）和进一步的视觉语言模型（VLMs）引领的在处理各种查询方面的进展，使其成为通用助手。VLMs 作为连接视觉世界和语言语料库的桥梁，接收视觉内容和各种文本指令以生成相应的响应。尽管 VLMs 在各个领域取得了巨大成功，但在本文中，我们质疑现有的 VLMs 是否可以作为领域专家，准确回答需要大量领域专业知识和解决特殊领域挑战/要求的海洋问题。为了全面评估现有 VLMs 的效果并探索其边界，我们构建了第一个大规模海洋 VLM 数据集和基准 MarineEval，包含 2,000 个基于图像的问题-答案对。在数据集构建过程中，我们确保了构建数据的多样性和覆盖面：7 个任务维度和 20 个能力维度。领域要求特别整合到数据构建中，并由相应的海洋领域专家进一步验证。我们在 MarineEval 上全面基准测试了 17 个现有 VLMs，并调查了现有模型在回答海洋研究问题方面的局限性。实验结果表明，现有 VLMs 无法有效回答领域特定问题，仍有很大的性能提升空间。我们希望我们的新基准和观察结果能促进未来的研究。项目页面：http://marineeval.hkustvgd.com/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MarineEval is a large-scale dataset and benchmark for evaluating the marine intelligence of vision-language models (VLMs). The dataset includes 2,000 image-based question-answering pairs designed to test the models&#x27; ability to answer marine-specific questions. The benchmark reveals that current VLMs struggle to accurately answer domain-specific marine questions, indicating significant room for improvement in this area. The study aims to facilitate future research by providing a comprehensive evaluation framework for VLMs in marine contexts.</div>
<div class="mono" style="margin-top:8px">MarineEval 通过构建包含 2,000 个基于图像的问题-答案对的大规模数据集，涵盖 7 个任务维度和 20 个能力维度，评估视觉语言模型的海洋智能。评估结果显示现有的视觉语言模型难以准确回答特定的海洋问题，表明它们在领域专业知识和性能方面仍有很大的提升空间。本研究旨在促进该领域的未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters</div>
<div class="meta-line">Authors: Yongkun Du, Zhineng Chen, Yazhen Xie, Weikang Baiand Hao Feng, Wei Shi, Yuchen Su, Can Huang, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-12-24T10:35:21+00:00 · Latest: 2025-12-24T10:35:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21095v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21095v1">PDF</a> · <a href="https://github.com/Topdu/OpenOCR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniRec-0.1B: 统一文本和公式识别模型，参数量仅为0.1B</div>
<div class="mono" style="margin-top:8px">文本和公式是许多文档的核心信息组件。准确高效地识别两者对于开发稳健且通用的文档解析系统至关重要。最近，视觉-语言模型（VLMs）在统一识别文本和公式方面取得了令人印象深刻的成果。然而，它们体积庞大且计算需求高，限制了它们在许多应用中的使用。在本文中，我们提出了一种仅包含0.1B参数的统一识别模型UniRec-0.1B。该模型能够在字符、单词、行、段落和文档等多个层次上执行文本和公式识别。为了实现这一任务，我们首先建立了UniRec40M，这是一个包含4000万文本、公式及其混合样本的大规模数据集，使我们能够训练出强大而轻量级的模型。其次，我们识别了构建这样一个轻量级但统一专家模型时的两个挑战：层次结构中的结构变异性以及文本和公式内容之间的语义纠缠。为了解决这些问题，我们引入了一种层次监督训练方法，明确指导结构理解，并引入了一种语义解耦分词器，将文本和公式表示分离。最后，我们开发了一个全面的评估基准，涵盖了多个领域和多个层次的中文和英文文档。在该基准和公开基准上的实验结果表明，UniRec-0.1B 在性能上优于通用视觉语言模型和领先文档解析专家模型，同时实现了2-9倍的速度提升，验证了其有效性和效率。代码库和数据集：https://github.com/Topdu/OpenOCR.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces UniRec-0.1B, a unified text and formula recognition model with only 0.1 billion parameters, which can perform recognition at multiple levels. The authors address challenges such as structural variability and semantic entanglement by proposing hierarchical supervision and a semantic-decoupled tokenizer. Experimental results show that UniRec-0.1B outperforms both general-purpose vision-language models and document parsing expert models, achieving a 2-9 times speedup.</div>
<div class="mono" style="margin-top:8px">该论文介绍了UniRec-0.1B，这是一种仅含0.1亿参数的统一文本和公式识别模型，能够进行多层级的识别。作者创建了大规模数据集UniRec40M，并解决了结构变异性和语义纠缠等挑战。该模型在识别准确性和效率上均优于通用视觉语言模型和专门的文档解析模型。关键发现包括相比现有模型，其识别准确性和效率均有显著提升。代码和数据集可在GitHub上获取。</div>
</details>
</div>
<div class="card">
<div class="title">Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction</div>
<div class="meta-line">Authors: Gangxiong Zhang, Yongchao Long, Yong Zhang, Yuxi Zhou, Shenda Hong</div>
<div class="meta-line">First: 2025-12-17T12:29:53+00:00 · Latest: 2025-12-24T08:34:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19735v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于病例提示以减轻大型语言模型偏差以降低ICU病死率预测</div>
<div class="mono" style="margin-top:8px">ICU患者准确的病死率预测对于临床决策至关重要。尽管大型语言模型（LLMs）在预测结构化医疗数据的结果方面显示出潜力，但它们的预测可能表现出与性别、年龄和种族相关的统计偏差，限制了其在临床实践中的可信应用。现有的去偏方法往往降低了预测性能，使得难以同时优化公平性和准确性。在本研究中，我们系统地检查了LLM基于ICU病死率预测中的偏差，并提出了一种无需训练、临床适应性的提示框架，以同时提高公平性和性能。我们首先开发了一种多维度的偏差评估方案，用于全面的模型诊断。在此分析的基础上，我们引入了CAse Prompting（CAP），这是一种新颖的提示框架，将传统的去偏提示与案例推理相结合。CAP引导模型从历史上的错误预测案例及其正确结果中学习，从而纠正了有偏的推理模式。在MIMIC-IV数据集上的实验表明，CAP显著提高了预测准确性和公平性。CAP将AUROC从0.806提高到0.873，AUPRC从0.497提高到0.694，并通过超过90%的减少性别和种族相关的差异。特征依赖性分析进一步表明，不同人口统计学组之间的注意力模式高度一致，相似度分数超过0.98。这些结果表明，LLMs在ICU病死率预测中表现出可测量的偏差，并且精心设计的提示框架可以在无需重新训练的情况下有效协同优化公平性和性能，提供了一个可转移的公平临床决策支持范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the issue of demographic biases in large language models (LLMs) used for ICU mortality prediction. It proposes a training-free prompting framework called CAse Prompting (CAP) to improve both fairness and predictive accuracy. The method involves developing a multi-dimensional bias assessment scheme and integrating conventional debiasing prompts with case-based reasoning. Experiments on the MIMIC-IV dataset show that CAP significantly enhances AUROC and AUPRC, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis also indicates consistent attention patterns across demographic groups, suggesting that CAP effectively mitigates bias without retraining the model.</div>
<div class="mono" style="margin-top:8px">该研究针对用于预测ICU死亡率的大语言模型（LLMs）中存在的偏差问题，这些偏差可能影响临床决策。作者提出了一种名为CAse Prompting（CAP）的无训练提示框架，该框架结合了传统的去偏提示和案例推理，以纠正偏差的推理模式。实验结果表明，CAP在提高预测准确性和公平性方面表现出色，AUROC和AUPRC分数显著提高，同时减少了性别和种族相关的差异。</div>
</details>
</div>
<div class="card">
<div class="title">O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</div>
<div class="meta-line">Authors: Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-18T11:18:08+00:00 · Latest: 2025-12-24T08:17:05+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14368v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14368v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>O3SLM：开放权重、开放数据和开放词汇量草图语言模型</div>
<div class="mono" style="margin-top:8px">虽然大型视觉语言模型（LVLMs）在越来越多的实际应用中被部署，但它们对抽象视觉输入的理解能力仍然有限。具体来说，它们难以理解手绘草图，这种模态提供了一种直观的方式来表达难以用文字描述的概念。我们确定的主要瓶颈是没有一个大规模的数据集可以同时建模草图、照片级真实图像及其相应的自然语言指令。为了解决这个问题，我们提出了两个关键贡献：（1）一个新设计的、大规模的图像-草图-指令三元组数据集，旨在促进预训练和指令调优；（2）O3SLM，一个在该数据集上训练的LVLM。在多个基于草图的任务上的全面评估：（a）物体定位，（b）计数，（c）图像检索，即（SBIR和细粒度SBIR），以及（d）视觉问答（VQA），结合现有的三个草图数据集，即QuickDraw！、Sketchy和Tu Berlin，以及我们生成的SketchVCL数据集，表明O3SLM达到了最先进的性能，显著优于现有的LVLMs在草图理解和推理方面的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance Large Vision Language Models (LVLMs) to better interpret abstract visual inputs like hand-drawn sketches. The authors introduce a new large-scale dataset, O3SLM, which includes image-sketch-instruction triplets for pretraining and instruction tuning. Experiments on tasks such as object localization, counting, image retrieval, and visual question answering demonstrate that O3SLM outperforms existing LVLMs in understanding and reasoning about sketches, achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">研究旨在提升大型视觉语言模型对手绘草图的解释能力，这是当前模型面临的挑战。为此，作者引入了一个新的大规模数据集O3SLM，其中包括图像-草图-指令三元组。实验结果表明，O3SLM在对象定位、计数、图像检索和视觉问答等任务中表现优异，特别是在草图理解和推理方面超越了现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</div>
<div class="meta-line">Authors: Donghyuk Kim, Sejeong Yang, Wonjin Shin, Joo-Young Kim</div>
<div class="meta-line">First: 2025-12-13T11:02:04+00:00 · Latest: 2025-12-24T07:46:59+00:00</div>
<div class="meta-line">Comments: 14 pages, 20 figures, conference, accepted by HPCA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12284v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.12284v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-Rex：通过动态KV缓存检索实现实时流式视频LLM加速</div>
<div class="mono" style="margin-top:8px">流式视频大型语言模型（LLMs）越来越多地用于实时多模态任务，如视频字幕、问答、对话代理和增强现实。然而，这些模型面临着根本性的内存和计算挑战，因为它们的键值（KV）缓存会随着连续的流式视频输入而显著增长。这一过程需要一个迭代预填充阶段，这是流式视频LLMs的一个独特特征。由于其迭代预填充阶段，它遭受了显著的限制，包括大量的计算、大量的数据传输和准确性的下降。至关重要的是，这个问题在边缘部署中被进一步放大，这是这些模型的主要目标。
  在这项工作中，我们提出了V-Rex，这是第一个软件和硬件协同设计的加速器，全面解决了流式视频LLM推理中的算法和硬件瓶颈。V-Rex的核心是引入了ReSV，这是一种无需训练的动态KV缓存检索算法。ReSV利用基于时间和空间相似性的令牌聚类来减少视频帧之间的冗余KV缓存内存。为了充分利用这些算法上的优势，V-Rex提供了一个紧凑的、低延迟的硬件加速器，其中包括一个动态KV缓存检索引擎（DRE），具有位级和早期退出的计算单元。V-Rex在边缘部署中实现了前所未有的实时性能（3.9-8.3 FPS）和高效的流式视频LLM推理，几乎没有任何准确性的损失。虽然DRE仅占2.2%的功耗和2.0%的面积，但该系统在功耗和能效上分别比AGX Orin GPU提高了1.9-19.7倍和3.1-18.5倍。这项工作首次全面解决了算法和硬件中的KV缓存检索问题，使实时流式视频LLM推理能够在资源受限的边缘设备上实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">V-Rex is a software-hardware co-designed accelerator for real-time streaming video LLM inference, addressing memory and computational challenges by introducing ReSV, a dynamic KV cache retrieval algorithm. ReSV reduces KV cache memory usage through token clustering, and V-Rex offers a compact hardware accelerator with a dynamic KV cache retrieval engine (DRE) for low-latency processing. V-Rex achieves 3.9-8.3 FPS and 1.9-19.7x speedup with negligible accuracy loss on edge devices, demonstrating significant energy efficiency improvements over AGX Orin GPU.</div>
<div class="mono" style="margin-top:8px">V-Rex 是一种软件硬件协同设计的加速器，用于实时流式视频 LLM 推断，通过引入基于 token 聚类的动态 KV 缓存检索算法 ReSV 来解决内存和计算挑战。ReSV 通过 token 聚类减少 KV 缓存的内存使用，V-Rex 提供了一个紧凑的硬件加速器，带有动态 KV 缓存检索引擎 (DRE)，实现低延迟处理。V-Rex 达到 3.9-8.3 FPS，并在边缘设备上实现 1.9-19.7 倍的加速和显著的能效提升，优于 AGX Orin GPU。</div>
</details>
</div>
<div class="card">
<div class="title">Generalization of Diffusion Models Arises with a Balanced Representation Space</div>
<div class="meta-line">Authors: Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu</div>
<div class="meta-line">First: 2025-12-24T05:40:40+00:00 · Latest: 2025-12-24T05:40:40+00:00</div>
<div class="meta-line">Comments: 40 pages, 19 figures. The first two authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20963v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized &quot;spiky&quot; representations, whereas (ii) generalization arises when the model captures local data statistics, producing &quot;balanced&quot; representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的泛化能力源于平衡的表示空间</div>
<div class="mono" style="margin-top:8px">扩散模型在生成高质量、多样化样本方面表现出色，但当过度拟合训练目标时，它们可能会记住训练数据。我们通过表示学习的视角分析扩散模型中的记忆与泛化之间的区别。通过研究两层ReLU去噪自编码器（DAE），我们证明了(i) 记忆对应于模型在编码和解码的学得权重中存储原始训练样本，产生局部的“尖峰”表示；而(ii) 泛化则发生在模型捕捉局部数据统计时，产生“平衡”的表示。此外，我们在现实世界的无条件和文本到图像扩散模型上验证了这些理论发现，展示了这些表示结构在深度生成模型中的重要实践意义。基于这些见解，我们提出了一种基于表示的检测记忆的方法以及一种无需训练的编辑技术，允许通过表示引导实现精确控制。我们的结果共同强调了学习良好表示对于新颖和有意义的生成建模至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the generalization and memorization in diffusion models by analyzing a two-layer ReLU denoising autoencoder. It shows that memorization leads to localized &#x27;spiky&#x27; representations, while generalization results in &#x27;balanced&#x27; representations. The research validates these findings on real-world models and proposes a method for detecting memorization and a training-free editing technique. The key insight is that learning good representations is crucial for meaningful generative modeling.</div>
<div class="mono" style="margin-top:8px">该研究通过分析两层ReLU去噪自编码器，探讨了扩散模型中的记忆和泛化现象。研究显示，记忆导致局部‘尖峰’表示，而泛化则产生‘平衡’表示。该研究在实际模型上验证了这些发现，并提出了一种检测记忆的方法和一种无需训练的编辑技术。关键见解是，学习良好的表示对于生成有意义的内容至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning</div>
<div class="meta-line">Authors: Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy</div>
<div class="meta-line">First: 2025-12-24T04:30:21+00:00 · Latest: 2025-12-24T04:30:21+00:00</div>
<div class="meta-line">Comments: Project Website: https://transductive-visualprogram.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20934v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://transductive-visualprogram.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>归纳视觉编程：从经验演化工具库以进行空间推理</div>
<div class="mono" style="margin-top:8px">在3D场景中的空间推理需要精确的几何计算，这挑战了视觉语言模型的能力。视觉编程通过将问题分解为步骤并调用专门的工具来解决这一问题，但现有方法要么依赖固定的工具集，要么在解决问题之前进行推测性的工具诱导，导致生成的程序效果不佳且工具利用不足。我们提出了一种新的框架——归纳视觉编程（TVP），该框架能够从自身经验中构建新的工具，而不是基于推测。TVP 首先使用基本工具解决问题，同时将经验性解决方案积累到示例库中，然后从这些程序中抽象出重复出现的模式，形成可重用的高级工具，从而构建一个不断演化的工具库。这使得TVP能够利用从经验中学到的越来越强大的工具来解决新问题。在Omni3D-Bench上，TVP达到了最先进的性能，比GPT-4o高出22%，比之前最好的视觉编程系统高出11%。我们归纳学习到的工具比推测生成的工具更常被用作核心程序依赖，使用频率高5倍，显示出更有效的工具发现和重用。演化出的工具还表现出强大的泛化能力，无需对测试集进行任何修改，就在SpatialScore-Hard集合的基准测试中取得了优异的性能。我们的工作确立了经验驱动的归纳工具创建作为构建自我演化的视觉编程代理的强大范式，这些代理能够有效应对具有挑战性的空间推理任务。我们将在https://transductive-visualprogram.github.io/发布我们的代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve spatial reasoning in 3D scenes by developing a framework called Transductive Visual Programming (TVP) that evolves tool libraries from experience. TVP first solves problems using basic tools and accumulates solutions into an Example Library, then abstracts recurring patterns into reusable higher-level tools for an evolving Tool Library. On the Omni3D-Bench, TVP outperforms GPT-4o by 22% and the previous best visual programming system by 11%, with transductively learned tools being used 5x more frequently as core program dependencies and showing strong generalization to unseen spatial tasks without modification. This work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents for spatial reasoning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过开发一种称为Transductive Visual Programming (TVP)的框架来改进3D场景中的空间推理，该框架从经验中演化工具库。TVP使用基本工具解决问题并将解决方案收集到Example Library中，然后从这些模式中抽象出可重用的更高层次工具以形成一个不断进化的Tool Library。在Omni3D-Bench上，TVP的性能优于GPT-4o 22%，优于之前最佳的视觉编程系统11%，并且通过经验驱动学习的工具作为核心程序依赖被使用5倍更频繁，并且在未进行任何测试集特定修改的情况下，对未见过的空间任务表现出强大的泛化能力。该工作确立了经验驱动的Transductive工具创建作为构建自我进化的视觉编程代理的强大范式，这些代理能够有效解决具有挑战性空间推理任务。代码可在https://transductive-visualprogram.github.io/获取。</div>
</details>
</div>
<div class="card">
<div class="title">Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Yoonwoo Jeong, Cheng Sun, Frank Wang, Minsu Cho, Jaesung Choe</div>
<div class="meta-line">First: 2025-12-24T04:16:18+00:00 · Latest: 2025-12-24T04:16:18+00:00</div>
<div class="meta-line">Comments: Will be updated</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20927v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20927v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分位数渲染：高效嵌入高维特征的3D高斯点绘制</div>
<div class="mono" style="margin-top:8px">计算机视觉领域的最新进展通过利用3D高斯点绘制（3D-GS）成功地将开放词汇分割（OVS）扩展到了3D领域。尽管取得了这些进展，但高效渲染用于开放词汇查询所需的高维特征仍然面临重大挑战。现有方法使用码本或特征压缩，导致信息丢失，从而降低分割质量。为了解决这一限制，我们提出了分位数渲染（Q-Render），这是一种新颖的3D高斯渲染策略，能够高效处理高维特征同时保持高保真度。与传统的体渲染不同，后者沿每个射线密集采样所有相交的3D高斯，Q-Render仅稀疏采样沿射线具有主导影响的那些。通过将Q-Render集成到一个通用的3D神经网络中，我们还提出了高斯点绘制网络（GS-Net），该网络以通用方式预测高斯特征。在ScanNet和LeRF上的广泛实验表明，我们的框架优于最先进的方法，同时能够实现接近43.7倍的加速进行实时渲染。代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Quantile Rendering (Q-Render), a novel rendering technique for 3D Gaussian splatting that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, Q-Render sparsely samples only the most influential 3D Gaussians along each ray. The authors integrate Q-Render into a Gaussian Splatting Network (GS-Net) to predict Gaussian features in a generalizable manner. Experiments on ScanNet and LeRF show that this framework outperforms state-of-the-art methods and enables real-time rendering with an approximate 43.7x speedup on 512-D feature maps.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为Quantile Rendering (Q-Render)的新颖渲染策略，该策略能够高效处理高维特征并保持高保真度。不同于传统的体渲染方法，Q-Render仅稀疏采样沿光线具有主导影响的3D高斯分布。作者提出了一种名为Gaussian Splatting Network (GS-Net)的一般化3D神经网络，该网络将Q-Render集成进来，以一般化的方式预测高斯特征。实验结果表明，所提出的方法在ScanNet和LeRF上优于最先进的方法，并且能够在512-D特征图上实现约43.7倍的实时渲染加速。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</div>
<div class="meta-line">Authors: Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</div>
<div class="meta-line">First: 2025-12-17T18:59:55+00:00 · Latest: 2025-12-24T03:37:34+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures, conference or other essential info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15713v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15713v2">PDF</a> · <a href="https://github.com/hustvl/DiffusionVL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionVL：将任何自回归模型转化为扩散视觉语言模型</div>
<div class="mono" style="margin-top:8px">在最近的多模态研究中，扩散范式因其独特的解码优势，已成为自回归范式（AR）的有前途的替代方案。然而，由于基础扩散语言模型能力的限制，扩散视觉语言模型（dVLM）的性能仍然远远落后于主流模型。这引发了一个简单而基本的问题：是否可以基于现有的强大自回归模型构建dVLM？为此，我们提出了DiffusionVL，这是一个可以从任何强大自回归模型转换而来的dVLM家族。通过简单的微调，我们成功地将自回归预训练模型适应到扩散范式中。这种方法产生了两个关键观察结果：（1）从基于自回归的多模态模型到扩散的范式转变非常有效。（2）直接将自回归语言模型转换为dVLM也是可行的，性能与LaLaVA风格的视觉指令调优相当。此外，我们引入了一种块解码设计到dVLM中，支持任意长度的生成和KV缓存重用，实现了显著的推理速度提升。我们进行了大量的实验。尽管使用了比先前方法少于5%的数据进行训练，DiffusionVL在MMMU-Pro（视觉）基准上实现了34.4%的整体性能提升，在MME（认知）基准上实现了37.5%的提升，同时推理速度提升了2倍。模型和代码发布在https://github.com/hustvl/DiffusionVL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DiffusionVL is a family of diffusion vision language models (dVLMs) that can be translated from existing powerful autoregressive (AR) models through simple fine-tuning. This approach shows that the shift from AR-based models to the diffusion paradigm is highly effective, and direct conversion of AR language models to dVLMs can achieve performance comparable to LLaVA-style visual-instruction-tuning. Additionally, a block-decoding design is introduced to support arbitrary-length generation and KV cache reuse, leading to a significant inference speedup. Despite using less than 5% of the data required by previous methods, DiffusionVL achieves a 34.4% gain on the MMMU-Pro (vision) benchmark and a 37.5% gain on the MME (Cog.) benchmark, alongside a 2x inference speedup.</div>
<div class="mono" style="margin-top:8px">DiffusionVL 将现有的强大自回归模型转化为扩散视觉语言模型，使用不到5%的训练数据实现了显著的性能提升和2倍的推理速度提升。它引入了一种块解码设计，支持任意长度的生成和KV缓存重用，提高了效率。</div>
</details>
</div>
<div class="card">
<div class="title">PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding</div>
<div class="meta-line">Authors: Seongmin Jung, Seongho Choi, Gunwoo Jeon, Minsu Cho, Jongwoo Lim</div>
<div class="meta-line">First: 2025-12-24T03:18:51+00:00 · Latest: 2025-12-24T03:18:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20907v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20907v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PanoGrounder：通过全景场景表示连接2D和3D的基于VLM的3D视觉定位</div>
<div class="mono" style="margin-top:8px">3D视觉定位（3DVG）是视觉语言感知与机器人技术之间的关键桥梁，需要语言理解与3D场景推理。传统监督模型利用显式的3D几何结构，但由于3D视觉语言数据集稀缺及与现代视觉语言模型（VLM）相比推理能力有限，表现出有限的泛化能力。我们提出PanoGrounder，这是一种通用的3DVG框架，将多模态全景表示与预训练的2D VLM结合，进行强大的视觉语言推理。全景渲染图，结合3D语义和几何特征，作为2D和3D之间的中间表示，提供了两大优势：（i）可以直接输入到VLM中，无需大量适应；（ii）由于其360度的视野，保留了长距离的物体间关系。我们设计了一个三阶段流水线，考虑场景布局和几何结构放置一组紧凑的全景视点，使用VLM在每个全景渲染图上定位文本查询，并通过提升将每个视点的预测融合成一个3D边界框。我们的方法在ScanRefer和Nr3D上达到了最先进的结果，并在未见过的3D数据集和文本重述方面展示了优越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PanoGrounder is a framework for 3D Visual Grounding that combines panoramic scene representations with pretrained 2D vision-language models. It uses panoramic renderings, enriched with 3D semantic and geometric features, to bridge the gap between 2D and 3D, enabling strong vision-language reasoning. The method involves a three-stage pipeline that places panoramic viewpoints, grounds text queries on these views, and fuses predictions into a 3D bounding box. PanoGrounder achieves state-of-the-art results on ScanRefer and Nr3D and shows good generalization to new datasets and text variations.</div>
<div class="mono" style="margin-top:8px">PanoGrounder 是一种结合全景场景表示与预训练的 2D 视觉语言模型的方法，以增强 3D 视觉定位。它使用包含 3D 语义和几何特征的全景渲染作为中间表示，以提高泛化能力。该方法在 ScanRefer 和 Nr3D 数据集上达到了最先进的效果，并在未见过的 3D 数据集和文本重述方面表现出更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking and Enhancing VLM for Compressed Image Understanding</div>
<div class="meta-line">Authors: Zifu Zhang, Tongda Xu, Siqi Li, Shengxi Li, Yue Zhang, Mai Xu, Yan Wang</div>
<div class="meta-line">First: 2025-12-24T02:59:01+00:00 · Latest: 2025-12-24T02:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20901v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20901v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型在压缩图像理解中的基准测试与增强</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）的快速发展及其应用需求的不断增加，高效压缩图像输入变得越来越重要。现有的VLMs主要处理和理解高比特率压缩图像，而它们对低比特率压缩图像的理解能力尚未得到充分探索。在本文中，我们介绍了第一个全面的基准测试，以评估VLM在处理压缩图像方面的能力，涵盖了广泛使用的图像编解码器和多种任务，基准中包含超过一百万张压缩图像。接下来，我们通过将性能差距分为a) 压缩过程中的信息损失和b) VLM的一般化失败来分析性能差距。我们通过具体的例子可视化这些差距，并确定对于压缩图像，只能通过缓解一般化差距来改善。最后，我们提出了一种通用的VLM适配器，以增强现有编解码器压缩图像的模型性能。结果表明，一个适配器可以提高VLM在不同编解码器和比特率图像上的性能10%-30%。我们认为，我们的基准测试和增强方法提供了有价值的见解，并有助于弥合VLMs与压缩图像之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a benchmark to evaluate VLMs on compressed images, covering various image codecs and tasks. It identifies that the performance gap is mainly due to the generalization failure of VLMs and proposes a universal VLM adaptor to improve performance by 10%-30% across different codecs and bitrates.</div>
<div class="mono" style="margin-top:8px">该论文引入了一个全面的基准来评估Vision-Language模型在压缩图像上的表现，涵盖了多种图像编解码器和任务。作者发现VLMs在压缩图像上的性能差距主要是由于泛化能力不足，而不是压缩过程中信息的丢失。他们提出了一种通用的VLM适配器，可以在不同编解码器和比特率的图像上将模型性能提高10%-30%。这项工作为改进VLMs处理压缩图像提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</div>
<div class="meta-line">Authors: Zhenyuan Chen, Chenxi Wang, Ningyu Zhang, Feng Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-02T03:01:23+00:00 · Latest: 2025-12-24T01:10:58+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Dataset and Benchmark Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.01907v5">Abs</a> · <a href="https://arxiv.org/pdf/2509.01907v5">PDF</a> · <a href="https://github.com/Bili-Sakura/RSCC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,351 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC&#x27;s ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at https://github.com/Bili-Sakura/RSCC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RSCC：用于灾害事件的大型遥感变化描述数据集</div>
<div class="mono" style="margin-top:8px">遥感对于灾害监测至关重要，但现有数据集缺乏时间图像对和详细的文本注释。当前资源主要以单张快照图像为主，无法捕捉灾害随时间的变化影响。为解决这一问题，我们引入了遥感变化描述（RSCC）数据集，这是一个包含62,351个灾前/灾后图像对（涵盖地震、洪水、野火等）的大规模基准，每个图像对都配有丰富的、类似人类的变更描述。通过在遥感数据中架起时间与语义的桥梁，RSCC 使视觉-语言模型能够进行灾害意识的双时相理解的稳健训练和评估。我们的结果突显了RSCC在促进详细灾害相关分析方面的潜力，为遥感中更准确、可解释和可扩展的视觉-语言应用铺平了道路。代码和数据集可在https://github.com/Bili-Sakura/RSCC/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The RSCC dataset addresses the lack of temporal image pairs and detailed textual annotations in existing remote sensing datasets, by providing 62,351 pre-/post-disaster image pairs with rich change captions. This dataset enables robust training and evaluation of vision-language models for disaster monitoring. Key findings show that RSCC facilitates detailed disaster-related analysis, enhancing the accuracy and interpretability of remote sensing applications.</div>
<div class="mono" style="margin-top:8px">RSCC 数据集通过提供 62,351 对灾前/灾后图像配以丰富的变化描述，解决了现有遥感数据集中缺乏时间序列图像对和详细文本注释的问题。该数据集使视觉-语言模型能够稳健地训练和评估以应对灾害。关键发现表明，RSCC 促进了详细的灾害相关分析，提高了遥感应用的准确性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference</div>
<div class="meta-line">Authors: Putu Indah Githa Cahyani, Komang David Dananjaya Suartana, Novanto Yudistira</div>
<div class="meta-line">First: 2025-12-23T23:30:56+00:00 · Latest: 2025-12-23T23:30:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20839v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20839v1">PDF</a> · <a href="https://github.com/kmdavidds/mlfastlm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>输入自适应视觉预处理以提高快速视觉-语言模型推理效率</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多模态推理任务中表现出强大的性能，但由于部署时存在高推理延迟和计算成本问题，特别是在处理高分辨率视觉输入时，其部署仍然具有挑战性。尽管最近的架构如FastVLM通过优化视觉编码器提高了效率，但现有的管道仍然依赖于静态视觉预处理，导致对于视觉简单的输入存在冗余计算。在本文中，我们提出了一种自适应视觉预处理方法，该方法根据图像内容特征动态调整输入分辨率和空间覆盖范围。所提出的方法结合了内容感知图像分析、自适应分辨率选择和内容感知裁剪，以在视觉编码前减少视觉冗余。重要的是，该方法与FastVLM集成，无需修改其架构或重新训练。我们在DocVQA数据集的子集上仅进行推理评估，重点关注效率导向的指标。实验结果表明，自适应预处理可以将每张图像的推理时间减少超过50%，降低平均完整生成时间，并且与基线管道相比，视觉标记数量减少超过55%。这些发现表明，输入感知预处理是一种有效且轻量级的策略，可以提高视觉-语言模型的部署效率。为了便于可重复性，我们的实现作为FastVLM仓库的分支提供，包含所提出方法的文件，并可在https://github.com/kmdavidds/mlfastlm/获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of high inference latency in Vision-Language Models (VLMs) by proposing an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content. The method integrates with FastVLM without altering its architecture and reduces per-image inference time by over 50%, lowers mean full generation time, and decreases visual token count by more than 55% compared to the baseline pipeline.</div>
<div class="mono" style="margin-top:8px">本文提出了一种自适应视觉预处理方法，该方法根据图像内容动态调整输入分辨率和空间覆盖范围，以解决视觉语言模型（VLM）的高推理延迟问题。该方法与FastVLM集成，无需修改其架构或重新训练。在DocVQA数据集上的实验表明，自适应预处理可以将每张图像的推理时间减少超过50%，降低平均完整生成时间，并将视觉标记数量减少超过55%，证明了输入感知预处理对提高VLM部署效率的有效性和轻量性。</div>
</details>
</div>
<div class="card">
<div class="title">VL4Gaze: Unleashing Vision-Language Models for Gaze Following</div>
<div class="meta-line">Authors: Shijing Wang, Chaoqun Cui, Yaping Huang, Hyung Jin Chang, Yihua Cheng</div>
<div class="meta-line">First: 2025-12-23T19:47:11+00:00 · Latest: 2025-12-23T19:47:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20735v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20735v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VL4Gaze：释放视觉语言模型的凝视跟踪潜力</div>
<div class="mono" style="margin-top:8px">人类凝视为理解注意力、意图和社会互动提供了关键线索，但在当前的视觉语言模型（VLMs）中，凝视理解尚未得到充分探索。尽管最近的VLMs在各种视觉任务中表现出强大的场景级推理能力，但尚无基准系统地评估或训练它们进行凝视解释，这使得人们质疑是否可以从通用视觉语言预训练中自然涌现出凝视理解能力。为解决这一问题，我们引入了VL4Gaze，这是首个旨在研究、评估和解锁VLMs在凝视理解方面潜力的大规模基准。VL4Gaze包含489K个自动生成的问题-答案对，覆盖124K张图像，并通过四个互补任务将凝视理解统一为一个VQA问题：(1) 凝视对象描述，(2) 凝视方向描述，(3) 凝视点定位，(4) 含义模糊问题识别。我们全面评估了商业和开源VLMs在上下文学习和微调设置下的表现。结果表明，即使大规模VLMs在没有特定任务监督的情况下也难以可靠地推断出凝视语义和空间定位。相比之下，通过VL4Gaze进行训练在所有任务上都带来了显著且一致的改进，突显了为开发VLMs的凝视理解能力而进行针对性多任务监督的重要性。我们将发布该数据集和代码，以支持该领域的进一步研究和开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VL4Gaze introduces a new benchmark to evaluate and train vision-language models (VLMs) for gaze understanding, addressing the gap in current VLMs&#x27; ability to interpret gaze cues. The benchmark includes 489K question-answer pairs across 124K images, formulated into four tasks. Evaluations show that large-scale VLMs perform poorly without specific supervision, but training on VL4Gaze significantly improves their gaze understanding capabilities across all tasks, emphasizing the need for targeted multi-task supervision.</div>
<div class="mono" style="margin-top:8px">VL4Gaze 提出了一个新的基准，用于评估和训练视觉-语言模型（VLMs）在理解注视方面的能力，填补了当前 VLMs 在解释注视线索方面的空白。基准包括 489K 个问题-答案对，跨越 124K 张图像，被表述为四个任务：注视对象描述、注视方向描述、注视点定位和模糊问题识别。评估结果显示，大型 VLMs 在没有特定任务监督的情况下难以可靠地推断注视语义和空间定位，但通过在 VL4Gaze 上进行训练，所有任务的性能都有显著和一致的提升，突显了为 VLMs 发展注视理解能力需要有针对性的多任务监督的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</div>
<div class="meta-line">Authors: Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</div>
<div class="meta-line">First: 2025-12-23T18:05:43+00:00 · Latest: 2025-12-23T18:05:43+00:00</div>
<div class="meta-line">Comments: Under submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20561v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20561v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashVLM：文本引导的视觉标记选择框架用于大型多模态模型</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）通常处理每张图像或视频帧数百或数千个视觉标记，导致二次注意力成本和大量冗余。现有的标记减少方法往往忽视了文本查询或依赖于深度注意力图，这些图在剧烈剪枝下的不稳定性导致语义对齐下降。
我们提出了一种FlashVLM，这是一种文本引导的视觉标记选择框架，能够动态适应查询。FlashVLM 不依赖于嘈杂的注意力权重，而是计算投影图像标记与语言模型空间中归一化文本嵌入之间的显式跨模态相似性。这种外在的相关性与内在的视觉显著性通过对数域加权和温度控制锐化进行融合。此外，一种保留多样性的划分保留了一组最小但具有代表性的背景标记，以保持全局上下文。
在相同的标记预算和评估协议下，FlashVLM 实现了超越无损压缩的效果，即使在对LLaVA 1.5剪枝高达77.8%的情况下，仍略优于未剪枝的基线，同时保持92.8%的准确率，即使在高达94.4%的压缩下也是如此。在14个图像和视频基准上的广泛实验表明，FlashVLM 在保持强大鲁棒性和泛化能力的同时，实现了最先进的效率性能权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FlashVLM is a text-guided visual token selection framework that dynamically adapts visual inputs to textual queries by computing explicit cross-modal similarities and fusing them with intrinsic visual saliency. It achieves beyond lossless compression, surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 benchmarks show that FlashVLM provides state-of-the-art efficiency performance trade-offs while maintaining strong robustness and generalization across mainstream VLMs.</div>
<div class="mono" style="margin-top:8px">FlashVLM 是一种文本引导的视觉标记选择框架，通过计算显式的跨模态相似性并将其与内在的视觉显著性融合，动态适应视觉输入到文本查询。它实现了超越无损压缩的效果，在LLaVA 1.5 上通过剪枝高达 77.8% 的视觉标记，同时在 94.4% 的压缩下仍保持 92.8% 的准确性。在 14 个图像和视频基准上的广泛实验表明，FlashVLM 在各种 VLM 中提供了最先进的效率性能权衡，并具有强大的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</div>
<div class="meta-line">Authors: Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</div>
<div class="meta-line">First: 2025-12-23T17:56:36+00:00 · Latest: 2025-12-23T17:56:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20557v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在四维中学习推理：视觉语言模型的动态空间理解</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在一般理解方面表现出色，但在动态空间推理（DSR）方面仍然较弱，即在时间维度上对3D空间中物体几何形状和关系的变化进行推理，这主要是由于缺乏可扩展的4D感知训练资源。为了在数据集、基准和模型的各个方面弥合这一差距，我们引入了DSR套件。首先，我们提出了一种自动流水线，从野外视频中生成DSR的多项选择题-答案对。通过利用现代视觉基础模型，该流水线提取了丰富的几何和运动信息，包括相机姿态、局部点云、物体掩码、方向和3D轨迹。这些几何线索使得DSR-Train的构建成为可能，并进一步构建了DSR-Bench用于评估。与以往工作相比，我们的数据强调（i）野外视频来源，（ii）物体和场景级别的3D需求，（iii）视角变换，（iv）多物体交互，以及（v）细粒度、程序化的答案。除了数据，我们还提出了一种轻量级的几何选择模块（GSM），以无缝地将几何先验整合到VLM中，该模块压缩了问题语义，并从预训练的4D重建先验中提取与问题相关的信息，形成一组紧凑的几何标记。这种有针对性的提取避免了向模型灌输无关知识。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B中显著增强了其动态空间推理能力，同时在通用视频理解基准测试中保持了准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve vision-language models&#x27; dynamic spatial reasoning (DSR) by addressing the scarcity of 4D-aware training resources. It introduces DSR Suite, which includes an automated pipeline for generating DSR question-answer pairs from in-the-wild videos and a lightweight Geometry Selection Module (GSM) to integrate geometric priors into VLMs. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B enhances its dynamic spatial reasoning capability while maintaining accuracy on general video understanding benchmarks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决4D感知训练资源稀缺问题，提升视觉语言模型的动态空间推理能力（DSR）。引入了DSR Suite，包括从野外视频自动生成DSR问题-答案对的自动化管道和几何选择模块（GSM）以整合几何先验。DSR-Train数据集和DSR-Bench评估集强调野外来源、3D物体和场景要求、视角变换、多物体交互和细粒度答案。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B中，显著增强了动态空间推理能力，同时保持了通用视频理解的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</div>
<div class="meta-line">Authors: Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-12-23T17:55:35+00:00 · Latest: 2025-12-23T17:55:35+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20556v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多粒度文本引导图像融合以应对多曝光和多焦距场景</div>
<div class="mono" style="margin-top:8px">图像融合旨在从在具有挑战性条件下拍摄的一对输入中合成一张高质量的图像，例如不同的曝光水平或焦距深度。核心挑战在于有效处理输入之间的动态范围和焦距深度差异。随着视觉语言模型的出现，最近的方法将文本描述作为辅助指导以提高融合质量。然而，简单地引入粗粒度描述会阻碍对细粒度细节的理解，并且对跨模态对齐提出了挑战。为了解决这些限制，我们提出了多粒度文本引导图像融合（MTIF），这是一种具有三个关键设计的新型融合范式。首先，它引入了多粒度的文本描述，分别捕捉细粒度细节、结构线索和语义内容，通过分层跨模态调制模块引导图像融合。其次，它在每个粒度级别引入监督信号，以促进视觉和文本特征之间的对齐并增强辅助文本的实用性。第三，它采用了一种基于显著性的增强模块，通过密集的语义内容增强训练数据，进一步加强跨模态调制和对齐。广泛的实验表明，MTIF在多曝光和多焦距图像融合任务中始终优于先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes Multi-grained Text-guided Image Fusion (MTIF) to address the challenges of image fusion under multi-exposure and multi-focus scenarios. MTIF introduces multi-grained textual descriptions to capture fine details, structural cues, and semantic content, guiding the fusion process through a hierarchical cross-modal modulation module. Supervision signals at each granularity and a saliency-driven enrichment module further enhance alignment and cross-modal modulation. Experiments demonstrate that MTIF outperforms previous methods in both multi-exposure and multi-focus image fusion tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决输入之间动态范围和焦深差异的问题来提升图像融合效果。提出的多粒度文本引导图像融合（MTIF）方法使用层次化的文本描述来引导图像融合，通过粒度特定的监督信号增强跨模态对齐，并通过显著性驱动的增强模块进一步增强跨模态对齐。实验结果表明，MTIF在多曝光和多焦点场景中均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Video Generation Models Are Good Latent Reward Models</div>
<div class="meta-line">Authors: Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang</div>
<div class="meta-line">First: 2025-11-26T16:14:18+00:00 · Latest: 2025-12-23T15:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21541v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21541v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成模型是良好的潜在奖励模型</div>
<div class="mono" style="margin-top:8px">奖励反馈学习（ReFL）已被证明对于使图像生成与人类偏好对齐非常有效。然而，将其扩展到视频生成面临着重大挑战。现有的视频奖励模型依赖于为像素空间输入设计的视觉-语言模型，这将ReFL优化限制在昂贵的VAE解码之后的近完全去噪步骤中。这种像素空间的方法会产生大量的内存开销并增加训练时间，而且其后期优化缺乏早期监督，仅能细化视觉质量而不能从根本上改善运动动态和结构一致性。在本文中，我们展示了预训练的视频生成模型天然适合在嘈杂的潜在空间中进行奖励建模，因为它们明确设计为可以处理任意时间步的嘈杂潜在表示，并通过其序列建模能力内在地保留时间信息。因此，我们提出了过程奖励反馈学习（PRFL）框架，该框架在潜在空间中完全进行偏好优化，从而在整个去噪链中实现高效的梯度反向传播，而无需VAE解码。广泛的实验表明，PRFL在提高与人类偏好的对齐程度方面显著优于RGB ReFL，同时在内存消耗和训练时间上实现了显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of applying reward feedback learning (ReFL) to video generation by proposing Process Reward Feedback Learning (PRFL). PRFL leverages pre-trained video generation models to optimize preferences in the noisy latent space, avoiding the need for computationally expensive VAE decoding. This approach reduces memory usage and training time while improving alignment with human preferences. Key findings include significant improvements in preference alignment and substantial reductions in memory consumption and training time compared to traditional RGB ReFL methods.</div>
<div class="mono" style="margin-top:8px">本文提出了Process Reward Feedback Learning (PRFL) 方法来解决将奖励反馈学习 (ReFL) 应用于视频生成的问题。PRFL 利用预训练的视频生成模型直接在噪声的潜在空间中优化偏好，避免了昂贵的 VAE 解码步骤。这种方法使得与人类偏好的对齐效果更好，同时减少了内存消耗和训练时间，相比传统的像素空间 ReFL 方法具有显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Laws for Energy Efficiency of Local LLMs</div>
<div class="meta-line">Authors: Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Mikail Okyay, Bakbergen Ryskulov, David Montero, Samuel Mugel, Román Orús</div>
<div class="meta-line">First: 2025-12-18T13:40:33+00:00 · Latest: 2025-12-23T15:02:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16531v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.16531v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven &quot;resolution knee&quot;, where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部LLM能效的标度律</div>
<div class="mono" style="margin-top:8px">在边缘设备上部署局部大型语言模型和视觉-语言模型需要在准确性与受限的计算和能源预算之间进行权衡。尽管图形处理器主导了现代人工智能部署，但大多数消费级硬件——包括笔记本电脑、台式机、工业控制器和嵌入式系统——仍依赖于中央处理器。尽管如此，仅中央处理器的推理计算法则对局部语言和视觉-语言工作负载的研究仍然相对较少。我们系统地在两个广泛用于局部推理的中央处理器级别上对大型语言和视觉-语言模型进行了基准测试：MacBook Pro M2，代表主流笔记本电脑级部署，以及Raspberry Pi 5，代表受限的、低功耗嵌入式设置。基于连续采样处理器和内存使用情况并结合面积下曲线积分的统一方法，我们描述了计算负载随输入文本长度对语言模型和随图像分辨率对视觉-语言模型的标度关系。我们发现了两条经验标度律：（1）语言模型推理的计算成本大约与标记长度成线性关系；（2）视觉-语言模型表现出预处理驱动的“分辨率拐点”，其中计算在内部分辨率限制以上保持恒定，在以下则急剧下降。除了这些定律，我们展示了量子启发式压缩可将处理器和内存使用量最多减少71.9%，能源消耗最多减少62%，同时保持或提高语义准确性。这些结果提供了对局部语言和视觉-语言工作负载的多模态中央处理器仅计算法则的系统量化，并确定了模型压缩和输入分辨率预处理作为有效、低成本的可持续边缘推理杠杆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the energy efficiency of deploying large language models and vision-language models on edge devices, focusing on central processing units. By benchmarking these models on a MacBook Pro M2 and a Raspberry Pi 5, the researchers discovered two scaling laws: the computational cost for language models scales linearly with token length, and vision-language models exhibit a preprocessing-driven &#x27;resolution knee&#x27; where compute remains constant above a certain resolution and decreases below it. Additionally, quantum-inspired compression was found to reduce processor and memory usage by up to 71.9% and energy consumption by up to 62%, while maintaining or improving semantic accuracy.</div>
<div class="mono" style="margin-top:8px">该研究探讨了在边缘设备上部署大型语言模型和视觉-语言模型时的能效问题，重点关注中央处理单元。通过在MacBook Pro M2和Raspberry Pi 5上进行基准测试，研究人员发现了两个缩放定律：语言模型的计算成本与词元长度成线性关系，而视觉-语言模型表现出一种预处理驱动的“分辨率拐点”，即在某一分辨率以上，计算量保持不变，在此之下则急剧下降。此外，量子启发式压缩被发现可将处理器和内存使用量最多减少71.9%，能量消耗最多减少62%，同时保持或提高语义准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Anomaly Thoughts with Large Vision-Language Models</div>
<div class="meta-line">Authors: Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo</div>
<div class="meta-line">First: 2025-12-23T15:01:05+00:00 · Latest: 2025-12-23T15:01:05+00:00</div>
<div class="meta-line">Comments: 2 pages, 3 figures, 1 table. Accepted for RECPAD 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20417v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20417v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型视觉语言模型的异常链推理</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在自动化视频监控中的应用受到其对正常情况的固有偏见限制，往往无法检测犯罪。虽然链式推理策略在语言任务中显示出显著的潜力，但在推理过程中缺乏归纳异常偏见进一步引导模型向正常解释方向发展。为了解决这一问题，我们提出了一种名为异常链推理（CoAT）的多智能体推理框架，通过引入最终的异常分类层来在推理过程中引入归纳犯罪偏见。我们的方法显著提高了异常检测性能，低分辨率视频上的F1分数提高了11.8个百分点，在高分辨率视频上的异常分类提高了3.78个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance automated video surveillance by addressing the limitations of large vision-language models in detecting anomalies, particularly crimes. The method introduces Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that incorporates inductive criminal bias in the reasoning process. This approach significantly improves anomaly detection, increasing the F1-score by 11.8 percentage points on low-resolution footage and anomaly classification by 3.78 percentage points in high-resolution videos.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决大型视觉-语言模型偏向正常场景的问题，提升自动视频监控。提出的Chain-of-Anomaly-Thoughts (CoAT)框架通过引入异常分类层来改善异常检测和分类。该方法在低分辨率视频中实现了F1分数11.8个百分点的提升，在高分辨率视频中实现了3.78个百分点的提升。</div>
</details>
</div>
<div class="card">
<div class="title">LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer</div>
<div class="meta-line">Authors: Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang</div>
<div class="meta-line">First: 2025-08-01T09:51:54+00:00 · Latest: 2025-12-23T14:27:42+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.00477v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.00477v2">PDF</a> · <a href="https://github.com/Suchenl/LAMIC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC&#x27;s superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC&#x27;s performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAMIC：基于布局感知的多图像合成通过多模态扩散变换器的可扩展性</div>
<div class="mono" style="margin-top:8px">在可控图像合成中，从多个参考中生成具有空间布局意识的连贯且一致的图像仍然是一个开放的挑战。我们提出了LAMIC，一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。基于MMDiT模型，LAMIC引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体分离；2）区域调节注意力（RMA）以实现布局感知生成。为进一步评估模型能力，我们还引入了三个指标：1）包含比（IN-R）和填充比（FI-R）以评估布局控制；2）背景相似度（BG-S）以衡量背景一致性。大量实验表明，LAMIC在大多数主要指标上均取得了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和AVG得分上始终优于现有多参考基线，并在复杂合成任务中实现了最佳的DPG。这些结果表明，LAMIC在保持身份、保存背景、布局控制和遵循提示方面具有优越的能力，所有这些均无需任何训练或微调，展示了强大的零样本泛化能力。通过继承先进的单参考模型的优势并使其实现无缝扩展到多图像场景，LAMIC为可控多图像合成建立了一个新的无需训练的范式。随着基础模型的不断进化，LAMIC的性能预计会相应地扩展。我们的实现可在以下链接获取：https://github.com/Suchenl/LAMIC。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LAMIC is a Layout-Aware Multi-Image Composition framework that extends single-reference diffusion models to multi-reference scenarios without training. It introduces two mechanisms: Group Isolation Attention (GIA) for entity disentanglement and Region-Modulated Attention (RMA) for layout-aware generation. LAMIC outperforms existing multi-reference baselines in metrics such as Inclusion Ratio (IN-R), Fill Ratio (FI-R), and Background Similarity (BG-S), demonstrating superior identity keeping, background preservation, and layout control capabilities.</div>
<div class="mono" style="margin-top:8px">LAMIC 是一种布局感知的多图像合成框架，它在无需训练的情况下将单参考扩散模型扩展到多参考场景。它引入了 GIA 以增强实体分离，并引入了 RMA 以实现布局感知生成。LAMIC 在身份保持、背景一致性以及布局控制等指标上优于现有基线，展示了强大的零样本泛化能力。全面的实验表明，LAMIC 在复杂合成任务中表现出色，无需任何微调。该框架为可控多图像合成建立了一个新的无需训练的范式。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
