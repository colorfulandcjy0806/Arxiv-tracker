<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-03 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260103_0325</div>
    <div class="row"><div class="card">
<div class="title">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</div>
<div class="meta-line">Authors: Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam</div>
<div class="meta-line">First: 2025-12-31T17:32:31+00:00 · Latest: 2025-12-31T17:32:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24986v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24986v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a &quot;render and wait&quot; paradigm toward an interactive dialogue with a modern, physics-informed pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysTalk: 3D 高斯场景中的语言驱动实时物理</div>
<div class="mono" style="margin-top:8px">逼真的视觉模拟无处不在，但其创建需要计算时间、渲染和专家动画知识。基于文本输入的开放式词汇视觉效果生成有望解锁巨大的创意潜力。然而，当前的工作流程缺乏物理真实性和有效的语言界面，需要缓慢的离线优化。相比之下，PhysTalk 将 3D 高斯点积 (3DGS) 场景作为输入，并将任意用户提示翻译成实时、基于物理的 4D 动画。一个大型语言模型 (LLM) 生成可执行代码，直接通过轻量级代理和粒子动力学修改 3DGS 参数。值得注意的是，PhysTalk 是第一个直接将 3DGS 与物理模拟器结合而无需依赖耗时的网格提取的框架。尽管保持开放式词汇，这种设计使得通过碰撞感知的基于物理的操纵任意多材料对象的交互式 3D 高斯动画成为可能。最后，PhysTalk 是无训练的且计算量轻：这使得 4D 动画广泛可及，并将这些工作流程从“渲染和等待”的范式转向与现代、基于物理的管道进行互动对话。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PhysTalk is a framework that translates user prompts into real-time, physics-based 4D animations using a 3D Gaussian Splatting (3DGS) scene as input. It leverages a large language model to generate executable code that modifies 3DGS parameters through lightweight proxies and particle dynamics, enabling interactive manipulation of multi-material objects. Key findings include the first direct coupling of 3DGS with a physics simulator, avoiding time-consuming mesh extraction, and achieving interactive 3D Gaussian animation with collision awareness. PhysTalk is computationally lightweight and train-free, making 4D animation accessible and shifting workflows from a &#x27;render and wait&#x27; paradigm to an interactive dialogue.</div>
<div class="mono" style="margin-top:8px">PhysTalk 是一个框架，通过将用户提示转化为实时的物理基础4D动画，使用3D高斯散点图（3DGS）场景作为输入。它利用大型语言模型生成可执行代码，通过轻量级代理和粒子动力学修改3DGS参数，实现对多材料对象的交互式操控。关键发现包括首次直接将3DGS与物理模拟器结合，避免了耗时的网格提取，实现了具有碰撞感知的交互式3D高斯动画。PhysTalk 计算量轻且无需训练，使4D动画更加普及，并将工作流从“渲染等待”模式转变为与现代物理导向管道的互动对话。</div>
</details>
</div>
<div class="card">
<div class="title">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</div>
<div class="meta-line">Authors: Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</div>
<div class="meta-line">First: 2025-12-31T17:31:29+00:00 · Latest: 2025-12-31T17:31:29+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Robotics and Automation Letters (RA-L)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24985v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&#x27; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DarkEQA：在低光室内环境中的视觉语言模型体态问答基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）越来越多地被用作体态代理的核心推理模块。现有的基准测试在理想、光线充足的条件下评估其能力，但全天候24/7运行需要在各种视觉退化条件下表现出色，包括夜间或黑暗环境中的低光条件——这一核心需求已被很大程度上忽视。为应对这一未充分探索的挑战，我们提出了DarkEQA，这是一个开源基准测试，用于在多级低光条件下评估与体态问答相关的感知基本能力。DarkEQA通过在受控退化条件下从第一人称观察进行问答评估，隔离了感知瓶颈，使可归因的鲁棒性分析成为可能。DarkEQA的一个关键设计特点是其物理保真度：视觉退化在线性RAW空间中建模，模拟基于物理的照明下降和传感器噪声，随后通过ISP启发式的渲染管道。我们通过评估一系列最先进的VLMs和低光图像增强（LLIE）模型展示了DarkEQA的实用性。我们的分析系统地揭示了这些视觉条件下的体态操作限制。我们的代码和基准数据集将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DarkEQA is a benchmark designed to evaluate the performance of Vision-Language Models (VLMs) in low-light indoor environments, addressing the underexplored challenge of robust 24/7 operation. It isolates the perception bottleneck by degrading egocentric observations and evaluates the models&#x27; ability to answer questions accurately. Key findings show that VLMs struggle with low-light conditions, highlighting their limitations in real-world applications where lighting is poor.</div>
<div class="mono" style="margin-top:8px">DarkEQA 是一个基准，旨在评估视觉-语言模型在低光环境下的性能，解决了现有研究中对24/7稳健运行的不足。它通过模拟低光环境来控制降级过程，并在 RAW 空间中评估模型的感知能力。关键发现表明，当前的视觉-语言模型在低光条件下从第一人称观察回答问题时存在局限性，突显了它们在实际应用中的不足，特别是在照明条件不佳的情况下。</div>
</details>
</div>
<div class="card">
<div class="title">DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</div>
<div class="meta-line">Authors: Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig</div>
<div class="meta-line">First: 2025-12-19T04:09:24+00:00 · Latest: 2025-12-31T17:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17221v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.17221v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder&#x27;s alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DAVE：一种用于文档理解和网络代理的VLM视觉编码器</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）在多模态任务中表现出色，但它们所选择的视觉编码器存在根本性弱点：其低级特征缺乏文档理解和网络代理所需的稳健的结构和空间信息。为弥补这一差距，我们引入了DAVE，一种专为VLMs设计并针对这些任务定制的视觉编码器。我们的训练管道旨在利用大量未标注数据，以绕过对文档和网络图像的大规模注释成本。我们首先在未标注图像上进行自我监督预训练阶段，然后在监督自回归预训练阶段，模型从有限的高质量数据中学习解析和定位等任务。在监督阶段内，我们采用了两种策略来提高编码器与通用视觉知识和多样化文档及网络代理任务的对齐：(i) 我们引入了一种新的模型合并方案，将使用不同文本解码器训练的编码器结合在一起，以确保与不同网络代理架构的广泛兼容性。(ii) 我们使用集成训练将预训练的通用编码器（例如SigLIP2）的特征与我们自己的文档和网络特定表示融合在一起。在经典文档任务、VQAs、网络定位和基于代理的基准测试中的广泛实验验证了我们方法的有效性，确立了DAVE作为文档和网络应用的强大视觉编码器的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DAVE is a vision encoder designed for VLMs to enhance document understanding and web agent tasks by leveraging self-supervised and supervised pretraining on unlabeled and high-quality data, respectively. It incorporates a model-merging scheme and ensemble training to ensure broad compatibility and improved performance. Experiments show DAVE outperforms existing models on document tasks, VQAs, web localization, and agent-based benchmarks, making it a robust vision encoder for these applications.</div>
<div class="mono" style="margin-top:8px">DAVE 是一种专门为文档理解和网页代理任务设计的视觉编码器，通过无监督和有监督预训练分别利用未标注和高质量数据进行训练。它引入了模型合并方案和集成训练，以提高兼容性和特征融合。实验结果表明，DAVE 在经典文档任务、VQA、网页定位和基于代理的基准测试中均优于现有视觉编码器，使其成为文档和网页应用的稳健选择。</div>
</details>
</div>
<div class="card">
<div class="title">CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement</div>
<div class="meta-line">Authors: Wentao Zhang, Tao Fang, Lina Lu, Lifei Wang, Weihe Zhong</div>
<div class="meta-line">First: 2025-12-31T16:21:31+00:00 · Latest: 2025-12-31T16:21:31+00:00</div>
<div class="meta-line">Comments: This paper is 6 pages in length and contains 2 figures. Tao Fang (Corresponding Author), Lina Lu (Co-corresponding Author)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24947v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24947v1">PDF</a> · <a href="https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \textbf{+22.7} pp in disease classification and \textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CPJ: 通过 Caption-Prompt-Judge 与 LLM 判定修正实现可解释的农业害虫诊断</div>
<div class="mono" style="margin-top:8px">准确且可解释的作物疾病诊断对于农业决策至关重要，但现有方法往往依赖于昂贵的监督微调，并且在领域迁移时表现不佳。我们提出了一种无需训练的少量样本框架 Caption--Prompt--Judge (CPJ)，通过结构化、可解释的图像描述来增强农业害虫问答。CPJ 利用大型视觉-语言模型生成多角度描述，并通过一个 LLM-as-Judge 模块进行迭代修正，然后指导一个双答案问答过程，用于识别和管理响应。在 CDDMBench 上评估，CPJ 显著提高了性能：使用 GPT-5-mini 描述，GPT-5-Nano 在疾病分类上的得分提高了 \textbf{+22.7} 个百分点，在问答得分上提高了 \textbf{+19.5} 分，超过无描述基线。该框架提供了透明、基于证据的推理，无需微调即可实现稳健且可解释的农业诊断。我们的代码和数据已公开发布在：https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an accurate and interpretable method for crop disease diagnosis to support agricultural decision-making. The proposed Caption-Prompt-Judge (CPJ) framework uses large vision-language models to generate multi-angle captions, which are iteratively refined by an LLM-as-Judge module to inform a dual-answer VQA process. On the CDDMBench, CPJ significantly improves performance, achieving a 22.7 percentage point increase in disease classification and a 19.5 point increase in QA score over no-caption baselines, without requiring fine-tuning. The framework provides transparent, evidence-based reasoning for robust and explainable agricultural diagnosis.</div>
<div class="mono" style="margin-top:8px">研究旨在提高农业中作物疾病诊断的准确性和可解释性。提出的Caption-Prompt-Judge (CPJ)框架利用大型视觉-语言模型生成多角度的描述，这些描述通过LLM-as-Judge模块迭代优化，进而指导一个双答案的VQA系统进行识别和管理。在CDDMBench上，CPJ显著优于无描述基线，使用GPT-5-mini描述时，在疾病分类上提高了22.7个百分点，在问答得分上提高了19.5分，同时提供了透明的推理过程而无需微调。</div>
</details>
</div>
<div class="card">
<div class="title">ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</div>
<div class="meta-line">Authors: Abhijit Mishra, Mingda Li, Hsiang Fu, Richard Noh, Minji Kim</div>
<div class="meta-line">First: 2025-02-20T18:01:41+00:00 · Latest: 2025-12-31T15:43:05+00:00</div>
<div class="meta-line">Comments: Accepted and to appear in IJCNLP-AACL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14780v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.14780v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReVision：一种用于隐私保护任务导向视觉指令重写的数据集和基线VLM</div>
<div class="mono" style="margin-top:8px">随着AR、VR和配备强大摄像头的现代智能手机成为人机通信的主要接口，高效的隐私保护多模态交互变得至关重要。现有的强大视觉-语言模型（VLMs）支持多模态交互，通常依赖于基于云的处理，这引发了（1）视觉隐私问题，即传输敏感的视觉数据到服务器，以及（2）其有限的实时、设备端可用性问题。本文探讨了视觉指令重写这一新颖的方法，即将多模态指令转换为纯文本命令，允许轻量级设备端指令重写VLM（参数量250M）与现有对话AI系统的无缝集成，增强视觉数据隐私。为此，我们提供了一个涵盖14个领域的超过39,000个示例的数据集，并开发了一个紧凑的VLM，该模型在图像字幕数据集上进行预训练，并针对指令重写进行了微调。实验结果通过NLG指标（如BLEU、METEOR和ROUGE）以及语义解析分析评估，表明即使是最小量化版本的模型（存储占用&lt;500MB）也能实现有效的指令重写，从而实现以隐私为中心的多模态AI应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for efficient and privacy-preserving multimodal interaction by introducing ReVision, a dataset and baseline vision-language model for visual instruction rewriting. The model transforms multimodal instructions into text-only commands, enhancing privacy and on-device usability. Experiments show that even a quantized version of the model can effectively rewrite instructions, achieving good performance on NLG metrics and semantic parsing analysis.</div>
<div class="mono" style="margin-top:8px">该论文通过引入ReVision数据集和视觉指令重写的基本视觉语言模型，解决了高效且隐私保护的多模态交互需求。该模型将多模态指令转换为纯文本命令，增强隐私性和设备端的可用性。实验表明，即使是最小量化版本的模型也能有效重写指令，达到良好的自然语言生成指标，并能支持隐私导向的多模态AI应用。</div>
</details>
</div>
<div class="card">
<div class="title">Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach</div>
<div class="meta-line">Authors: Yuchen Jiao, Na Li, Changxiao Cai, Gen Li</div>
<div class="meta-line">First: 2025-12-31T15:35:53+00:00 · Latest: 2025-12-31T15:35:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24927v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24927v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.
  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一阶扩散采样器真的更慢吗？一种快速的前向值方法</div>
<div class="mono" style="margin-top:8px">高阶ODE求解器已成为加速扩散概率模型(DPM)采样的标准工具，这促使人们普遍认为一阶方法本质上更慢，并且提高离散化阶数是实现更快生成的主要途径。本文挑战了这一观点，并从互补的角度重新审视加速：除了求解器阶数之外，DPM评估在反向时间动力学中的位置会在低神经网络评估次数(NFE)区间内显著影响采样精度。
我们提出了一种新的无需训练的一阶采样器，其主要离散化误差与DDIM相反。从算法上讲，该方法通过廉价的一步前瞻预测器近似前向值评估。我们提供了理论保证，表明该采样器能够证明逼近理想的前向值轨迹，同时保持一阶收敛性。实验上，在标准图像生成基准（CIFAR-10、ImageNet、FFHQ和LSUN）上，所提出的采样器在相同的NFE预算下始终能提高样本质量，并且有时可以与最先进的高阶采样器竞争。总体而言，结果表明，DPM评估的位置提供了加速扩散采样的另一个独立设计角度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the belief that first-order diffusion samplers are inherently slower than higher-order methods. It proposes a novel first-order sampler that approximates the forward-value evaluation via a cheap one-step lookahead predictor, achieving first-order convergence while improving sample quality under the same neural function evaluation budget. Empirically, the sampler outperforms or matches state-of-the-art higher-order samplers across various image generation benchmarks.</div>
<div class="mono" style="margin-top:8px">本文挑战了第一阶扩散采样器比高阶方法更慢的观念。它提出了一种新型的第一阶采样器，通过廉价的一步前瞻预测器近似前向值评估，从而在相同的神经函数评估预算下提高样本质量。实验结果显示，该提出的方法在各种图像生成基准上可以超越或匹配最先进的高阶采样器。</div>
</details>
</div>
<div class="card">
<div class="title">Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control</div>
<div class="meta-line">Authors: Jason Armitage, Rico Sennnrich</div>
<div class="meta-line">First: 2025-12-31T12:39:03+00:00 · Latest: 2025-12-31T12:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24826v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>2D系统中2D视觉输入与3D多对象场景的语言对齐</div>
<div class="mono" style="margin-top:8px">跨模态系统在处理3D场景时面临维度跃迁问题，通过场景内相机可以弥合维度差距，但需要学习一个控制模块。我们提出了一种新方法，通过无导数优化实现遗憾最小化，以提高多元互信息估计。我们的算法使基于2D视觉输入训练的即插即用跨模态系统能够在线适应物体遮挡并区分特征。表达性度量与基于价值的优化相结合，帮助场景内相机直接从视觉-语言模型的嘈杂输出中学习。由此产生的流水线在多对象3D场景的跨模态任务中提高了性能，无需预训练或微调。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of processing 3D scenes by 2D cross-modal systems, introducing a method that uses regret minimisation with derivative-free optimisation to improve multivariate mutual information estimates. This approach enables the systems to adapt online to object occlusions and differentiate features, leading to better performance in cross-modal tasks on multi-object 3D scenes without pretraining or fine-tuning.</div>
<div class="mono" style="margin-top:8px">研究解决了2D跨模态系统处理3D场景时面临的维度差异问题，这些系统通常难以应对这种差异。作者提出了一种使用无导数优化的后悔最小化方法，以提高多变量互信息估计。这使得系统能够在线适应物体遮挡并区分特征，从而在多对象3D场景的跨模态任务中获得更好的性能，无需进行预训练或微调。</div>
</details>
</div>
<div class="card">
<div class="title">CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation</div>
<div class="meta-line">Authors: ZhenQi Chen, TsaiChing Ni, YuanFu Yang</div>
<div class="meta-line">First: 2025-12-27T19:08:18+00:00 · Latest: 2025-12-31T10:44:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22681v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22681v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt&#x27;s intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CritiFusion: 语义批评和光谱对齐以实现忠实的文本到图像生成</div>
<div class="mono" style="margin-top:8px">近期的文本到图像扩散模型在视觉保真度方面取得了显著进展，但往往难以与复杂的提示实现语义对齐。我们提出了一种名为CritiFusion的新型推理时框架，该框架结合了多模态语义批评机制和频域细化，以提高文本到图像的一致性和细节。所提出的CritiCore模块利用视觉语言模型和多个大型语言模型来丰富提示上下文并生成高层次的语义反馈，引导扩散过程更好地与提示的意图对齐。此外，SpecFusion在频域中合并中间生成状态，注入粗略的结构信息同时保留高频细节。无需额外的模型训练。CritiFusion作为与现有扩散主干兼容的插件细化阶段。在标准基准上的实验表明，我们的方法显著提高了文本到图像对应和视觉质量的人类对齐指标。CritiFusion在人类偏好评分和美学评估中持续提升性能，达到与最先进的奖励优化方法相当的结果。定性结果进一步证明了我们的语义批评和光谱对齐策略在细节、真实性和提示忠实度方面的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CritiFusion is a novel framework that enhances text-to-image generation by integrating a semantic critique mechanism and spectral alignment. It uses a vision-language model and multiple language models to enrich prompt context and guide the diffusion process, ensuring better alignment with the prompt&#x27;s intent. Additionally, it merges intermediate generation states in the spectral domain to preserve high-frequency details while incorporating coarse structural information. Experiments show that CritiFusion improves human-aligned metrics, aesthetic evaluations, and human preference scores, achieving results comparable to state-of-the-art reward optimization approaches.</div>
<div class="mono" style="margin-top:8px">CritiFusion 是一种新颖的框架，通过集成多模态语义批评机制和频域对齐来增强文本到图像生成。它使用视觉语言模型和多个大型语言模型来丰富提示上下文并引导扩散过程，确保更好地与提示的意图对齐。此外，它在频域中合并中间生成状态，以保留高频细节。实验表明，CritiFusion 改进了人类对齐的指标、美学评估和人类偏好评分，达到了与最先进的奖励优化方法相当的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Fact-Checking: An Agent-based Approach</div>
<div class="meta-line">Authors: Danni Xu, Shaojing Fan, Harry Cheng, Mohan Kankanhalli</div>
<div class="meta-line">First: 2025-12-28T13:58:33+00:00 · Latest: 2025-12-31T09:37:15+00:00</div>
<div class="meta-line">Comments: Code and dataset will be released at https://github.com/xudanni0927/AgentFact</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22933v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22933v2">PDF</a> · <a href="https://github.com/xudanni0927/AgentFact">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于代理的多模态事实核查：一种代理导向的方法</div>
<div class="mono" style="margin-top:8px">多模态错误信息的快速传播对自动化事实核查系统构成了日益严峻的挑战。现有的方法，包括大型视觉语言模型（LVLM）和深度多模态融合方法，往往由于推理能力有限和证据利用浅显而效果不佳。一个关键瓶颈是没有专门的数据集提供完整的现实世界多模态错误信息实例及其注释的推理过程和可验证的证据。为了解决这一限制，我们引入了RW-Post，这是一个高质量且可解释的现实世界多模态事实核查数据集。RW-Post将现实世界多模态声明与其原始社交媒体帖子对齐，保留了声明中丰富的上下文信息。此外，该数据集还包括详细的推理过程和明确链接的证据，这些证据是通过大型语言模型辅助提取管道从人类撰写的事实核查文章中提取出来的，从而实现全面的验证和解释。基于RW-Post，我们提出了AgentFact，这是一种代理导向的多模态事实核查框架，旨在模拟人类验证工作流程。AgentFact 包含五个专门的代理，它们协作处理关键的事实核查子任务，包括策略规划、高质量证据检索、视觉分析、推理和解释生成。这些代理通过迭代工作流协调，该工作流在证据搜索和任务感知证据过滤与推理之间交替进行，促进战略决策和系统性证据分析。广泛的实验结果表明，RW-Post 和 AgentFact 的协同作用显著提高了多模态事实核查的准确性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of automated fact-checking for multimodal misinformation by introducing RW-Post, a new dataset that includes detailed reasoning and linked evidence, and AgentFact, an agent-based framework that emulates human verification processes. The framework consists of five specialized agents that handle key subtasks such as evidence retrieval and reasoning. Experiments show that this approach significantly enhances the accuracy and interpretability of multimodal fact-checking.</div>
<div class="mono" style="margin-top:8px">该论文通过引入RW-Post数据集，提供完整的现实世界多模态虚假信息及其注释的推理和证据，解决了自动化多模态虚假信息查证的挑战。基于RW-Post，作者提出了AgentFact框架，该框架协作处理关键的查证任务。实验结果表明，AgentFact在多模态查证的准确性和可解释性上都优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving</div>
<div class="meta-line">Authors: Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li</div>
<div class="meta-line">First: 2025-12-28T14:06:37+00:00 · Latest: 2025-12-31T09:18:13+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures. Project page: https://pqh22.github.io/projects/ColaVLA/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22939v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22939v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pqh22.github.io/projects/ColaVLA/index.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ColaVLA：利用认知潜在推理进行自主驾驶分层并行轨迹规划</div>
<div class="mono" style="margin-top:8px">自主驾驶需要从复杂的多模态输入中生成安全可靠的轨迹。传统模块化管道将感知、预测和规划分离，而近期的端到端（E2E）系统则联合学习它们。视觉语言模型（VLMs）进一步丰富了这一范式，通过引入跨模态先验和常识推理，但当前基于VLM的规划器面临三个关键挑战：(i) 离散文本推理与连续控制之间的不匹配，(ii) 自回归链式思考解码的高延迟，以及(iii) 低效或非因果规划器，限制了实时部署。我们提出ColaVLA，这是一种统一的视觉语言行动框架，将推理从文本转移到统一的潜在空间，并与分层并行轨迹解码器耦合。认知潜在推理器通过自我适应选择将场景理解压缩为决策导向的元动作嵌入，仅需两次VLM前向传递。分层并行规划器则在单次前向传递中生成多尺度、因果一致的轨迹。这些组件共同保留了VLM的泛化能力和可解释性，同时实现了高效、准确和安全的轨迹生成。在nuScenes基准测试中，ColaVLA在开环和闭环设置中均实现了最先进的性能，具有良好的效率和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ColaVLA addresses the challenges of traditional modular pipelines and recent end-to-end systems in autonomous driving by proposing a unified vision-language-action framework. It uses a Cognitive Latent Reasoner to compress scene understanding into compact embeddings and a Hierarchical Parallel Planner to generate multi-scale, causality-consistent trajectories. Experiments show that ColaVLA outperforms existing methods in terms of efficiency and robustness on the nuScenes benchmark, achieving state-of-the-art performance in both open-loop and closed-loop settings.</div>
<div class="mono" style="margin-top:8px">ColaVLA 通过利用统一的视觉-语言-动作框架来解决生成安全可靠轨迹的挑战。它使用认知潜空间推理器将场景理解压缩为紧凑的元动作嵌入，并使用层次并行规划器在单次前向传递中生成多尺度、因果一致的轨迹。实验表明，ColaVLA 在开放环和闭环设置中均优于现有方法，在性能、效率和鲁棒性方面均表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving</div>
<div class="meta-line">Authors: Qian Cheng, Weitao Zhou, Cheng Jing, Nanshan Deng, Junze Wen, Zhaoyang Liu, Kun Jiang, Diange Yang</div>
<div class="meta-line">First: 2025-12-31T08:27:10+00:00 · Latest: 2025-12-31T08:27:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24712v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers&#x27; gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LSRE：自主驾驶中的实时语义风险检测的潜在语义规则编码</div>
<div class="mono" style="margin-top:8px">现实世界中的自主驾驶必须遵守复杂的社会规则，这些规则超出了法律规定的交通法规。许多语义约束，如为紧急车辆让路、遵守交通警察的手势或为校车停车，对于人类来说是直观的，但很难明确编码。尽管大型视觉-语言模型（VLMs）可以解释这些语义，但它们的推理成本使其在实时部署中不切实际。本文提出了一种LSRE（潜在语义规则编码）框架，将稀疏采样的VLM判断转化为递归世界模型潜在空间中的决策边界。通过将语言定义的安全语义编码到轻量级的潜在分类器中，LSRE能够在10 Hz的频率下进行实时语义风险评估，而无需每帧查询VLM。在CARLA上的六个语义失败场景实验表明，LSRE在语义风险检测准确性方面与大型VLM基线相当，同时提供显著更早的危险预见，并保持较低的计算延迟。此外，LSRE还能够泛化到罕见的语义相似测试案例，表明语言引导的潜在分类为自主驾驶中的语义安全监控提供了一种有效且可部署的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The work addresses the challenge of real-time semantic risk detection in autonomous driving, where complex social rules beyond traffic regulations must be followed. It proposes LSRE, which converts VLM judgments into decision boundaries in a recurrent world model&#x27;s latent space, enabling real-time semantic risk assessment at 10 Hz without querying VLMs per frame. Experiments show LSRE matches a large VLM baseline in accuracy but with earlier hazard anticipation and low computational latency, and it generalizes well to unseen semantic-similar cases.</div>
<div class="mono" style="margin-top:8px">该研究针对自动驾驶中复杂的社交规则进行实时语义风险检测的挑战，提出LSRE框架，将VLM判断转化为递归世界模型潜在空间中的决策边界，实现每秒10次的实时语义风险评估，无需每帧查询VLM。实验表明，LSRE在准确性和早期危险预判方面与大型VLM基线相当，且具有较低的计算延迟，并且能够很好地泛化到未见过的语义相似测试案例。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</div>
<div class="meta-line">Authors: Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao</div>
<div class="meta-line">First: 2025-12-31T08:10:03+00:00 · Latest: 2025-12-31T08:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24702v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24702v1">PDF</a> · <a href="https://github.com/AHideoKuzeA/Evol-SAM3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &quot;generate-then-segment&quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &quot;Generate-Evaluate-Evolve&quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演化而非训练：通过演化提示实现零样本推理分割</div>
<div class="mono" style="margin-top:8px">推理分割要求模型解释复杂的、上下文相关的语言查询以实现像素级定位。当前主流方法主要依赖监督微调（SFT）或强化学习（RL）。然而，SFT容易出现灾难性遗忘和领域依赖性问题，而RL则常常受到训练不稳定性及对预定义奖励函数的严格依赖的困扰。尽管最近的无训练方法绕过了这些训练负担，但它们本质上受限于静态推理范式。这些方法通常依赖于一次性的“生成-分割”链，这导致推理深度不足，缺乏自我纠正语言幻觉或空间误解的能力。在本文中，我们挑战这些限制并提出EVOL-SAM3，这是一种新颖的零样本框架，将推理分割重新定义为推理时的演化搜索过程。EVOL-SAM3 不依赖于固定提示，而是维护一组提示假设，并通过“生成-评估-演化”循环逐步优化它们。我们引入了视觉竞技场来通过参考无关的两两对决评估提示适应度，并引入语义变异操作来注入多样性并纠正语义错误。此外，异构竞技场模块将几何先验与语义推理结合，以确保最终选择的鲁棒性。大量实验表明，EVOL-SAM3 不仅在零样本设置下显著优于静态基线，还在具有挑战性的ReasonSeg基准上显著超越完全监督的最新方法。代码可在https://github.com/AHideoKuzeA/Evol-SAM3 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of current reasoning segmentation methods, which rely on either supervised fine-tuning or reinforcement learning, by proposing EVOL-SAM3. This framework reformulates reasoning segmentation as an evolutionary search process at inference time, maintaining a population of prompt hypotheses and iteratively refining them. It uses a Visual Arena for prompt fitness assessment and a Semantic Mutation operator to inject diversity and correct semantic errors. The method outperforms static baselines and fully supervised state-of-the-art models on the ReasonSeg benchmark in a zero-shot setting, demonstrating significant improvements in performance and robustness. The code is available on GitHub.</div>
<div class="mono" style="margin-top:8px">论文针对当前依赖监督微调或强化学习的推理分割方法的局限性，提出了EVOL-SAM3框架，将推理分割重新定义为推理时的进化搜索过程，维护一个提示假设的群体并迭代优化。该方法使用视觉竞技场进行提示适应性评估，并使用语义变异操作符注入多样性并纠正语义错误。该方法在零样本设置下显著优于静态基线和完全监督的最新方法，在ReasonSeg基准测试中表现出显著的性能和鲁棒性提升。代码可在GitHub上获取。</div>
</details>
</div>
<div class="card">
<div class="title">Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention</div>
<div class="meta-line">Authors: Zhen Yang, Mingyang Zhang, Feng Chen, Ganggui Ding, Liang Hou, Xin Tao, Pengfei Wan, Ying-Cong Chen</div>
<div class="meta-line">First: 2025-10-15T17:59:45+00:00 · Latest: 2025-12-31T07:36:24+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/EnVision-Research/MTI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13940v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13940v2">PDF</a> · <a href="https://github.com/EnVision-Research/MTI">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model&#x27;s KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少即是多：通过最小化测试时干预提高LLM推理能力</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的近期进展集中在通过增加推理计算来提高测试时的推理能力，但往往以效率为代价。我们重新审视测试时的行为，并发现一个简单但尚未充分探索的现象：推理不确定性是高度局部化的——只有少量高熵令牌主要影响输出的正确性。受此启发，我们提出了最小化测试时干预（MTI），这是一种无需训练的框架，通过最小的开销来增强推理准确性和稳定性。MTI 包括：(i) 选择性CFG干预，在不确定位置仅应用分类器自由引导；(ii) 轻量级负提示引导，重用主模型的KV缓存以高效地近似无条件解码。MTI 在通用任务、编程任务和STEM任务中均表现出一致的改进——例如，DeepSeek-R1-7B在六个基准上的平均改进为+9.28%，AIME2024使用Ling-mini-2.0时为+11.25%，同时保持高度高效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the inefficiency of test-time scaling in large language models (LLMs) by proposing Minimal Test-Time Intervention (MTI), which enhances reasoning accuracy and stability with minimal overhead. MTI uses selective classifier-free guidance and lightweight negative-prompt guidance, focusing on uncertain positions. It achieves consistent gains across various tasks, such as +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0, while maintaining efficiency.</div>
<div class="mono" style="margin-top:8px">论文针对大型语言模型（LLMs）在测试时放大带来的低效率问题，提出了最小测试时干预（MTI）框架，以最小的开销提升推理准确性和稳定性。MTI 选择性地应用分类器无指导干预，并使用轻量级的负提示指导，利用主模型的 KV 缓存。它在各种任务中实现了持续的改进，例如在六个基准测试中 DeepSeek-R1-7B 提升了 9.28%，在 AIME2024 中使用 Ling-mini-2.0 提升了 11.25%，同时保持了高效性。</div>
</details>
</div>
<div class="card">
<div class="title">ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</div>
<div class="meta-line">Authors: Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</div>
<div class="meta-line">Venue: AAAI 2026 poster</div>
<div class="meta-line">First: 2025-12-19T07:27:19+00:00 · Latest: 2025-12-31T06:37:00+00:00</div>
<div class="meta-line">Comments: Accepted for poster presentation at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17298v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model&#x27;s temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProCache：基于约束的特征缓存与选择性计算以加速扩散变换器</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiTs）在生成建模中取得了最先进的性能，但其高昂的计算成本阻碍了实时部署。虽然特征缓存通过利用时间冗余提供了一种无训练的加速解决方案，但现有方法存在两个关键限制：（1）均匀的缓存间隔无法与DiT的时间非均匀动态对齐；（2）使用过大的缓存间隔进行简单的特征重用会导致严重的误差累积。在本文中，我们分析了去噪过程中DiT特征的演变，发现特征变化和误差传播在时间和深度上都高度变化。受此启发，我们提出了ProCache，这是一种基于约束的动态特征缓存框架，通过两个核心组件解决了这些问题：（i）一种约束感知的缓存模式搜索模块，通过离线约束采样生成非均匀的激活时间表，以适应模型的时间特性；（ii）一种选择性计算模块，在深层块和高重要性标记中选择性地计算缓存段，以最小的开销减轻误差累积。在PixArt-alpha和DiT上的广泛实验表明，ProCache在几乎不降低质量的情况下实现了高达1.96倍和2.90倍的加速，显著优于先前的基于缓存的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProCache is a training-free dynamic feature caching framework designed to accelerate Diffusion Transformers (DiTs) by addressing the limitations of uniform caching intervals and excessive error accumulation. It uses a constraint-aware caching pattern search module to generate non-uniform activation schedules and a selective computation module to minimize error propagation. Experiments show that ProCache can achieve up to 1.96x and 2.90x acceleration with negligible quality degradation compared to previous caching-based methods.</div>
<div class="mono" style="margin-top:8px">ProCache 是一个无需训练的动态特征缓存框架，旨在通过解决均匀缓存间隔和过度误差累积的问题来加速扩散变换器（DiTs）。它使用一个约束感知的缓存模式搜索模块生成非均匀激活调度，并使用一个选择性计算模块来最小化误差传播。实验表明，ProCache 可以实现最高 1.96 倍和 2.90 倍的加速，同时保持质量基本不变，显著优于之前的基于缓存的方法。</div>
</details>
</div>
<div class="card">
<div class="title">CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection</div>
<div class="meta-line">Authors: Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</div>
<div class="meta-line">First: 2025-10-16T15:27:10+00:00 · Latest: 2025-12-31T05:45:29+00:00</div>
<div class="meta-line">Comments: 28 pages, 13 Figures, 12 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14792v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14792v2">PDF</a> · <a href="https://github.com/hchoi256/cotpl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. Code and models are available at https://github.com/hchoi256/cotpl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoT-PL：视觉链式推理与伪标签结合在开放词汇对象检测中的应用</div>
<div class="mono" style="margin-top:8px">开放词汇对象检测（OVD）旨在识别和定位训练期间未见过的对象类别。近期方法通常利用视觉语言模型（VLMs）通过图像-文本对齐生成伪标签，使检测器能够在没有显式监督的情况下泛化到未见过的类别。然而，这些方法高度依赖直接的图像-文本匹配，忽视了解释语义复杂场景所必需的中间推理步骤。这导致在拥挤或遮挡的视觉上下文中表现有限。本文提出了一种新的框架CoT-PL，该框架将结构化的视觉链式推理（CoT）融入伪标签生成过程。CoT-PL将对象理解分解为三个可解释的步骤：（1）即使对于未见过的对象也能感知区域，（2）通过零样本推理进行类别识别，（3）背景定位以分离语义复杂的对象。最关键的是，第三步自然地促使我们使用预先计算的背景线索作为负样本，促进对象和背景之间的特征解耦。这样，CoT推理和对比背景学习（CBL）形成了一条针对拥挤或遮挡场景的集成流水线，以实现稳健的伪标签生成。值得注意的是，在这两种情况下，我们对新类伪标签的质量分别比最佳先前方法提高了103.4%和168.4%。我们的大量实验表明，CoT-PL在开放词汇COCO数据集上实现了+7.7 AP50，在LVIS数据集上实现了+2.9掩码AP，创下了新的最佳水平。代码和模型可在https://github.com/hchoi256/cotpl获取。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning</div>
<div class="meta-line">Authors: Fuyu Dong, Ke Li, Di Wang, Nan Luo, Yiming Zhang, Kaiyu Li, Jianfei Yang, Quan Wang</div>
<div class="meta-line">First: 2025-12-31T03:28:17+00:00 · Latest: 2025-12-31T03:28:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24591v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过决策模糊性引导的强化微调改进少样本变化检测视觉问答</div>
<div class="mono" style="margin-top:8px">变化检测视觉问答（CDVQA）需要通过推理生物时相遥感图像中的语义变化来回答文本查询。一种直接的方法是通过监督微调（SFT）增强CDVQA性能。尽管取得了近期进展，我们观察到，大量失败并非源自明显错误的预测，而是决策模糊性，即模型对正确答案和强干扰项赋予了相似的置信度。为了正式化这一挑战，我们将决策模糊样本（DAS）定义为真实答案与最竞争替代方案之间概率差距较小的实例。我们认为，明确优化DAS对于提高CDVQA模型的可区分性和鲁棒性至关重要。为此，我们提出了DARFT框架，该框架首先使用SFT训练的参考策略挖掘DAS，然后在挖掘的子集上应用组相对策略优化。通过利用多样本解码和组内相对优势，DARFT抑制了强干扰项并细化了决策边界，而无需额外监督。大量实验表明，DARFT在少样本设置中相对于SFT基线具有一致的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of decision ambiguity in change detection visual question answering (CDVQA) by defining Decision-Ambiguous Samples (DAS) and proposing DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework. This framework mines DAS using an SFT-trained reference policy and applies group-relative policy optimization, which suppresses strong distractors and sharpens decision boundaries. Experiments show consistent improvements over supervised fine-tuning baselines, especially in few-shot settings.</div>
<div class="mono" style="margin-top:8px">论文通过定义决策模糊样本（DAS）并提出决策模糊引导强化微调（DARFT）框架来解决变化检测视觉问答（CDVQA）中的决策模糊问题。DARFT 使用经过监督微调（SFT）训练的参考策略挖掘 DAS，并应用组相对策略优化以提高模型的区分能力和鲁棒性。实验结果显示，在少量样本设置下，DARFT 优于监督微调基线。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time</div>
<div class="meta-line">Authors: Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun</div>
<div class="meta-line">First: 2025-12-31T02:46:04+00:00 · Latest: 2025-12-31T02:46:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24574v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24574v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解与引导测试时推理模型的认知行为</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常依赖长链推理（CoT）来解决复杂任务。虽然有效，但这些路径往往效率低下，导致因过度生成标记而产生高延迟，或者产生不稳定推理，交替出现浅层且不一致的推理和冗长且重复的推理。在本工作中，我们研究了推理路径的结构，并发现与验证和回溯等不同认知行为相关的专门注意头。通过在推理时轻柔地干预这些头，我们可以引导模型远离低效模式。基于这一见解，我们提出了CREST，一种无需训练的方法，用于测试时的认知推理引导。CREST有两个组成部分：（1）一个离线校准步骤，用于识别认知头并推导出特定于头的引导向量，以及（2）一个推理时的程序，用于旋转隐藏表示以抑制沿这些向量的成分。CREST自适应地抑制无生产力的推理行为，从而提高准确性和降低计算成本。在各种推理基准和模型上，CREST将准确率提高了最多17.5%，同时减少了37.6%的标记使用量，提供了一条简单而有效的快速、可靠LLM推理途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the inefficiencies in reasoning trajectories of large language models (LLMs) and proposes CREST, a training-free method for steering cognitive behaviors at test-time. By identifying specialized attention heads associated with verification and backtracking, CREST suppresses unproductive reasoning, improving accuracy by up to 17.5% and reducing token usage by 37.6%. This method enhances both the reliability and efficiency of LLMs in solving complex tasks.</div>
<div class="mono" style="margin-top:8px">该研究探讨了大型语言模型在使用长链推理时的低效问题，并提出了一种名为CREST的无训练方法来引导模型进行更高效的推理。通过识别与验证和回溯等认知行为相关的特殊注意力头，CREST抑制了无用的推理过程，从而提高了准确性和降低了计算成本。在各种基准测试中，CREST将准确率提高了最多17.5%，同时减少了37.6%的令牌使用量。</div>
</details>
</div>
<div class="card">
<div class="title">PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</div>
<div class="meta-line">Authors: Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou</div>
<div class="meta-line">First: 2025-12-31T01:19:14+00:00 · Latest: 2025-12-31T01:19:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24551v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24551v1">PDF</a> · <a href="https://github.com/caiyuanhao1998/Open-PhyGDPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://caiyuanhao1998.github.io/project/PhyGDPO">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhyGDPO：物理感知的组内直接偏好优化方法以实现物理一致的文本到视频生成</div>
<div class="mono" style="margin-top:8px">近年来，文本到视频(T2V)生成取得了良好的视觉效果，但合成严格遵循物理定律的视频仍然是一个开放的挑战。现有方法主要基于图形或提示扩展，难以泛化到复杂的模拟环境或学习隐含的物理推理。训练数据中缺乏丰富的物理交互和现象也是一个问题。在本文中，我们首先引入了一个物理增强的视频数据构建流水线PhyAugPipe，利用具有链式推理的视觉语言模型(VLM)收集大规模训练数据集PhyVidGen-135K。然后，我们提出了一个基于组内Plackett-Luce概率模型的物理感知的组内直接偏好优化PhyGDPO框架，以捕捉超越成对比较的整体偏好。在PhyGDPO中，我们设计了一种物理引导奖励(PGR)方案，将基于VLM的物理奖励嵌入其中，以引导优化向物理一致性方向发展。我们还提出了一种LoRA-Switch参考(LoRA-SR)方案，以消除内存密集型的参考重复，实现高效的训练。实验表明，我们的方法在PhyGenBench和VideoPhy2上显著优于最先进的开源方法。请访问我们的项目页面https://caiyuanhao1998.github.io/project/PhyGDPO获取更多视频结果。我们的代码、模型和数据将在https://github.com/caiyuanhao1998/Open-PhyGDPO发布</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generating physically consistent videos from text descriptions. It introduces PhyAugPipe, a pipeline that uses a vision-language model for data augmentation, and PhyGDPO, a framework that optimizes video generation based on groupwise Plackett-Luce preferences and physics-guided rewards. The method outperforms existing open-source approaches on PhyGenBench and VideoPhy2 benchmarks, demonstrating improved physical consistency in generated videos.</div>
<div class="mono" style="margin-top:8px">本文旨在解决从文本描述生成物理上一致的视频的挑战。它引入了PhyAugPipe，一种使用视觉语言模型进行数据增强的管道，以及PhyGDPO框架，该框架基于群体偏好和物理奖励优化视频生成。该方法在PhyGenBench和VideoPhy2基准测试中显著优于现有开源方法。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways</div>
<div class="meta-line">Authors: Vladimir Frants, Sos Agaian</div>
<div class="meta-line">First: 2025-12-30T22:53:33+00:00 · Latest: 2025-12-30T22:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24499v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24499v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of generative AI has normalized large-scale synthetic media creation, enabling new forms of covert communication. Recent generative steganography methods, particularly those based on diffusion models, can embed high-capacity payloads without fine-tuning or auxiliary decoders, creating significant challenges for detection and remediation. Coverless diffusion-based techniques are difficult to counter because they generate image carriers directly from secret data, enabling attackers to deliver stegomalware for command-and-control, payload staging, and data exfiltration while bypassing detectors that rely on cover-stego discrepancies. This work introduces Adversarial Diffusion Sanitization (ADS), a training-free defense for security gateways that neutralizes hidden payloads rather than detecting them. ADS employs an off-the-shelf pretrained denoiser as a differentiable proxy for diffusion-based decoders and incorporates a color-aware, quaternion-coupled update rule to reduce artifacts under strict distortion limits. Under a practical threat model and in evaluation against the state-of-the-art diffusion steganography method Pulsar, ADS drives decoder success rates to near zero with minimal perceptual impact. Results demonstrate that ADS provides a favorable security-utility trade-off compared to standard content transformations, offering an effective mitigation strategy against diffusion-driven steganography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需训练的色彩感知对抗扩散净化：针对安全网关的扩散型隐秘软件防御</div>
<div class="mono" style="margin-top:8px">生成式AI的迅速发展使大规模合成媒体的创建变得司空见惯，开启了新的隐蔽通信形式。最近基于扩散模型的生成式隐写术方法，无需微调或辅助解码器即可嵌入高容量载荷，给检测和修复带来了巨大挑战。无伪装的基于扩散的技术难以对抗，因为它们直接从秘密数据生成图像载体，使攻击者能够通过绕过依赖于伪装-隐写差异的检测器来交付隐秘软件，实现命令与控制、载荷部署和数据泄露。本文提出了一种无需训练的对抗扩散净化（ADS），这是一种针对安全网关的防御方法，旨在中和隐藏的载荷而非检测它们。ADS 使用现成的预训练去噪器作为可微代理扩散型解码器，并结合色彩感知的四元数耦合更新规则，在严格失真限制下减少伪影。在实际威胁模型下，与最先进的扩散隐写术方法Pulsar进行评估，ADS 将解码成功率驱动至接近零，同时对感知影响最小。结果表明，ADS 提供了与标准内容转换相比更有利的安全-效用权衡，为对抗驱动扩散的隐写术提供了一种有效的缓解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of detecting and mitigating diffusion-based steganography methods that embed high-capacity payloads covertly in images. It introduces Adversarial Diffusion Sanitization (ADS), a training-free defense mechanism that uses an off-the-shelf denoiser as a differentiable proxy for diffusion-based decoders and incorporates a color-aware, quaternion-coupled update rule to neutralize hidden payloads. Under a practical threat model, ADS significantly reduces decoder success rates to near zero with minimal perceptual impact, demonstrating a favorable security-utility trade-off compared to standard content transformations.</div>
<div class="mono" style="margin-top:8px">这项工作针对基于扩散模型的隐写术挑战，该隐写术不需微调即可在图像中隐蔽嵌入高容量负载。提出的对抗扩散净化（ADS）使用现成的预训练去噪器作为扩散解码器的可微代理，并包含一种颜色感知的四元数耦合更新规则以减少伪影。评估结果显示，ADS可以在几乎无感知影响的情况下将解码成功率降低到接近零，相比标准内容变换提供更优的安全-实用性权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</div>
<div class="meta-line">Authors: Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert</div>
<div class="meta-line">First: 2025-12-30T21:20:41+00:00 · Latest: 2025-12-30T21:20:41+00:00</div>
<div class="meta-line">Comments: 17 pages without bibliography or appendix. The main paper has 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24470v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24470v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert-&gt;fallback maneuver-&gt;operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird&#x27;s-eye-view perception and short-horizon replanning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>桥梁上的基础模型：基于视觉语言模型的海上自主航行语义风险检测与安全机动</div>
<div class="mono" style="margin-top:8px">国际海事组织（IMO）的MASS代码草案要求自主和远程监督的海上船舶在偏离其操作设计域时能够检测到这种情况，进入预定义的后备模式通知操作员，允许立即的人工干预，并在未经批准的情况下不得更改航程计划。在警报到接管的间隙满足这些义务需要一种短时间范围、可人工干预的后备机动。传统的海上自主系统在需要理解意义的情况下（例如，潜水员标志意味着水中有人员，火意味着危险）难以应对。我们认为（i）视觉语言模型（VLMs）为这些分布外情况提供了语义意识，（ii）快速-慢速异常检测流水线与短时间范围、可人工干预的后备机动使这一操作在交接窗口内成为可能。我们引入了语义瞭望，这是一种仅使用摄像头、候选受限的视觉语言模型（VLM）后备机动选择器，它在持续的人类监督下从水有效、世界锚定的轨迹中选择一个谨慎的动作（或保持位置）。在40个港口场景中，我们测量了每次呼叫的场景理解和延迟，与人类共识（模型三票多数投票）的对齐，以及在火灾危险场景中的短时间范围风险缓解，并进行了水上警报-&gt;后备机动-&gt;操作员交接。亚10秒的模型保留了大多数先进模型的大部分意识。后备机动选择器优于仅几何模型基准，并在火灾场景中增加了安全距离。现场运行验证了端到端操作。这些结果支持VLMs作为与IMO MASS代码草案兼容的语义后备机动选择器，符合实际的延迟预算，并激励未来工作，即领域适应的混合自主，将基础模型语义与多传感器鸟瞰感知和短时间范围重新规划相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to address the need for autonomous maritime vessels to detect and respond to hazards with semantic understanding and human override capabilities. The study introduces Semantic Lookout, a vision-language model-based fallback maneuver selector that selects cautious actions or station-keeping under continuous human authority. Key findings include sub-10 second latency, alignment with human consensus, and increased standoff distance on fire scenes compared to geometry-only baselines. The system meets the requirements of the draft IMO MASS Code and supports the practical implementation of semantic fallback maneuvers in maritime autonomy systems.</div>
<div class="mono" style="margin-top:8px">研究旨在解决自主海上船舶在检测和应对危险时需要具备语义理解和人工干预能力的需求。该研究引入了基于视觉-语言模型的备用机动选择器Semantic Lookout，在持续的人类监督下选择谨慎行动或保持原位。关键发现包括不到10秒的延迟、与人类共识的对齐以及与几何基线相比，在火灾场景中增加了安全距离。该系统符合IMO MASS代码草案的要求，并支持在海上自主系统中实现语义备用机动的实用实施。</div>
</details>
</div>
<div class="card">
<div class="title">DermaVQA-DAS: Dermatology Assessment Schema (DAS) &amp; Datasets for Closed-Ended Question Answering &amp; Segmentation in Patient-Generated Dermatology Images</div>
<div class="meta-line">Authors: Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy</div>
<div class="meta-line">First: 2025-12-30T16:48:20+00:00 · Latest: 2025-12-30T16:48:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24340v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://osf.io/72rp3">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (https://osf.io/72rp3).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DermaVQA-DAS：皮肤病评估方案（DAS）及数据集，用于患者生成的皮肤病图像的封闭式问题回答与分割</div>
<div class="mono" style="margin-top:8px">皮肤病图像分析的最新进展得益于大规模标注数据集；然而，现有大多数基准主要集中在皮肤镜图像上，缺乏患者撰写的查询和临床背景，限制了其在以患者为中心的护理中的应用。为解决这一问题，我们引入了DermaVQA-DAS，这是DermaVQA数据集的扩展，支持两种互补任务：封闭式问题回答（QA）和皮肤病病灶分割。本研究的核心是皮肤病评估方案（DAS），这是一种新颖的专家开发框架，系统地以结构化和标准化形式捕捉临床有意义的皮肤病特征。DAS 包含36个高层次和27个细粒度评估问题，其中包含英文和中文的多项选择题。利用DAS，我们提供了专家标注的数据集，用于封闭式QA和分割，并对最先进的多模态模型进行了基准测试。对于分割，我们评估了多种提示策略，并展示了提示设计对性能的影响：默认提示在Mean-of-Max和Mean-of-Mean评估聚合方案下表现最佳，而结合患者查询标题和内容的增强提示在基于多数投票的微评分评估下表现最佳，使用BiomedParse时Jaccard指数为0.395，Dice得分为0.566。对于封闭式QA，模型的整体性能很强，平均准确率从0.729到0.798不等；o3的整体准确率最高（0.798），紧随其后的是GPT-4.1（0.796），而Gemini-1.5-Pro在Gemini家族中表现出竞争力（0.783）。我们公开发布了DermaVQA-DAS、DAS方案和评估协议，以支持和加速未来在以患者为中心的皮肤病视觉语言建模方面的研究（https://osf.io/72rp3）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces DermaVQA-DAS, an extension of the DermaVQA dataset that supports closed-ended question answering and dermatological lesion segmentation using a Dermatology Assessment Schema (DAS). DAS includes 36 high-level and 27 fine-grained questions with multiple-choice options. For segmentation, the default prompt achieves the best results with a Jaccard index of 0.395 and a Dice score of 0.566. For closed-ended QA, models like o3 and GPT-4.1 show strong performance with accuracies ranging from 0.729 to 0.798, with o3 achieving the highest accuracy of 0.798. The datasets and evaluation protocols are publicly available to support future research in dermatological vision-language modeling.</div>
<div class="mono" style="margin-top:8px">研究引入了DermaVQA-DAS，这是一个扩展的DermaVQA数据集，支持使用Dermatology Assessment Schema (DAS)进行闭合式问题回答和皮肤病变分割。DAS包含36个高层次和27个细粒度的问题，具有多选选项。对于分割，使用默认提示可以获得最佳结果，Jaccard指数为0.395，Dice分数为0.566。对于闭合式问题回答，模型如o3和GPT-4.1表现出很强的性能，准确率范围在0.729到0.798之间，其中o3的准确率最高，为0.798。数据集和评估协议已公开发布，以支持未来在皮肤病变视觉语言建模方面的研究。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-aware Vision Language Model for Autonomous Driving</div>
<div class="meta-line">Authors: Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong</div>
<div class="meta-line">First: 2025-12-30T16:35:00+00:00 · Latest: 2025-12-30T16:35:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24331v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具备空间意识的视觉语言模型在自动驾驶中的应用</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型（VLMs）通过嵌入在语言模型中的常识在端到端的自动驾驶中展现出显著的潜力，但它们依赖于2D图像线索进行复杂场景理解和决策，这成为确保安全性和可靠性的关键瓶颈。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致不可靠的驾驶策略。为解决这一问题，我们提出了一种名为LVLDrive（LiDAR-视觉-语言）的新框架，该框架通过引入LiDAR点云作为额外输入模态，专门设计用于增强现有VLMs的稳健的3D度量空间理解能力，以适应自动驾驶的需求。一个关键挑战在于减轻3D数据引入的灾难性干扰对预训练VLMs的影响。为此，我们引入了一种渐进融合Q-Former，逐步注入LiDAR特征，确保VLMs的稳定性及其现有知识库的保存。此外，我们还开发了一个空间意识问答（SA-QA）数据集，以明确教授模型先进的3D感知和推理能力。在驾驶基准上的广泛实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅基于视觉的模型。我们的工作强调了构建可信赖的基于VLM的自动驾驶系统时明确的3D度量数据的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the spatial reasoning capabilities of Vision-Language Models (VLMs) for autonomous driving by integrating LiDAR data. The proposed LVLDrive framework introduces a Gradual Fusion Q-Former to incrementally incorporate LiDAR features into pre-trained VLMs, ensuring stability. The study also develops a spatial-aware question-answering dataset to train models in 3D perception and reasoning. Experiments show that LVLDrive outperforms vision-only models in scene understanding, metric spatial perception, and driving decision-making, emphasizing the importance of 3D data for reliable autonomous systems.</div>
<div class="mono" style="margin-top:8px">研究旨在通过集成LiDAR数据来增强Vision-Language Models (VLMs)的空间推理能力，以实现自主驾驶。提出的LVLDrive框架逐步将LiDAR特征融合到预训练的VLMs中，以提高3D度量空间理解。关键实验结果表明，LVLDrive在场景理解、度量空间感知和驾驶决策方面优于仅基于视觉的模型，突显了可靠自主系统中3D数据的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</div>
<div class="meta-line">Authors: Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu</div>
<div class="meta-line">First: 2025-12-30T16:31:45+00:00 · Latest: 2025-12-30T16:31:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24330v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&#x27;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SenseNova-MARS：通过强化学习赋能多模态代理推理和搜索</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）可以通过代理推理解决复杂任务，但其能力主要局限于文本导向的链式思考或孤立工具调用。它们无法展现出人类所需的熟练度，以无缝地将动态工具操作与持续推理交织在一起，特别是在需要协调外部工具（如搜索和图像裁剪）的知识密集型和视觉复杂场景中。在本文中，我们提出了SenseNova-MARS，这是一种新颖的多模态代理推理和搜索框架，通过强化学习（RL）赋予VLMs交织的视觉推理和工具使用能力。具体而言，SenseNova-MARS动态整合了图像搜索、文本搜索和图像裁剪工具，以应对精细和知识密集型的视觉理解挑战。在RL阶段，我们提出了批标准化组序列策略优化（BN-GSPO）算法，以提高训练稳定性并增强模型调用工具和有效推理的能力。为了全面评估代理VLMs在复杂视觉任务上的表现，我们引入了HR-MMSearch基准，这是第一个由高分辨率图像和知识密集型及搜索驱动的问题组成的搜索导向基准。实验表明，SenseNova-MARS在开源搜索和精细图像理解基准上达到了最先进的性能。具体而言，在搜索导向基准上，SenseNova-MARS-8B在MMSearch上的得分为67.84，在HR-MMSearch上的得分为41.64，超过了诸如Gemini-3-Flash和GPT-5等专有模型。SenseNova-MARS代表了代理VLMs的一个有前景的步骤，通过提供有效的和稳健的工具使用能力。为了促进该领域的进一步研究，我们将发布所有代码、模型和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance Vision-Language Models (VLMs) to perform agentic reasoning and search tasks more effectively, especially in visually complex scenarios. SenseNova-MARS, a new framework, integrates reinforcement learning to enable VLMs to dynamically use tools like image search and cropping. The BN-GSPO algorithm improves training stability and tool invocation. Experiments show that SenseNova-MARS outperforms existing models on search and fine-grained image understanding benchmarks, achieving scores of 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models like Gemini-3-Flash and GPT-5.</div>
<div class="mono" style="margin-top:8px">本文介绍了SenseNova-MARS框架，该框架通过强化学习增强视觉语言模型（VLMs）的视觉推理和工具使用能力。提出了BN-GSPO算法以提高训练稳定性和工具调用能力。SenseNova-MARS在搜索导向基准上表现出色，分别在MMSearch和HR-MMSearch上得分67.84和41.64，超越了如Gemini-3-Flash和GPT-5等专有模型。这项工作通过提供稳健的工具使用能力，推进了生成式VLMs的发展，并将发布所有代码、模型和数据集以促进进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Bringing The Consistency Gap: Explicit Structured Memory for Interleaved Image-Text Generation</div>
<div class="meta-line">Authors: Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang</div>
<div class="meta-line">First: 2025-10-13T03:19:45+00:00 · Latest: 2025-12-30T15:40:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10969v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.10969v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision Language Models (VLMs) often struggle to preserve logic, entity identity, and artistic style during extended, interleaved image-text interactions. We identify this limitation as &quot;Multimodal Context Drift&quot;, which stems from the inherent tendency of implicit neural representations to decay or become entangled over long sequences. To bridge this gap, we propose IUT-Plug, a model-agnostic Neuro-Symbolic Structured State Tracking mechanism. Unlike purely neural approaches that rely on transient attention maps, IUT-Plug introduces the Image Understanding Tree (IUT) as an explicit, persistent memory module. The framework operates by (1) parsing visual scenes into hierarchical symbolic structures (entities, attributes, and relationships); (2) performing incremental state updates to logically lock invariant properties while modifying changing elements; and (3) guiding generation through topological constraints. We evaluate our approach on a novel benchmark comprising 3,000 human-annotated samples. Experimental results demonstrate that IUT-Plug effectively mitigates context drift, achieving significantly higher consistency scores compared to unstructured text-prompting baselines. This confirms that explicit symbolic grounding is essential for maintaining robust long-horizon consistency in multimodal generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弥补一致性差距：交错图像-文本生成的显式结构化记忆</div>
<div class="mono" style="margin-top:8px">现有的视觉语言模型（VLMs）在长时间的交错图像-文本交互中往往难以保持逻辑性、实体身份和艺术风格。我们将其局限性称为“多模态上下文漂移”，这源于隐式神经表示在长序列中固有的衰减或纠缠倾向。为了解决这一问题，我们提出了IUT-Plug，这是一种模型无关的神经-符号结构化状态跟踪机制。不同于依赖于瞬态注意力图的纯神经方法，IUT-Plug 引入了图像理解树（IUT）作为显式的持久性记忆模块。该框架通过以下步骤运作：(1) 将视觉场景解析为分层的符号结构（实体、属性和关系）；(2) 进行增量状态更新，逻辑锁定不变的属性并修改变化的元素；(3) 通过拓扑约束指导生成。我们在一个包含3,000个人工标注样本的新基准上评估了我们的方法。实验结果表明，IUT-Plug 有效地缓解了上下文漂移，与无结构的文本提示基线相比，实现了显著更高的一致性得分。这证实了显式的符号定位对于在多模态生成中保持稳健的长期一致性至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of &#x27;Multimodal Context Drift&#x27; in Vision Language Models (VLMs) by proposing IUT-Plug, a model-agnostic mechanism that uses an explicit, persistent Image Understanding Tree (IUT) to preserve logic, entity identity, and artistic style during extended image-text interactions. The method involves parsing visual scenes into hierarchical symbolic structures and performing incremental state updates to maintain consistency. Experiments on a new benchmark show that IUT-Plug outperforms unstructured text-prompting baselines in terms of consistency scores, indicating the importance of explicit symbolic grounding for robust long-horizon multimodal generation.</div>
<div class="mono" style="margin-top:8px">研究通过提出一种模型无关的神经符号结构状态跟踪机制IUT-Plug来解决视觉语言模型(VLMs)中的‘多模态上下文漂移’问题。IUT-Plug利用显式的图像理解树(IUT)将视觉场景解析为层次化的符号结构，并进行增量状态更新，有助于在长时间的图像-文本交互中保持一致性。该方法在包含3,000个人标注样本的新基准上，显著提高了与无结构文本提示基线相比的一致性得分。</div>
</details>
</div>
<div class="card">
<div class="title">ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation</div>
<div class="meta-line">Authors: Ziquan Liu, Zhewei Zhu, Xuyang Shi</div>
<div class="meta-line">First: 2025-12-30T13:38:30+00:00 · Latest: 2025-12-30T13:38:30+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24224v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP&#x27;s internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP&#x27;s internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere&quot; paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARM：一种可学习的插件式模块用于基于CLIP的开放词汇语义分割</div>
<div class="mono" style="margin-top:8px">开放词汇语义分割（OVSS）从根本上受到CLIP粗略的图像级表示的限制，这些表示缺乏精确的像素级细节。现有的无需训练的方法试图通过从昂贵的外部基础模型（例如SAM、DINO）导入先验知识或通过应用静态的手工制作启发式方法来解决这一问题，CLIP的内部特征。这些方法要么计算成本高，要么效果不佳。我们提出了注意力精炼模块（ARM），这是一种轻量级、可学习的模块，有效地解锁并精炼了CLIP的内部潜力。与静态融合方法不同，ARM学习自适应地融合层次特征。它采用语义引导的交叉注意力块，使用鲁棒的深层特征（K, V）来选择和精炼细节丰富的浅层特征（Q），然后通过一个自我注意力块。关键创新在于“一次训练，随处使用”的范式。ARM在通用数据集（例如COCO-Stuff）上训练一次后，作为通用插件式后处理器，适用于多种无需训练的框架。大量实验表明，ARM在多个基准测试上始终能提升基线性能，且几乎无推理开销，建立了高效的无需训练的OVSS范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of CLIP in open-vocabulary semantic segmentation by proposing the Attention Refinement Module (ARM), a lightweight, learnable module. ARM effectively refines CLIP&#x27;s internal features through a semantically-guided cross-attention mechanism, which selects and refines detail-rich shallow features using robust deep features. Experiments demonstrate that ARM consistently improves baseline performance across multiple benchmarks with minimal computational cost, establishing a practical paradigm for training-free open-vocabulary semantic segmentation.</div>
<div class="mono" style="margin-top:8px">研究旨在解决CLIP在开放词汇语义分割中的粗略图像级表示限制。提出了一个可学习模块ARM，以适应性地细化CLIP的内部特征。它使用语义引导的交叉注意力块来选择和细化浅层特征，并使用稳健的深层特征，随后是自我注意力块。ARM只需一次训练即可作为各种无监督框架的通用后处理器使用。实验表明，ARM在多个基准上一致地提高了基线性能，并且具有极小的推理开销。</div>
</details>
</div>
<div class="card">
<div class="title">RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</div>
<div class="meta-line">Authors: Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</div>
<div class="meta-line">First: 2025-12-30T13:25:22+00:00 · Latest: 2025-12-30T13:25:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24212v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANGER：通过上下文适应的单目零样本语义导航框架</div>
<div class="mono" style="margin-top:8px">在复杂环境中高效地找到目标是现实世界体态应用的基础。尽管最近多模态基础模型的进步使得零样本物体目标导航成为可能，允许机器人搜索任意物体而无需微调，但现有方法面临两个关键限制：（1）对模拟器提供的精确深度和姿态信息的高度依赖，这限制了其在现实世界场景中的应用；（2）缺乏上下文学习（ICL）能力，使得难以快速适应新环境，如利用短视频。为了解决这些挑战，我们提出了一种名为RANGER的新型零样本、开放式词汇语义导航框架，仅使用单目相机操作。利用强大的3D基础模型，RANGER消除了对深度和姿态的依赖，同时展示了强大的ICL能力。通过简单观察新环境的短视频，系统也可以显著提高任务效率，无需进行架构修改或微调。该框架整合了几个关键组件：基于关键帧的3D重建、语义点云生成、基于视觉语言模型（VLM）的探索价值估计、高层自适应航点选择和低层动作执行。在HM3D基准和真实世界环境中进行的实验表明，RANGER在导航成功率和探索效率方面表现出竞争力，同时展示了优越的ICL适应性，无需对环境进行先前的3D建图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RANGER is a zero-shot semantic navigation framework that uses only a monocular camera to address the limitations of existing methods in terms of dependency on precise depth and pose information and lack of in-context learning capability. By leveraging 3D foundation models, RANGER can perform 3D reconstruction, semantic point cloud generation, and exploration value estimation through a vision-language model, enabling it to adapt quickly to new environments without requiring fine-tuning. Experiments show that RANGER achieves competitive navigation success rates and exploration efficiency, and demonstrates superior in-context learning adaptability in both simulated and real-world environments.</div>
<div class="mono" style="margin-top:8px">RANGER 是一种仅使用单目相机的零样本语义导航框架，旨在解决现有方法对精确深度和姿态信息的依赖以及缺乏上下文学习能力的问题。通过利用 3D 基础模型并集成关键组件，如基于关键帧的 3D 重建和语义点云生成，RANGER 可以快速适应新环境。实验表明，RANGER 在导航成功率和探索效率方面表现出色，并且在不需要对环境进行先前 3D 映射的情况下展示了更强的上下文学习适应性。</div>
</details>
</div>
<div class="card">
<div class="title">UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?</div>
<div class="meta-line">Authors: Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao</div>
<div class="meta-line">First: 2025-12-29T14:49:50+00:00 · Latest: 2025-12-30T13:23:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23512v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23512v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified structure with a concise model, UniHetero, under large-scale pretraining (&gt;200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. A common assumption in unified vision-language models is that adding generation will naturally strengthen understanding. However, this is not always true at scale. At 200M+ pretraining samples, generation helps understanding only when it operates at the semantic level, i.e. when the model learns to autoregress high-level visual representations inside the LLM. Once pixel-level objectives (e.g., diffusion losses) directly interfere with the LLM, understanding performance often degrades. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. Unified generation-understanding demonstrates a superior scaling trend compared to understanding alone, revealing a more effective way to learn vision-only knowledge directive from vision modality rather than captioning to text. (3) Autoregression on Input Embedding is effective to capture visual details. Compared to the commonly-used vision encoder, make visual autoregression on input embedding shows less cumulative error and is modality independent, which can be extend to all modalities. The learned semantic representations capture visual information such as objects, locations, shapes, and colors; further enable pixel-level image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniHetero：生成能否在大规模数据下增强视觉-语言模型的理解？</div>
<div class="mono" style="margin-top:8px">视觉-语言大型模型正朝着统一视觉理解与生成任务的方向发展。然而，在大规模数据下，生成是否能增强理解仍是一个未被充分探索的问题。在本工作中，我们通过一个简洁的统一结构模型UniHetero，在超过200万样本的大规模预训练下进行分析。我们的主要观察结果如下：(1) 生成可以提高理解，但只有在生成语义而非像素时才有效。统一的视觉-语言模型中普遍认为添加生成任务会自然增强理解，但在大规模数据下并非总是如此。在超过200万样本的预训练下，生成任务仅在操作语义层面时才有助于理解，即模型学会在大型语言模型内部自回归高层次的视觉表示时。一旦像素级目标（如扩散损失）直接干扰大型语言模型，理解性能往往会下降。(2) 生成揭示了更优的数据扩展趋势和更高的数据利用效率。统一的生成-理解模型相比单独的理解模型，展示了更优的扩展趋势，揭示了一种更有效的从视觉模态直接学习视觉知识的方式，而不是通过描述到文本。(3) 在输入嵌入上进行自回归可以有效捕捉视觉细节。与常用的视觉编码器相比，在输入嵌入上进行视觉自回归显示出较少的累积误差，并且是跨模态的，可以扩展到所有模态。学习到的语义表示捕捉了视觉信息，如物体、位置、形状和颜色；进一步支持了像素级图像生成。</div>
</details>
</div>
<div class="card">
<div class="title">CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers</div>
<div class="meta-line">Authors: Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim</div>
<div class="meta-line">First: 2025-12-30T12:55:38+00:00 · Latest: 2025-12-30T12:55:38+00:00</div>
<div class="meta-line">Comments: 16 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24195v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CorGi: 贡献指导的块级区间缓存加速扩散变换器推理</div>
<div class="mono" style="margin-top:8px">扩散变换器（DiT）在视觉生成方面取得了显著的性能，但其迭代去噪过程与较大的容量导致了高昂的推理成本。近期研究表明，DiT模型的迭代去噪过程在各步骤中存在大量的冗余计算。为了有效减少DiT中的冗余计算，我们提出了CorGi（贡献指导的块级区间缓存），这是一种无需训练的DiT推理加速框架，它选择性地在去噪步骤中重用DiT中的变压器块输出。CorGi缓存低贡献块并在每个区间内的后续步骤中重用它们，以减少冗余计算并保持生成质量。对于文本到图像任务，我们进一步提出了CorGi+，它利用每个块的交叉注意力图来识别重要标记，并应用部分注意更新以保护重要的对象细节。在最先进的DiT模型上的评估表明，CorGi和CorGi+平均实现了2.0倍的加速，同时保持了高质量的生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CorGi is a training-free inference acceleration framework for diffusion transformers (DiT) that reduces redundant computation by selectively reusing outputs of transformer blocks across denoising steps. It caches low-contribution blocks and reuses them in later steps to maintain generation quality, achieving up to 2.0x speedup. For text-to-image tasks, CorGi+ uses per-block cross-attention maps to identify salient tokens and applies partial attention updates to preserve important object details, also achieving significant speedup while maintaining quality.</div>
<div class="mono" style="margin-top:8px">CorGi 是一种针对扩散变压器（DiT）的训练-free 加速框架，通过在去噪步骤间选择性地重用变压器块的输出来减少冗余计算。它缓存低贡献度的块并在后续步骤中重用它们以保持生成质量，平均可实现 2.0 倍的加速。对于文本到图像任务，CorGi+ 进一步通过使用每块的交叉注意力图来保护重要对象细节，同时仍然保持高质量的生成结果。</div>
</details>
</div>
<div class="card">
<div class="title">VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree</div>
<div class="meta-line">Authors: Wenlong Li, Yifei Xu, Yuan Rao, Zhenhua Wang, Shuiguang Deng</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-26T14:36:15+00:00 · Latest: 2025-12-30T12:31:56+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 poster</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22693v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.22693v3">PDF</a> · <a href="https://github.com/wenlongli10/VADTree">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video anomaly detection (VAD) focuses on identifying anomalies in videos. Supervised methods demand substantial in-domain training data and fail to deliver clear explanations for anomalies. In contrast, training-free methods leverage the knowledge reserves and language interactivity of large pre-trained models to detect anomalies. However, the current fixed-length temporal window sampling approaches struggle to accurately capture anomalies with varying temporal spans. Therefore, we propose VADTree that utilizes a Hierarchical Granularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree leverages the knowledge embedded in a pre-trained Generic Event Boundary Detection (GEBD) model to characterize potential anomaly event boundaries. Specifically, VADTree decomposes the video into generic event nodes based on boundary confidence, and performs adaptive coarse-fine hierarchical structuring and redundancy removal to construct the HGTree. Then, the multi-dimensional priors are injected into the visual language models (VLMs) to enhance the node-wise anomaly perception, and anomaly reasoning for generic event nodes is achieved via large language models (LLMs). Finally, an inter-cluster node correlation method is used to integrate the multi-granularity anomaly scores. Extensive experiments on three challenging datasets demonstrate that VADTree achieves state-of-the-art performance in training-free settings while drastically reducing the number of sampled video segments. The code will be available at https://github.com/wenlongli10/VADTree.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VADTree：基于层次粒度感知树的无训练视频异常检测</div>
<div class="mono" style="margin-top:8px">视频异常检测（VAD）专注于识别视频中的异常。监督方法需要大量领域内的训练数据，并且无法为异常提供清晰的解释。相比之下，无训练方法利用大型预训练模型的知识储备和语言互动能力来检测异常。然而，当前固定长度的时间窗口采样方法难以准确捕捉具有不同时间跨度的异常。因此，我们提出VADTree，利用层次粒度感知树（HGTree）结构进行灵活的VAD采样。VADTree利用预训练的通用事件边界检测（GEBD）模型嵌入的知识来表征潜在的异常事件边界。具体来说，VADTree基于边界置信度将视频分解为通用事件节点，并进行自适应粗细层次结构构建和冗余去除以构建HGTree。然后，将多维先验注入视觉语言模型（VLMs）以增强节点级别的异常感知，并通过大型语言模型（LLMs）实现通用事件节点的异常推理。最后，使用跨簇节点相关方法整合多粒度异常评分。在三个具有挑战性的数据集上的广泛实验表明，VADTree在无训练设置中实现了最先进的性能，同时大幅减少了采样的视频片段数量。代码将在https://github.com/wenlongli10/VADTree上提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VADTree is proposed to address the limitations of fixed-length temporal window sampling in video anomaly detection (VAD). It uses a Hierarchical Granularity-aware Tree (HGTree) structure to flexibly sample video segments, leveraging a pre-trained Generic Event Boundary Detection (GEBD) model to identify potential anomaly event boundaries. VADTree significantly outperforms existing training-free methods on three challenging datasets, reducing the number of sampled video segments and achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">VADTree 通过利用层次粒度感知树（HGTree）结构来解决训练免费视频异常检测方法的局限性。它将视频分解为通用事件节点并构建 HGTree 进行自适应粗细层次结构化。VADTree 通过视觉语言模型中的多维先验增强异常感知，并使用跨簇节点相关方法整合多粒度异常得分。实验表明，VADTree 在减少采样视频片段数量的同时优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</div>
<div class="meta-line">Authors: TsaiChing Ni, ZhenQi Chen, YuanFu Yang</div>
<div class="meta-line">First: 2025-12-30T11:45:22+00:00 · Latest: 2025-12-30T11:45:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24160v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向大规模多模态数据的工业缺陷开放词汇理解</div>
<div class="mono" style="margin-top:8px">我们提出了IMDD-1M，这是首个包含1,000,000个图像-文本对的大型工业多模态缺陷数据集，旨在推动制造和质量检测中的多模态学习。IMDD-1M 包含了60多种材料类别和400多种缺陷类型的高分辨率真实世界缺陷，每种缺陷都附有专家验证的注释和详细的文本描述，详细说明了缺陷的位置、严重程度和上下文属性。该数据集支持包括分类、分割、检索、描述生成和生成模型在内的广泛应用。基于IMDD-1M，我们从零开始训练了一个基于扩散的视觉-语言基础模型，特别适用于工业场景。该模型作为可泛化的基础模型，可以通过轻量级微调高效适应特定领域。与专门的专家模型相比，它只需要不到5%的任务特定数据即可达到相当的性能，突显了数据高效基础模型适应在工业检测和生成中的潜力，为可扩展、领域适应和知识导向的制造智能铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance multimodal learning for industrial defect understanding by introducing IMDD-1M, a large-scale multimodal dataset with 1,000,000 aligned image-text pairs. The dataset includes high-resolution defects from over 60 material categories and 400 defect types, each with expert annotations and detailed descriptions. A diffusion-based vision-language model was trained on this dataset, achieving comparable performance to expert models with only 5% of the task-specific data, demonstrating the potential for data-efficient adaptation in industrial inspection and generation tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入包含1,000,000个图像-文本配对的大型多模态数据集IMDD-1M，推进工业缺陷理解的多模态学习。该数据集包含来自超过60种材料类别和400种缺陷类型的高分辨率缺陷，每种缺陷都有专家注释和详细的描述。基于该数据集训练了一个扩散型视觉-语言模型，仅使用5%的任务特定数据就达到了与专家模型相当的性能，展示了在工业检测和生成任务中高效数据适应的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
