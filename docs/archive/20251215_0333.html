<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-15 03:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251215_0333</div>
    <div class="row"><div class="card">
<div class="title">Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</div>
<div class="meta-line">Authors: Tsai-Shien Chen, Aliaksandr Siarohin, Guocheng Gordon Qian, Kuan-Chieh Jackson Wang, Egor Nemchinov, Moayed Haji-Ali, Riza Alp Guler, Willi Menapace, Ivan Skorokhodov, Anil Kag, Jun-Yan Zhu, Sergey Tulyakov</div>
<div class="meta-line">First: 2025-12-11T18:59:56+00:00 · Latest: 2025-12-11T18:59:56+00:00</div>
<div class="meta-line">Comments: Project page: https://snap-research.github.io/omni-attribute</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10955v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://snap-research.github.io/omni-attribute">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Omni-Attribute：面向视觉概念个性化的大词汇量属性编码器</div>
<div class="mono" style="margin-top:8px">视觉概念个性化旨在将特定的图像属性，如身份、表情、光照和风格，转移到未见的上下文中。然而，现有方法依赖于通用图像编码器的整体嵌入，这会将多个视觉因素纠缠在一起，使得难以隔离单一属性。这通常会导致信息泄露和不一致的合成。为了解决这一局限性，我们引入了Omni-Attribute，这是第一个用于学习高保真度、属性特定表示的大词汇量图像属性编码器。我们的方法联合设计数据和模型：(i) 我们收集了带有正负属性标注的语义关联图像对，以明确地教导编码器保留或抑制什么；(ii) 我们采用了一种双目标训练范式，平衡生成保真度与对比性解耦。生成的嵌入在开放词汇量属性检索、个性化和组合生成方面证明有效，实现了多个基准上的最佳性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Omni-Attribute is an open-vocabulary image attribute encoder that aims to isolate specific visual attributes like identity, expression, lighting, and style. It uses semantically linked image pairs and a dual-objective training method to learn high-fidelity, attribute-specific representations. The method outperforms existing approaches in open-vocabulary attribute retrieval, personalization, and compositional generation across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">Omni-Attribute 是一种开放词汇量的图像属性编码器，旨在分离出如身份和表情等特定视觉属性以实现个性化。它使用语义关联的图像对和双重目标训练方法来学习高保真的属性特定表示，从而提高属性检索、个性化和组合生成的效果，并在多个基准测试中超越现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</div>
<div class="meta-line">Authors: Delong Chen, Mustafa Shukor, Theo Moutakanni, Willy Chung, Jade Yu, Tejaswi Kasarla, Allen Bolourchi, Yann LeCun, Pascale Fung</div>
<div class="meta-line">First: 2025-12-11T18:59:22+00:00 · Latest: 2025-12-11T18:59:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10942v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA&#x27;s embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VL-JEPA：联合嵌入预测架构的跨模态模型</div>
<div class="mono" style="margin-top:8px">我们介绍了基于联合嵌入预测架构（JEPA）的跨模态模型VL-JEPA。与经典视觉语言模型（VLM）逐个生成标记不同，VL-JEPA预测目标文本的连续嵌入。通过在抽象表示空间中学习，该模型专注于与任务相关的语义，同时抽象掉表面语言的变异性。在严格控制的比较中，与使用相同视觉编码器和训练数据的标准标记空间VLM训练相比，VL-JEPA在参数量减少50%的情况下实现了更强的性能。在推理时，仅在需要时调用轻量级文本解码器将VL-JEPA预测的嵌入转换为文本。我们展示了VL-JEPA原生支持选择性解码，将解码操作减少2.85倍，同时保持与非自适应均匀解码相似的性能。除了生成之外，VL-JEPA的嵌入空间自然支持开放词汇分类、文本到视频检索和区分型VQA，无需任何架构修改。在八个视频分类数据集和八个视频检索数据集上，VL-JEPA的平均性能超过了CLIP、SigLIP2和感知编码器。同时，尽管只有1.6B参数，该模型在四个VQA数据集（GQA、TallyQA、POPE和POPEv2）上的性能与经典VLM（InstructBLIP、QwenVL）相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VL-JEPA is a vision-language model that uses a Joint Embedding Predictive Architecture to predict continuous embeddings of target texts, rather than generating tokens autoregressively. This approach leads to better performance with fewer parameters and supports selective decoding, reducing the number of decoding operations. VL-JEPA outperforms several models on video classification and retrieval tasks while achieving comparable results on VQA tasks with significantly fewer parameters.</div>
<div class="mono" style="margin-top:8px">VL-JEPA 是一种使用联合嵌入预测架构的视觉-语言模型，它预测目标文本的连续嵌入而不是自回归生成令牌。这种方法使得模型在更少的参数下表现出更好的性能，并支持选择性解码，将解码操作的数量减少2.85倍。VL-JEPA 在多个视频分类和检索任务中表现出色，并且在仅拥有1.6B参数的情况下，在VQA任务上也达到了与经典视觉语言模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models</div>
<div class="meta-line">Authors: Shengao Wang, Wenqi Wang, Zecheng Wang, Max Whitton, Michael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu, Helen Chen, David Li, Jeffrey Li, Shawn Li, Andrew Zagula, Amy Zhao, Andrew Zhu, Sayaka Nakamura, Yuki Yamamoto, Jerry Jun Yokono, Aaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong</div>
<div class="meta-line">First: 2025-12-11T18:57:05+00:00 · Latest: 2025-12-11T18:57:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10932v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10932v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early children&#x27;s developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children&#x27;s capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BabyVLM-V2：面向发展性基础视觉模型预训练和基准测试的框架</div>
<div class="mono" style="margin-top:8px">早期儿童的发展轨迹为高效样本预训练视觉基础模型提供了自然目标。我们介绍了BabyVLM-V2，这是一种基于发展的婴儿启发式视觉-语言建模框架，通过纵向多维度预训练集、多功能模型以及最重要的是DevCV工具箱进行认知评估，大幅改进了BabyVLM-V1。预训练集最大限度地覆盖了纵向的婴儿中心视听素材，减少了人工整理，提供了视频-语句、图像-语句和多轮对话数据，反映了婴儿的经验。DevCV工具箱将最近发布的NIH婴儿工具箱中的所有视觉相关度量指标整合成一个包含十个跨模态任务的基准套件，涵盖了空间推理、记忆和词汇理解，与早期儿童的能力相一致。实验结果表明，从零开始预训练的紧凑模型在DevCV工具箱上可以达到竞争力的表现，某些任务上优于GPT-4o。我们希望BabyVLM-V2这一原则性的统一框架能够加速发展性基础视觉模型预训练的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop vision foundation models that can learn efficiently from infant-like data to better understand early cognitive development. BabyVLM-V2 uses a longitudinal, multifaceted pretraining set and a DevCV Toolbox for cognitive evaluation. The pretraining set includes video-utterance, image-utterance, and multi-turn conversational data that reflect infant experiences. The model achieves competitive performance on a benchmark suite of ten multimodal tasks, outperforming GPT-4o on some tasks, demonstrating the effectiveness of the developmentally grounded approach.</div>
<div class="mono" style="margin-top:8px">研究旨在通过模仿早期儿童的发展轨迹来训练视觉基础模型，提高样本效率。BabyVLM-V2 提出了一个纵向、多方面的预训练数据集和 DevCV 工具箱来进行认知评估。预训练数据集包括视频-语音、图像-语音和多轮对话数据，反映了婴儿的经验。实验结果显示，从零开始预训练的紧凑型模型在 DevCV 工具箱上可以达到竞争力的表现，某些任务上甚至优于 GPT-4o。该框架有望加速视觉基础模型的开发性预训练研究。</div>
</details>
</div>
<div class="card">
<div class="title">Asynchronous Reasoning: Training-Free Interactive Thinking LLMs</div>
<div class="meta-line">Authors: George Yakushev, Nataliia Babina, Masoud Vahid Dastgerdi, Vyacheslav Zhdanovskiy, Alina Shutova, Denis Kuznedelev</div>
<div class="meta-line">First: 2025-12-11T18:57:02+00:00 · Latest: 2025-12-11T18:57:02+00:00</div>
<div class="meta-line">Comments: Preprint, work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10931v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10931v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to &lt;= 5s. and the overall real-time delays by 6-11x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异步推理：无需训练的交互式思考大语言模型</div>
<div class="mono" style="margin-top:8px">许多最先进的大语言模型在给出答案之前会先进行思考。推理可以大大提升语言模型的能力和安全性，但也使它们变得不那么互动：给定新的输入，模型必须停止思考才能做出回应。现实世界的应用场景，如基于语音或嵌入式助手，需要大语言模型代理能够实时响应并根据额外信息进行调整，这与顺序交互不兼容。相比之下，人类可以异步地听、思考和行动：我们在阅读问题时就开始思考，并在构思答案时继续思考。在本研究中，我们通过利用旋转嵌入的特性，使原本用于顺序交互的大语言模型能够在不进行额外训练的情况下同时思考、倾听和生成输出。我们对数学、常识和安全推理进行了评估，并发现这种方法可以实时生成准确的增强思考的答案，将首次无思考标记的时间从几分钟缩短到&lt;=5秒，并将整体实时延迟减少6-11倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitation of state-of-the-art LLMs that require sequential interactions, which is incompatible with real-world use cases requiring real-time responses. The authors propose a method to enable LLMs to think, listen, and generate outputs asynchronously using the properties of rotary embeddings. Evaluation on math, commonsense, and safety reasoning tasks shows that the approach can produce accurate answers in real time, significantly reducing the time to first non-thinking token and overall real-time delays by 6-11 times.</div>
<div class="mono" style="margin-top:8px">这项工作解决了当前最先进的LLM需要顺序交互的问题，这与需要实时响应的实际应用场景不兼容。作者提出了一种方法，利用旋转嵌入的特性，使LLM能够异步地思考、倾听和生成输出。在数学、常识和安全推理任务上的评估表明，该方法可以实时生成准确的答案，显著减少了首次非思考标记的时间和整体实时延迟，提高了6-11倍。</div>
</details>
</div>
<div class="card">
<div class="title">DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance</div>
<div class="meta-line">Authors: Peiying Zhang, Nanxuan Zhao, Matthew Fisher, Yiran Xu, Jing Liao, Difan Liu</div>
<div class="meta-line">First: 2025-12-11T18:23:03+00:00 · Latest: 2025-12-11T18:23:03+00:00</div>
<div class="meta-line">Comments: Project page: https://intchous.github.io/DuetSVG-site</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10894v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10894v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://intchous.github.io/DuetSVG-site">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model&#x27;s native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation is to address the limitations of existing vision-language model-based SVG generation methods, which often fail to produce visually appealing or geometrically coherent SVGs due to the lack of visual signals during decoding. The main method involves developing DuetSVG, a unified multimodal model that generates both image and SVG tokens end-to-end. The model is trained on both image and SVG datasets, and at inference, it uses a novel test-time scaling strategy to guide SVG decoding, improving its quality. Key experimental findings show that DuetSVG outperforms existing methods in generating visually faithful, semantically aligned, and syntactically clean SVGs across various applications.</div>
<div class="mono" style="margin-top:8px">研究动机是解决现有基于视觉语言模型（VLM）的方法在SVG生成中遇到的复杂语义处理不足和无法生成视觉上吸引人或几何上一致的SVG的问题。主要方法是DuetSVG，这是一种联合生成图像和SVG标记的统一多模态模型，通过同时训练图像和SVG数据集来实现端到端训练。在推理时，使用一种新颖的测试时缩放策略来指导SVG解码。关键实验发现表明，DuetSVG在各种应用中生成了视觉上忠实、语义上对齐和语法上干净的SVG。</div>
</details>
</div>
<div class="card">
<div class="title">PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction</div>
<div class="meta-line">Authors: Brandon Smock, Valerie Faucon-Morin, Max Sokolov, Libin Liang, Tayyibah Khanam, Maury Courtland</div>
<div class="meta-line">First: 2025-12-11T18:19:00+00:00 · Latest: 2025-12-11T18:19:00+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10888v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PubTables-v2：一种新的大规模数据集，用于全页和多页表格提取</div>
<div class="mono" style="margin-top:8px">表格提取（TE）是视觉文档理解中的一个关键挑战。传统方法首先检测表格，然后识别其结构。最近，人们开始开发可以直接在全页或文档上下文中提取表格的方法，例如视觉-语言模型（VLMs）。然而，由于缺乏标注数据，进步难以展示。为了解决这个问题，我们创建了一个新的大规模数据集，PubTables-v2。PubTables-v2 支持当前许多具有挑战性的表格提取任务。值得注意的是，它是第一个大规模的多页表格结构识别基准。我们通过在这些任务上评估领域专用的 VLMs 来展示其用途，并突出当前的进展。最后，我们使用 PubTables-v2 创建了 Page-Object Table Transformer（POTATR），这是一种图像到图的 Table Transformer 扩展，用于全面的页面级 TE。数据、代码和训练模型将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of table extraction in visual document understanding by creating a new large-scale dataset, PubTables-v2, which supports various challenging table extraction tasks, especially multi-page table structure recognition. The method involves evaluating domain-specialized vision-language models and developing the Page-Object Table Transformer (POTATR) for comprehensive page-level table extraction. Key findings include the demonstration of current progress in table extraction and the usefulness of PubTables-v2 for evaluating and advancing table extraction methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决标注数据不足的问题，推动视觉文档理解中的表格提取。作者引入了PubTables-v2，这是一个用于全页和多页表格提取的大规模数据集，特别关注多页表格结构识别。主要发现包括对领域特定的视觉语言模型的评估以及开发了POTATR，即表格变换器的图像到图扩展，用于全面的页面级表格提取。</div>
</details>
</div>
<div class="card">
<div class="title">From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models</div>
<div class="meta-line">Authors: Zongzhao Li, Xiangzhe Kong, Jiahui Su, Zongyang Ma, Mingze Li, Songyou Li, Yuelin Zhang, Yu Rong, Tingyang Xu, Deli Zhao, Wenbing Huang</div>
<div class="meta-line">First: 2025-12-11T18:00:21+00:00 · Latest: 2025-12-11T18:00:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10867v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10867v1">PDF</a> · <a href="https://huggingface.co/datasets/zongzhao/MiSI-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从宏观到微观：通过视觉语言模型评估分子微观空间智能</div>
<div class="mono" style="margin-top:8px">本文介绍了微观空间智能（MiSI）的概念，即感知和推理看不见的微观实体的空间关系的能力，这是科学研究的基础。为了评估视觉语言模型（VLMs）在这一领域的潜力，我们提出了一种系统性的基准框架MiSI-Bench。该框架包含超过163,000个问答对和587,000张图像，源自约4,000个分子结构，涵盖了九个互补任务，评估能力从基本的空间变换到复杂的关联识别。实验结果表明，当前最先进的VLMs在这一基准上的表现远低于人类水平。然而，微调后的7B模型显示出巨大的潜力，甚至在空间变换任务上超过了人类，而其在氢键识别等基于科学的任务上的表现不佳，突显了整合显式领域知识以实现科学AGI的必要性。数据集可在https://huggingface.co/datasets/zongzhao/MiSI-bench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Microscopic Spatial Intelligence (MiSI), the ability to perceive and reason about invisible microscopic entities, crucial for scientific discovery. A benchmark framework, MiSI-Bench, was developed with over 163,000 question-answer pairs and 587,000 images from 4,000 molecular structures, evaluating nine tasks from spatial transformations to relational identifications. State-of-the-art Vision-Language Models (VLMs) performed below human level, but a fine-tuned 7B model showed promise, especially in spatial transformation tasks, highlighting the need for integrating domain knowledge for scientific AGI.</div>
<div class="mono" style="margin-top:8px">本文介绍了微观空间智能（MiSI），这是一种感知和推理看不见的微观实体的空间关系的能力，对于科学研究至关重要。提出了一个基准框架MiSI-Bench，包含超过163,000个问答对和587,000张来自约4,000个分子结构的图像，评估了从空间变换到关系识别的九项任务。最先进的视觉-语言模型（VLMs）的表现低于人类水平，但经过微调的7B模型在空间变换任务中显示出潜力，强调了集成领域知识对于实现科学AGI的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</div>
<div class="meta-line">Authors: Zijian Gu, Yuxi Liu, Zhenhao Zhang, Song Wang</div>
<div class="meta-line">First: 2025-12-03T06:09:14+00:00 · Latest: 2025-12-11T17:17:07+00:00</div>
<div class="meta-line">Comments: 10 pages, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03477v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03477v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向医学青光眼诊断的公平性意识微调视觉-语言模型</div>
<div class="mono" style="margin-top:8px">视觉-语言模型在医学成像任务上达到专家级性能，但在不同人口群体中表现出显著的诊断准确性差异。我们引入了公平性意识的低秩适应方法，结合了参数效率和显式的公平优化。我们的主要算法贡献是一种可微分的MaxAccGap损失，使跨人口群体的准确性公平性能够端到端优化。我们提出了三种方法：FR-LoRA将MaxAccGap正则化整合到训练目标中，GR-LoRA应用逆频率加权以平衡梯度贡献，Hybrid-LoRA结合了这两种机制。在10,000张青光眼眼底图像上评估，GR-LoRA将诊断准确性差异降低了69%，同时保持53.15%的整体准确性。消融研究显示，较强的正则化强度能够实现最佳公平性，同时最小化准确性损失，种族特定优化可实现60%的差异减少。我们的方法只需要0.24%的可训练参数，使公平的医学AI在资源受限的医疗保健环境中具有实际部署的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the issue of diagnostic accuracy disparities in medical glaucoma diagnosis by vision-language models across demographic groups. It introduces a fairness-aware Low-Rank Adaptation (LoRA) method, including FR-LoRA, GR-LoRA, and Hybrid-LoRA, which combine parameter efficiency with explicit fairness optimization. GR-LoRA, which applies inverse frequency weighting, reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies show that strong regularization strength and race-specific optimization are effective in achieving fairness with minimal accuracy loss.</div>
<div class="mono" style="margin-top:8px">该研究针对医学青光眼诊断中视觉语言模型在不同 demographic 组群间诊断准确性差异的问题，引入了一种公平性意识的低秩适应（LoRA）方法，包括三种方法：FR-LoRA、GR-LoRA 和 Hybrid-LoRA。GR-LoRA 通过应用逆频率加权，显著减少了 69% 的诊断准确性差异，同时保持了 53.15% 的总体准确性。该方法仅需要 0.24% 的可训练参数，适用于资源受限的医疗保健环境。</div>
</details>
</div>
<div class="card">
<div class="title">Replace, Don&#x27;t Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly</div>
<div class="meta-line">Authors: Moshe Lahmy, Roi Yozevitch</div>
<div class="meta-line">First: 2025-12-11T16:31:29+00:00 · Latest: 2025-12-11T16:31:29+00:00</div>
<div class="meta-line">Comments: 24 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10787v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10787v1">PDF</a> · <a href="https://github.com/mosherino/SEAL-RAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don&#x27;t expand&#x27;&#x27;} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p&lt;0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>替换，不要扩展：通过固定预算证据组装在多跳RAG中缓解上下文稀释</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）系统在处理多跳查询时经常失败，因为初始检索遗漏了桥梁事实。之前的纠正方法，如Self-RAG、CRAG和Adaptive-$k$，通常通过增加更多上下文或修剪现有列表来解决这一问题。然而，简单地扩展上下文窗口往往会引发“上下文稀释”，即干扰信息挤占了相关信息。我们提出了SEAL-RAG，这是一种无需训练的控制器，采用“替换，不要扩展”的策略，在固定检索深度$k$下对抗上下文稀释。SEAL 执行一个（S搜索 → E提取 → A评估 → L循环）循环：它进行实时、实体锚定的提取，构建一个动态的“缺口规范”（缺失的实体/关系），触发有针对性的微查询，并使用实体优先排序主动替换干扰信息以获取缺口闭合证据。我们在共享环境中对SEAL-RAG与Basic RAG、CRAG、Self-RAG和Adaptive-$k$的忠实重实现进行了评估，评估数据集为HotpotQA和2WikiMultiHopQA。在HotpotQA（$k=3$）上，SEAL将答案正确性提高了3-13个百分点，证据精确度提高了12-18个百分点，超过Self-RAG。在2WikiMultiHopQA（$k=5$）上，它在准确性上比Adaptive-$k$高出8.0个百分点，并且保持了96%的证据精确度，而CRAG仅为22%。这些增益在统计上具有显著性（$p&lt;0.001$）。通过强制执行固定-$k$替换，SEAL提供了可预测的成本模型，同时确保前-$k$槽位优化的是精确度而非简单的广度。我们已在https://github.com/mosherino/SEAL-RAG/上发布了我们的代码和数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of context dilution in multi-hop retrieval-augmented generation (RAG) systems, where expanding the context window can lead to irrelevant information crowding out relevant facts. It introduces SEAL-RAG, which employs a &#x27;replace, don’t expand&#x27; strategy to mitigate this problem. SEAL-RAG uses a search-extract-assess-loop cycle to dynamically extract and replace distractors with relevant evidence, improving answer correctness and evidence precision. On HotpotQA and 2WikiMultiHopQA, SEAL-RAG outperforms existing methods like Self-RAG and Adaptive-$k$ by significant margins, demonstrating its effectiveness in handling multi-hop queries.</div>
<div class="mono" style="margin-top:8px">该论文解决了多跳检索增强生成（RAG）系统中的上下文稀释问题，即简单扩展上下文窗口会导致无关信息挤占关键信息。论文提出了一种名为SEAL-RAG的方法，采用‘替换，不要扩展’策略来缓解这一问题。SEAL-RAG通过搜索-提取-评估-循环周期动态提取并替换干扰信息，以相关证据填补空白。在HotpotQA和2WikiMultiHopQA上，SEAL-RAG在答案正确性和证据精度方面显著优于Self-RAG和Adaptive-$k$等现有方法，证明了其在处理多跳查询方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Optimization-Guided Diffusion for Interactive Scene Generation</div>
<div class="meta-line">Authors: Shihao Li, Naisheng Ye, Tianyu Li, Kashyap Chitta, Tuo An, Peng Su, Boyang Wang, Haiou Liu, Chen Lv, Hongyang Li</div>
<div class="meta-line">First: 2025-12-08T15:56:18+00:00 · Latest: 2025-12-11T15:08:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07661v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07661v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generate realistic and diverse multi-agent driving scenes for autonomous vehicle evaluation, addressing the limitations of existing data-driven models in terms of controllability and adherence to physical and social constraints. OMEGA, an optimization-guided framework, enhances generation by enforcing structural consistency and interaction awareness during diffusion-based sampling. Experiments on nuPlan and Waymo demonstrate that OMEGA significantly improves generation realism, consistency, and controllability, increasing the ratio of valid scenes and near-collision frames with a time-to-collision under three seconds.</div>
<div class="mono" style="margin-top:8px">论文旨在解决生成用于自主车辆评估的现实且多样化的多智能体驾驶场景的挑战，其中安全关键事件罕见。它提出了OMEGA，一种优化引导框架，通过在扩散采样过程中强制执行物理和社会约束来增强生成场景的可控性和真实性。实验表明，OMEGA显著提高了物理和行为上有效的场景比例，从自由探索的32.35%提高到72.27%，从可控性生成的11%提高到80%，同时增加了时间到碰撞小于三秒的接近碰撞帧的数量。</div>
</details>
</div>
<div class="card">
<div class="title">SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving</div>
<div class="meta-line">Authors: Peizheng Li, Zhenghao Zhang, David Holtz, Hang Yu, Yutong Yang, Yuzhi Lai, Rui Song, Andreas Geiger, Andreas Zell</div>
<div class="meta-line">First: 2025-12-11T14:59:07+00:00 · Latest: 2025-12-11T14:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10719v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaceDrive：将空间意识融入基于VLM的自动驾驶</div>
<div class="mono" style="margin-top:8px">基于视觉语言模型（VLMs）的端到端自动驾驶方法在大规模预训练获得的广泛视觉理解和强大推理能力的驱动下迅速发展。然而，我们发现当前的VLMs在理解精细的三维空间关系方面存在困难，这是与物理世界交互的系统的基本要求。为了解决这一问题，我们提出了SpaceDrive，这是一种空间感知的基于VLM的驾驶框架，将空间信息视为显式的位置编码（PEs），而不是文本数字标记，从而实现语义和空间表示的联合推理。SpaceDrive 使用一个通用的位置编码器对多视图深度估计、历史本体状态和文本提示生成的所有3D坐标进行编码。这些3D PE首先叠加以增强相应的2D视觉标记。同时，它们作为任务无关的坐标表示，取代了数字标记的数值标记，作为VLM的输入和输出。这种机制使模型在空间推理中更好地索引特定的视觉语义，并直接回归轨迹坐标，而不是逐个生成数字，从而提高规划准确性。广泛的实验验证了SpaceDrive在nuScenes数据集上实现了最先进的开环性能，并在Bench2Drive封闭环基准测试中获得了第二高的驾驶得分为78.02，超过了现有的VLM方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpaceDrive is a spatial-aware VLM-based driving framework that enhances the understanding of 3D spatial relationships by treating spatial information as positional encodings. It uses a universal positional encoder to process 3D coordinates from multi-view depth estimation, historical ego-states, and text prompts, which are then superimposed to augment 2D visual tokens. This approach improves the model&#x27;s ability to reason spatially and directly regress trajectory coordinates, leading to enhanced planning accuracy. Experiments show that SpaceDrive outperforms existing VLM-based methods on the nuScenes dataset and achieves the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark.</div>
<div class="mono" style="margin-top:8px">SpaceDrive通过提出一种空间感知的VLM驱动框架来解决当前视觉语言模型(VLMs)在理解3D空间关系方面的局限性。该框架使用来自多视图深度估计、历史自我状态和文本提示的通用位置编码(PEs)来增强2D视觉标记，并作为任务无关的坐标表示。这种方法使模型能够更好地索引特定的视觉语义，并直接回归轨迹坐标，从而提高规划准确性。实验表明，SpaceDrive在nuScenes数据集上超过了现有VLM方法，并在Bench2Drive封闭环基准测试中获得了第二高的驾驶得分为78.02。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning</div>
<div class="meta-line">Authors: Benjamin Gundersen, Nicolas Deperrois, Samuel Ruiperez-Campillo, Thomas M. Sutter, Julia E. Vogt, Michael Moor, Farhad Nooralahzadeh, Michael Krauthammer</div>
<div class="meta-line">First: 2025-12-11T14:36:14+00:00 · Latest: 2025-12-11T14:36:14+00:00</div>
<div class="meta-line">Comments: 10 pages main text (3 figures, 3 tables), 31 pages in total</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10691v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning (&quot;thinking&quot;) has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用强化学习增强放射学报告生成和视觉定位</div>
<div class="mono" style="margin-top:8px">近期视觉-语言模型（VLMs）在胸部X光（CXR）解释的多个方面取得了进步。然而，许多医学VLMs仅依赖于监督微调（SFT），这优化了下一个词的预测，但没有评估答案质量。相比之下，强化学习（RL）可以结合任务特定反馈，其与显式中间推理（“思考”）的结合在可验证的数学和编码任务上取得了显著进展。为了研究RL和思考在CXR VLM中的影响，我们首先在CXR数据上进行大规模SFT，基于Qwen3-VL构建更新的RadVLM，然后进行冷启动SFT阶段，使模型具备基本的思考能力。接着，我们应用组相对策略优化（GRPO）并使用临床相关的、任务特定的奖励进行报告生成和视觉定位，分别在特定领域和通用领域Qwen3-VL变体上进行匹配的RL实验，有思考和无思考。在这些设置中，我们发现虽然强大的SFT对于高基线性能至关重要，但RL在两个任务上提供了额外的增益，而显式思考似乎并未进一步改善结果。在统一的评估管道下，RL优化的RadVLM模型优于其基线版本，并在报告生成和定位上达到最先进的性能，突显了临床对齐的RL作为SFT有力补充的强大作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to enhance the performance of radiology report generation and visual grounding using reinforcement learning (RL) and explicit intermediate reasoning (&#x27;thinking&#x27;). After building an updated RadVLM through supervised fine-tuning (SFT) on CXR data, the researchers applied RL with clinically grounded rewards for report generation and visual grounding. The experiments showed that while strong SFT is essential for high base performance, RL provides additional gains on both tasks, and explicit thinking does not further improve results. The RL-optimized RadVLM models outperform their baselines and achieve state-of-the-art performance on both tasks.</div>
<div class="mono" style="margin-top:8px">该研究探讨了强化学习（RL）和显式推理（‘思考’）对胸部X光（CXR）解释中视觉语言模型（VLM）性能的影响。在经过大量监督微调（SFT）更新RadVLM后，该模型进一步使用与临床相关的奖励进行RL训练，以进行报告生成和视觉定位。结果表明，虽然强大的SFT对于高基线性能至关重要，但RL在两个任务上提供了额外的增益，而显式思考并未进一步改善结果。RL优化后的RadVLM模型在报告生成和定位任务上均优于基线模型，并达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</div>
<div class="meta-line">Authors: Siyu Chen, Ting Han, Chengzheng Fu, Changshe Zhang, Chaolei Wang, Jinhe Su, Guorong Cai, Meiliu Wu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-11T15:54:47+00:00 · Latest: 2025-12-11T14:33:57+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09881v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09881v3">PDF</a> · <a href="https://github.com/anonymouse-9c53tp182bvz/Vireo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用深度和语言实现开放词汇领域泛化语义分割</div>
<div class="mono" style="margin-top:8px">开放词汇语义分割（OVSS）和语义分割中的领域泛化（DGSS）突显了一种微妙的互补性，这激发了开放词汇领域泛化语义分割（OV-DGSS）的概念。OV-DGSS旨在生成未见类别的像素级掩码，同时在未见领域中保持鲁棒性，这对于在恶劣条件下进行自动驾驶等现实场景至关重要。我们提出了Vireo，这是一种新颖的一阶段框架，首次将开放词汇语义分割和领域泛化的优点统一起来。Vireo基于冻结的视觉基础模型（VFMs），并通过深度VFMs引入场景几何，以提取领域不变的结构特征。为了在领域转移下弥合视觉和文本模态之间的差距，我们提出了三个关键组件：（1）GeoText提示，将几何特征与语言线索对齐，并逐步细化VFM编码器表示；（2）粗略掩码先验嵌入（CMPE），以增强梯度流动，加快收敛速度并增强文本影响；（3）领域开放词汇向量嵌入头（DOV-VEH），将细化的结构和语义特征融合以实现稳健预测。对这些组件的全面评估证明了我们设计的有效性。我们提出的Vireo在领域泛化和开放词汇识别方面均达到了最先进的性能，并大幅超越了现有方法，提供了一种在多变和动态环境中实现稳健视觉理解的统一和可扩展解决方案。代码可在https://github.com/anonymouse-9c53tp182bvz/Vireo/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS) to handle unseen categories and domains, crucial for real-world applications like autonomous driving. Vireo, a single-stage framework, integrates depth information and language cues to improve domain-invariant feature extraction. Key components include GeoText Prompts for aligning geometric and textual features, CMPE for better gradient flow, and DOV-VEH for robust prediction. Vireo outperforms existing methods in both domain generalization and open-vocabulary recognition, providing a unified solution for visual understanding in dynamic environments.</div>
<div class="mono" style="margin-top:8px">论文通过引入Vireo，一种新颖的一阶段框架，解决了开放词汇域泛化语义分割（OV-DGSS）的挑战。Vireo利用冻结的视觉基础模型并结合深度VFMs提取域不变的结构特征。它还包括GeoText提示、CMPE和DOV-VEH，分别用于将几何特征与语言线索对齐、增强梯度流动和融合精炼的结构和语义特征。实验结果表明，Vireo在域泛化和开放词汇识别方面均优于现有方法，提供了一种在多样化环境中实现鲁棒视觉理解的统一解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching</div>
<div class="meta-line">Authors: Javier Villena Toro, Mehdi Tarkian</div>
<div class="meta-line">First: 2025-12-11T14:20:17+00:00 · Latest: 2025-12-11T14:20:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10674v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10674v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Geo6DPose is a lightweight, training-free pipeline for zero-shot 6D object pose estimation that leverages geometric filtering to achieve sub-second inference on a single commodity GPU. It combines foundation model visual features with a geometric filtering strategy, projecting scene patch centers to 3D and template descriptors to the object model coordinate system. The method uses correspondence-driven RANSAC and a weighted geometric alignment metric to recover and rank poses, demonstrating high robustness to noise, clutter, and partial visibility while matching the average recall of larger zero-shot baselines (53.7 AR, 1.08 FPS).</div>
<div class="mono" style="margin-top:8px">Geo6DPose 是一个轻量级、无需训练的零样本 6D 物体姿态估计管道，利用几何过滤策略在单个普通 GPU 上实现亚秒级推理。该方法结合了基础模型的视觉特征和几何过滤策略，将场景片段中心投影到 3D，并将模板描述符投影到物体模型坐标系中。该方法使用对应驱动的 RANSAC 和加权几何对齐度量来恢复和排名姿态，显示出对噪声、杂乱和部分可见性的高鲁棒性，同时与更大的零样本基线匹配（53.7 AR，1.08 FPS）。</div>
</details>
</div>
<div class="card">
<div class="title">CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</div>
<div class="meta-line">Authors: Tong Zhang, Carlos Hinojosa, Bernard Ghanem</div>
<div class="meta-line">First: 2025-12-11T14:01:47+00:00 · Latest: 2025-12-11T14:01:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10655v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAPTAIN：文本到图像扩散模型中记忆抑制的语义特征注入</div>
<div class="mono" style="margin-top:8px">扩散模型可能会无意中再现训练示例，这在这些系统被大规模部署时引发了隐私和版权方面的担忧。现有的推理时抑制方法通常会操控无分类引导（CFG）或扰动提示嵌入；然而，它们往往难以在不损害与条件提示的对齐的情况下减少记忆。我们引入了CAPTAIN，这是一种无需训练的框架，通过在去噪过程中直接修改潜在特征来抑制记忆。CAPTAIN 首先应用基于频率的噪声初始化以减少在去噪早期再现记忆模式的倾向。然后，它确定特征注入的最佳去噪时间步，并定位记忆区域。最后，CAPTAIN 将非记忆参考图像中的语义对齐特征注入局部化潜在区域，抑制记忆同时保持提示保真度和视觉质量。我们的实验表明，与基于CFG的基线相比，CAPTAIN 在显著减少记忆的同时保持了与预期提示的强大对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to mitigate the issue of diffusion models unintentionally reproducing training examples, which can pose privacy and copyright risks. CAPTAIN, a training-free framework, modifies latent features during denoising to reduce memorization. It initializes noise based on frequency, identifies optimal denoising timesteps, localizes memorized regions, and injects semantically aligned features from non-memorized images. Experiments demonstrate that CAPTAIN significantly reduces memorization compared to CFG-based methods while maintaining prompt alignment and visual quality.</div>
<div class="mono" style="margin-top:8px">研究旨在解决文本到图像扩散模型中的记忆化问题，这可能导致隐私和版权问题。CAPTAIN 是一个无需训练的框架，通过在去噪过程中修改潜特征来减轻记忆化。它使用基于频率的噪声初始化来减少早期复制记忆化模式的趋势，确定最佳去噪时间步长进行特征注入，定位记忆化区域，并从非记忆化参考图像中注入语义对齐的特征，从而抑制记忆化同时保持提示对齐和视觉质量。实验表明，CAPTAIN 相比于基于CFG的方法显著减少了记忆化，同时保持了提示的对齐和视觉质量。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence</div>
<div class="meta-line">Authors: Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, Weidi Xie</div>
<div class="meta-line">First: 2025-05-22T17:59:03+00:00 · Latest: 2025-12-11T13:21:59+00:00</div>
<div class="meta-line">Comments: Technical Report; Project Page: https://haoningwu3639.github.io/SpatialScore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17012v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17012v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://haoningwu3639.github.io/SpatialScore">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialScore：向着全面评估空间智能的方向</div>
<div class="mono" style="margin-top:8px">现有的多模态大型语言模型（MLLMs）在空间智能方面的评估通常是碎片化的且范围有限。在本工作中，我们旨在对现代MLLM的空间理解能力进行全面评估，并提出数据驱动和基于代理的互补解决方案。具体来说，我们做出了以下贡献：(i) 我们引入了SpatialScore，据我们所知，这是迄今为止最全面和多样的多模态空间智能基准。它涵盖了多种视觉数据类型、输入模态和问答格式，并包含约5000个手动验证的样本，覆盖30个不同的任务；(ii) 使用SpatialScore，我们广泛评估了40个代表性MLLM，揭示了当前模型与人类水平的空间智能之间持续存在的挑战和显著差距；(iii) 为了提高模型能力，我们构建了SpatialCorpus，这是一个包含33.1万个跨模态问答样本的大规模训练资源，支持空间推理任务的微调，并显著提高了现有模型的性能（例如Qwen3-VL）；(iv) 为了补充数据驱动的方法，我们开发了SpatialAgent，这是一个配备有12个专门空间感知工具的多代理系统，支持计划-执行和ReAct推理，能够在无需额外模型训练的情况下实现空间推理的显著提升。广泛的实验和深入的分析证明了我们基准、语料库和代理框架的有效性。我们期望这些资源能够为MLLMs向人类水平的空间智能迈进奠定坚实的基础。所有数据、代码和模型将向研究界开放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to comprehensively evaluate the spatial understanding capabilities of modern multimodal large language models (MLLMs) by proposing SpatialScore, a new benchmark that includes various visual data types, input modalities, and question-answering formats. The study evaluates 40 representative MLLMs using SpatialScore, revealing significant gaps between current models and human-level spatial intelligence. Additionally, the authors introduce SpatialCorpus, a large-scale training resource, and SpatialAgent, a multi-agent system with specialized spatial perception tools, to improve spatial reasoning capabilities without additional training. Extensive experiments show the effectiveness of these resources in advancing MLLMs towards human-level spatial intelligence.</div>
<div class="mono" style="margin-top:8px">该研究针对多模态大型语言模型（MLLMs）在空间智能评估上的碎片化问题，引入了SpatialScore，这是一个涵盖多种视觉数据类型、输入模态和问答格式的综合基准。作者使用SpatialScore对40个MLLMs进行了广泛评估，并构建了一个大规模训练资源SpatialCorpus，显著提升了模型性能。此外，他们开发了SpatialAgent，一个具备12种专门空间感知工具的多代理系统，能够在无需额外训练的情况下实现空间推理的显著提升。结果表明，当前模型与人类水平的空间智能之间存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM</div>
<div class="meta-line">Authors: Qintong Zhang, Junyuan Zhang, Zhifei Ren, Linke Ouyang, Zichen Wen, Junbo Niu, Yuan Qu, Bin Wang, Ka-Ho Chow, Conghui He, Wentao Zhang</div>
<div class="meta-line">First: 2025-12-11T13:16:33+00:00 · Latest: 2025-12-11T13:16:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10619v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10619v1">PDF</a> · <a href="https://github.com/ZZZZZQT/DOCR-Inspector">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DOCR-Inspector：使用VLM对文档解析进行细粒度和自动化评估</div>
<div class="mono" style="margin-top:8px">文档解析旨在将无结构的PDF图像转换为半结构化数据，促进信息在各个领域的数字化和利用。尽管视觉语言模型（VLMs）在这一任务中取得了显著进展，但在实际场景中实现可靠且高质量的解析仍然具有挑战性。通常的做法是在标准基准上选择表现最佳的模型，但这些基准可能带有数据集特定的偏差，导致模型排名不一致且与实际性能的相关性有限。此外，基准指标通常仅提供总体评分，这可能会掩盖输出中的不同错误模式。这提出了一个关键挑战：我们如何在野外可靠且全面地评估文档解析质量？我们通过DOCR-Inspector解决了这一问题，将文档解析评估形式化为细粒度的错误检测和分析。利用VLM-as-a-Judge，DOCR-Inspector分析文档图像及其解析输出，识别所有错误，将其归类为28种预定义类型之一，并生成全面的质量评估。为了实现这一能力，我们构建了DOCRcase-200K用于训练，并提出了检查清单推理范式，以实现解析质量评估的层次结构。为了实证验证，我们引入了DOCRcaseBench，这是一个包含882个真实世界文档解析案例的集合，并附有手动注释。在这一基准上，DOCR-Inspector-7B优于商业模型如Gemini 2.5 Pro，以及领先的开源模型。进一步的实验表明，其质量评估为解析结果的改进提供了有价值的指导，使DOCR-Inspector既是实用的评估工具，也是推动大规模文档解析系统发展的驱动力。模型和代码可在以下链接获取：https://github.com/ZZZZZQT/DOCR-Inspector。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DOCR-Inspector addresses the challenge of reliably evaluating document parsing quality in real-world scenarios by formalizing error detection and analysis. It uses VLM-as-a-Judge to identify and categorize errors into 28 predefined types, providing a comprehensive quality assessment. Empirical validation on DOCRcaseBench shows that DOCR-Inspector outperforms commercial and open-source models, offering valuable guidance for parsing result refinement.</div>
<div class="mono" style="margin-top:8px">DOCR-Inspector通过正式化错误检测和分析来可靠地评估文档解析质量，特别是在现实场景中。它使用VLM-as-a-Judge来识别并分类错误到28种类型，提供全面的质量评估。在DOCRcaseBench上的实证验证表明，DOCR-Inspector在性能上超过了商业和领先的开源模型，并为解析结果的优化提供了有价值的指导。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</div>
<div class="meta-line">Authors: J. Xiao, Y. Guo, X. Zi, K. Thiyagarajan, C. Moreira, M. Prasad</div>
<div class="meta-line">First: 2025-12-11T12:43:41+00:00 · Latest: 2025-12-11T12:43:41+00:00</div>
<div class="meta-line">Comments: 6 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10596v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10596v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model&#x27;s low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越像素：一种无需训练的文本到文本框架用于遥感图像检索</div>
<div class="mono" style="margin-top:8px">遥感（RS）图像的语义检索是一项关键任务，从根本上受到‘语义鸿沟’的挑战，即模型的低级视觉特征与高级人类概念之间的差异。虽然大型视觉-语言模型（VLMs）为弥合这一鸿沟提供了有希望的途径，但现有方法通常依赖于昂贵的、特定领域的训练，而且缺乏基准来评估VLM生成文本在零样本检索中的实用价值。为解决这一研究缺口，我们引入了遥感丰富文本（RSRT）数据集，这是一个新的基准，每个图像包含多个结构化的描述。基于此数据集，我们提出了一种完全无需训练的文本检索参考方法TRSLLaVA。我们的方法将跨模态检索重新表述为文本到文本（T2T）匹配问题，利用丰富的文本描述作为查询，与VLM生成的描述在统一的文本嵌入空间中的数据库进行匹配。这种方法完全绕过了模型的训练或微调。在RSITMD和RSICD基准上的实验表明，我们的无需训练方法在与最先进的监督模型的竞争中表现非常出色。例如，在RSITMD上，我们的方法达到了42.62%的平均召回率，几乎是标准零样本CLIP基线23.86%的两倍，并且超过了几个顶级的监督模型。这表明，通过结构化文本获得高质量的语义表示为遥感图像检索提供了一种强大且成本效益高的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the semantic gap in remote sensing image retrieval by introducing the RSRT dataset and a training-free text-to-text retrieval framework called TRSLLaVA. This framework reformulates cross-modal retrieval as a text-to-text matching problem, using rich text descriptions as queries against a database of VLM-generated captions. Experiments show that the proposed method outperforms state-of-the-art supervised models, achieving a mean Recall of 42.62% on RSITMD, nearly doubling the performance of the zero-shot CLIP baseline and surpassing several top supervised models.</div>
<div class="mono" style="margin-top:8px">论文通过引入RSRT数据集和提出训练-free的文本到文本检索框架TRSLLaVA，解决了遥感图像检索中的语义差距问题。TRSLLaVA将跨模态检索重新定义为文本到文本匹配问题，利用VLM生成的描述作为查询与数据库中的文本描述进行匹配，其结果与最先进的监督模型竞争。在RSITMD上，它实现了42.62%的平均召回率，超过了零样本CLIP基线和多个顶级监督模型。</div>
</details>
</div>
<div class="card">
<div class="title">Lightweight Model Attribution and Detection of Synthetic Speech via Audio Residual Fingerprints</div>
<div class="meta-line">Authors: Matías Pizarro, Mike Laszkiewicz, Dorothea Kolossa, Asja Fischer</div>
<div class="meta-line">First: 2024-11-21T10:55:49+00:00 · Latest: 2025-12-11T12:41:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.14013v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.14013v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As speech generation technologies advance, so do risks of impersonation, misinformation, and spoofing. We present a lightweight, training-free approach for detecting synthetic speech and attributing it to its source model. Our method addresses three tasks: (1) single-model attribution in an open-world setting, (2) multi-model attribution in a closed-world setting, and (3) real vs. synthetic speech classification. The core idea is simple: we compute standardized average residuals--the difference between an audio signal and its filtered version--to extract model-agnostic fingerprints that capture synthesis artifacts. Experiments across multiple synthesis systems and languages show AUROC scores above 99%, with strong reliability even when only a subset of model outputs is available. The method maintains high performance under common audio distortions, including echo and moderate background noise, while data augmentation can improve results in more challenging conditions. In addition, out-of-domain detection is performed using Mahalanobis distances to in-domain residual fingerprints, achieving an F1 score of 0.91 on unseen models, reinforcing the method&#x27;s efficiency, generalizability, and suitability for digital forensics and security applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>轻量级模型归因与合成语音检测——基于音频残差指纹的方法</div>
<div class="mono" style="margin-top:8px">随着语音生成技术的进步，冒充、误导和伪造的风险也在增加。我们提出了一种无需训练的轻量级方法，用于检测合成语音并将其归因于其来源模型。该方法解决了三个任务：（1）开放世界中的单模型归因，（2）封闭世界中的多模型归因，以及（3）真实语音与合成语音的分类。核心思想很简单：我们计算标准化平均残差——即音频信号与其滤波版本之间的差异——以提取模型无关的指纹，捕捉合成特征。在多个合成系统和语言的实验中，AUROC分数超过99%，即使只有模型输出的一部分可用时，也表现出很强的可靠性。该方法在常见的音频失真下保持高性能，包括回声和中等背景噪声，而数据增强可以在更具挑战性的条件下提高结果。此外，通过马哈拉诺比斯距离对域外残差指纹进行域外检测，未见过的模型F1分数达到0.91，进一步证明了该方法的高效性、普适性和在数字取证和安全应用中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a lightweight, training-free method for detecting synthetic speech and attributing it to its source model. The approach computes standardized average residuals to extract model-agnostic fingerprints, which are effective for single and multi-model attribution and real vs. synthetic speech classification. Experiments show high AUROC scores above 99% and strong reliability under various audio conditions, with improved performance using data augmentation in challenging scenarios. Out-of-domain detection using Mahalanobis distances achieves an F1 score of 0.91 on unseen models, highlighting the method&#x27;s efficiency and generalizability for digital forensics and security applications.</div>
<div class="mono" style="margin-top:8px">本文提出了一种轻量级、无需训练的方法，用于检测合成语音并将其归因于其源模型。该方法通过计算标准化平均残差来提取模型无关的指纹，适用于单模型和多模型归因以及真实与合成语音分类。实验结果显示AUROC分数超过99%，在各种音频条件下表现出色，通过数据增强在更具挑战性的场景中可进一步提高性能。使用马哈拉诺比斯距离进行跨域检测，对未见过的模型F1分数达到0.91，突显了该方法的高效性、普适性和在数字取证和安全应用中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data</div>
<div class="meta-line">Authors: Ivo Bueno, Ruikun Hou, Babette Bühler, Tim Fütterer, James Drimalla, Jonathan Kyle Foster, Peter Youngs, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2025-11-26T11:57:22+00:00 · Latest: 2025-12-11T11:15:19+00:00</div>
<div class="meta-line">Comments: This article has been accepted for publication in the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00087v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00087v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索多模态课堂数据中教学活动与对话的自动化识别</div>
<div class="mono" style="margin-top:8px">课堂互动观察可以为教师提供具体的反馈，但当前方法依赖于手动标注，这既耗资源又难以扩展。本研究探索了基于人工智能的课堂录像分析方法，重点在于多模态教学活动和对话识别，作为可操作反馈的基础。使用包含164小时视频和68份教案文本的密集标注数据集，我们设计了并行的、模态特定的流水线。对于视频，我们评估了零样本多模态LLM、微调视觉-语言模型以及自监督视频变换器在24个活动标签上的表现。对于文本，我们使用上下文化输入的变压器分类器进行微调，并将其与基于提示的LLM进行比较，以19个对话标签进行对比。为了解决类别不平衡和多标签复杂性问题，我们应用了标签阈值、上下文窗口和不平衡损失函数。结果表明，微调模型在视频和文本上的表现均优于基于提示的方法，视频的宏F1得分为0.577，文本的得分为0.460。这些结果证明了自动化课堂分析的可行性，并为可扩展的教师反馈系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to automate the recognition of instructional activities and discourse in classrooms using AI-driven analysis, addressing the resource-intensive nature of manual annotation. The research employs a dataset of 164 hours of video and 68 lesson transcripts, evaluating various models for video and transcript analysis. For video, zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers were assessed, while for transcripts, a transformer-based classifier was fine-tuned and compared with prompting-based LLMs. The results indicate that fine-tuned models outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts, demonstrating the potential for automated classroom analysis and scalable teacher feedback systems.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过AI驱动的多模态数据分析自动化识别课堂中的教学活动和对话。研究采用并行的模态特定管道，评估零样本多模态LLM、微调的视觉语言模型和自监督视频变换器对视频数据的性能，以及对转录文本进行微调的变换器分类器。结果表明，微调模型优于提示基模型，视频数据的宏F1得分为0.577，转录文本的得分为0.460，这表明自动化课堂分析的可行性，并为可扩展的教师反馈系统奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</div>
<div class="meta-line">Authors: Lim Chien Her, Ming Yan, Yunshu Bai, Ruihao Li, Hao Zhang</div>
<div class="meta-line">First: 2025-12-11T10:22:02+00:00 · Latest: 2025-12-11T10:22:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10501v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本3D地图生成：用于程序化内容生成的双智能体架构</div>
<div class="mono" style="margin-top:8px">程序化内容生成（PCG）提供了通过算法创建复杂可定制世界的可扩展方法。然而，控制这些管道需要精确配置不透明的技术参数。我们提出了一种无需训练的架构，利用LLM智能体进行零样本PCG参数配置。虽然大型语言模型（LLMs）承诺为PCG工具提供自然语言界面，但现成的模型往往无法弥合从抽象用户指令到严格参数规范之间的语义差距。我们的系统将一个执行者智能体与一个评论者智能体配对，使系统能够自主推理工具参数并逐步优化配置以与人类设计偏好相一致。我们在生成各种3D地图方面验证了这种方法，建立了PCG指令遵循的新基准。实验表明，我们的方法优于单智能体基线，能够从自然语言描述中生成多样且结构有效的环境。这些结果表明，现成的LLM可以有效重新利用为任意PCG工具的一般智能体。通过将负担从模型训练转移到架构推理，我们的方法提供了一种无需特定任务微调的复杂软件掌握的可扩展框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a zero-shot 3D map generation method using a dual-agent architecture with an Actor and a Critic agent. This approach leverages Large Language Models (LLMs) to bridge the gap between abstract user instructions and technical parameters, enabling natural language interface for procedural content generation. Experiments show that this method outperforms single-agent baselines, generating diverse and structurally valid 3D environments from natural language descriptions, thus demonstrating the potential of off-the-shelf LLMs for PCG tools.</div>
<div class="mono" style="margin-top:8px">论文提出了一种使用Actor和Critic双代理架构的零样本3D地图生成方法，利用大型语言模型（LLMs）弥合抽象用户指令和技术参数之间的差距，实现程序化内容生成的自然语言接口。实验表明，该方法在自然语言描述下生成多样且结构有效的3D环境方面优于单代理基线，从而展示了现成的LLMs在PCG工具中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</div>
<div class="meta-line">Authors: Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-22T09:16:34+00:00 · Latest: 2025-12-11T09:40:22+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17552v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.17552v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型能否在无需训练的情况下推理非文本模态？一种基于上下文表示学习的案例研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的出色性能可以通过测试时计算得到增强，这依赖于外部工具甚至其他深度学习模型。然而，将非文本模态表示集成到LLMs中的现有方法通常需要额外的昂贵的监督训练，限制了对新领域和模态的即时适应。在本文中，我们探讨了以无需训练的方式将非文本基础模型（FMs）的表示集成到基于文本的LLMs中的可行性。我们提出了一种基于上下文表示学习（ICRL）的概念，以允许LLMs通过少样本学习适应性地利用非文本模态表示。与传统的基于上下文学习不同，ICRL用FM表示替换文本输入，使LLM能够在无需微调的情况下进行多模态推理。我们在分子领域的多个任务上评估了ICRL，探讨了三个核心研究问题：（i）如何以无需训练的方式将FM表示映射到LLMs中，（ii）哪些因素影响ICRL的性能，（iii）ICRL有效性的机制是什么。据我们所知，ICRL是第一个无需训练的框架，用于将非文本模态表示集成到基于文本的LLMs中，为适应性、多模态泛化的研究提供了有希望的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the feasibility of integrating non-text modality representations into text-based Large Language Models (LLMs) in a training-free manner. The proposed In-Context Representation Learning (ICRL) allows LLMs to adaptively utilize non-text modality representations through few-shot learning. The study evaluates ICRL on molecular domain tasks and finds that it can perform multi-modal inference without fine-tuning, addressing the limitation of requiring additional costly supervised training for integrating non-text modalities into LLMs.</div>
<div class="mono" style="margin-top:8px">这项工作探讨了在无需训练的情况下将非文本模态表示集成到基于文本的大语言模型（LLMs）中的可行性。提出的In-Context Representation Learning（ICRL）允许LLMs通过少样本学习适应性地利用非文本模态表示。研究在分子领域任务上评估了ICRL，发现它可以在无需微调的情况下进行多模态推理，解决了将非文本模态集成到LLMs中需要额外昂贵的监督训练的限制。</div>
</details>
</div>
<div class="card">
<div class="title">Test-Time Distillation for Continual Model Adaptation</div>
<div class="meta-line">Authors: Xiao Chen, Jiazhen Huang, Zhiming Liu, Qinting Jiang, Fanding Huang, Jingyan Jiang, Zhi Wang</div>
<div class="meta-line">First: 2025-06-03T09:16:51+00:00 · Latest: 2025-12-11T09:34:01+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02671v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.02671v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks often suffer performance degradation upon deployment due to distribution shifts. Continual Test-Time Adaptation (CTTA) aims to address this issue in an unsupervised manner, yet existing methods, which rely on self-supervision, are prone to an inherent self-referential feedback loop that amplifies initial prediction errors, leading to model drift. We revisit this limitation and propose Test-Time Distillation (TTD), which reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM) as an external signal. While promising, we find that direct distillation is fraught with two pitfalls: the Generalist Trap, where the VLM&#x27;s broad but non-specialized knowledge leads to suboptimal performance on specific tasks and shifts, and the Entropy Bias, where naive model fusion techniques based on entropy fail due to the disparate calibration of heterogeneous models. These pitfalls motivate our insight: the key is to build a robust supervisory signal and leverage it to guide the target model toward stable adaptation. Hence, we present CoDiRe, a Continual Distillation and Rectification framework for TTD. CoDiRe first constructs a robust blended teacher by dynamically fusing the predictions of the VLM and the target model. Critically, it circumvents the Entropy Bias by leveraging Maximum Softmax Probability (MSP) as a more reliable confidence metric for weighting each model&#x27;s expertise. Then applies an Optimal Transport based rectification to further align predictions with the blended teacher, enabling continuous and stable adaptation. Extensive experiments show that CoDiRe outperforms state-of-the-art baselines, exceeding CoTTA by 10.55% while using only 48% of its time cost on ImageNet-C.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>部署时蒸馏以实现持续模型适应</div>
<div class="mono" style="margin-top:8px">深度神经网络在部署时由于分布偏移往往会遭受性能下降。持续测试时适应（CTTA）旨在以无监督的方式解决这一问题，但现有方法依赖于自我监督，容易产生一种固有的自我参照反馈循环，这会放大初始预测误差，导致模型漂移。我们重新审视了这一局限性，并提出了测试时蒸馏（TTD），将适应重新定义为由冻结的视觉-语言模型（VLM）作为外部信号引导的蒸馏过程。虽然前景广阔，但我们发现直接蒸馏存在两个陷阱：专家陷阱，其中VLM的广泛但非专门化的知识导致特定任务和转移上的次优性能；以及熵偏差，其中基于熵的简单模型融合技术由于异构模型的不一致校准而失效。这些陷阱促使我们得出一个见解：关键在于构建一个稳健的监督信号，并利用它来引导目标模型实现稳定的适应。因此，我们提出了CoDiRe，一种持续蒸馏和校正框架，用于TTD。CoDiRe首先通过动态融合VLM和目标模型的预测来构建一个稳健的混合教师。关键在于通过利用最大softmax概率（MSP）作为更可靠的置信度度量来规避熵偏差，从而为每个模型的专业知识分配权重。然后应用基于最优传输的校正，进一步使预测与混合教师对齐，从而实现持续和稳定的适应。大量实验表明，CoDiRe优于最先进的基线，其性能比CoTTA高出10.55%，同时仅使用其48%的时间成本在ImageNet-C上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the performance degradation of deep neural networks due to distribution shifts by proposing Test-Time Distillation (TTD) as a continual test-time adaptation method. TTD reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM). To overcome the Generalist Trap and Entropy Bias, the authors introduce CoDiRe, a framework that constructs a robust blended teacher and uses Maximum Softmax Probability (MSP) for weighting model expertise, followed by Optimal Transport-based rectification. Experiments demonstrate that CoDiRe outperforms existing methods, achieving a 10.55% improvement over CoTTA with only 48% of its time cost on ImageNet-C.</div>
<div class="mono" style="margin-top:8px">论文针对部署后由于分布变化导致的深度神经网络性能下降问题，提出了一种名为Test-Time Distillation (TTD)的方法，利用冻结的Vision-Language Model (VLM)作为外部信号进行模型适应。为了解决Generalist Trap和Entropy Bias的问题，作者引入了CoDiRe框架，该框架动态融合了VLM和目标模型的预测，并使用Maximum Softmax Probability (MSP)进行加权。此外，CoDiRe还采用了Optimal Transport进行对齐，实验结果显示其性能优于现有方法，且计算成本更低。</div>
</details>
</div>
<div class="card">
<div class="title">SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Yuyang Dong, Nobuhiro Ueda, Krisztián Boros, Daiki Ito, Takuya Sera, Masafumi Oyamada</div>
<div class="meta-line">First: 2025-05-20T14:03:24+00:00 · Latest: 2025-12-11T08:51:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.14381v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.14381v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCAN: 语义文档布局分析以增强文本和视觉检索生成</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）和视觉语言模型（VLMs）的广泛应用，用于检索增强生成（RAG）和视觉RAG等应用的丰富文档分析技术正获得广泛关注。最近的研究表明，使用VLMs可以提高RAG性能，但处理丰富文档仍是一个挑战，因为单页包含大量信息。本文提出了一种名为SCAN（语义文档布局分析）的新方法，该方法增强了处理丰富视觉文档的文本和视觉RAG系统。这是一种VLM友好的方法，能够以适当的语义粒度识别文档组件，平衡上下文保留与处理效率。SCAN采用粗粒度语义方法，将文档划分为包含连续组件的连贯区域。我们通过在标注数据集上微调对象检测模型来训练SCAN模型。我们的实验结果表明，SCAN在英语和日语数据集上的端到端文本RAG性能提高了9.4个点，视觉RAG性能提高了10.4个点，优于传统方法，甚至超越了商用文档处理解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SCAN is a novel approach that enhances both textual and visual RAG systems by identifying document components with appropriate semantic granularity. It uses a coarse-grained semantic approach to divide documents into coherent regions, improving end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points compared to conventional approaches and commercial solutions.</div>
<div class="mono" style="margin-top:8px">SCAN 是一种新颖的方法，通过识别具有适当语义粒度的文档组件来增强视觉丰富文档的 Retrieval-Augmented Generation (RAG) 系统。它使用粗粒度的语义方法将文档划分为一致的区域，分别提高文本和视觉 RAG 性能高达 9.4 和 10.4 点，优于传统方法和商业文档处理解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks</div>
<div class="meta-line">Authors: Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Huan Gao, Mingkun Xu, Shangyang Li</div>
<div class="meta-line">First: 2025-09-26T12:20:01+00:00 · Latest: 2025-12-11T08:31:33+00:00</div>
<div class="meta-line">Comments: 23 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22258v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22258v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://neuromedbench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越分类准确率：Neural-MedBench和深入推理基准的需求</div>
<div class="mono" style="margin-top:8px">近期视觉-语言模型（VLMs）在标准医学基准测试中取得了显著的性能，但其真正的临床推理能力仍然不清楚。现有数据集主要强调分类准确率，导致一种评估错觉，即模型看似熟练但实际上在高风险诊断推理方面仍然失败。我们引入了Neural-MedBench，这是一个紧凑但推理密集的基准，专门设计用于探索神经学多模态临床推理的极限。Neural-MedBench 结合了多序列MRI扫描、结构化的电子健康记录和临床笔记，并涵盖了三个核心任务家族：鉴别诊断、病灶识别和理由生成。为了确保可靠的评估，我们开发了一种混合评分流水线，结合了基于LLM的评分、临床验证和语义相似度指标。通过对包括GPT-4o、Claude-4和MedGemma在内的最新VLMs进行系统评估，我们观察到与传统数据集相比，性能出现了显著下降。错误分析表明，推理失败而非感知错误是模型的主要缺陷。我们的研究结果强调了双重评估框架的必要性：广度导向的大数据集用于统计泛化，以及深度导向、紧凑的基准如Neural-MedBench用于推理准确性。我们通过https://neuromedbench.github.io/发布了Neural-MedBench，作为开放和可扩展的诊断测试平台，指导未来基准的扩展，并实现严格的成本效益评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to evaluate the true clinical reasoning ability of vision-language models (VLMs) by introducing Neural-MedBench, a benchmark that focuses on reasoning-intensive tasks in neurology. The method involves integrating multi-sequence MRI scans, structured electronic health records, and clinical notes, and using a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Key findings show a significant performance drop in VLMs compared to conventional datasets, with reasoning failures being the primary issue. The study emphasizes the need for a Two-Axis Evaluation Framework combining breadth-oriented large datasets and depth-oriented compact benchmarks like Neural-MedBench for assessing reasoning fidelity in clinical applications.</div>
<div class="mono" style="margin-top:8px">论文提出了Neural-MedBench，一个旨在评估神经学领域中视觉-语言模型推理能力的基准，重点关注鉴别诊断、病灶识别和推理生成。通过整合MRI扫描、电子健康记录和临床笔记，并使用混合评分管道，研究发现最先进的模型在高风险诊断推理中表现不佳，主要问题是推理失败。作者提出了一个两轴评估框架，以指导开发更可靠的临床AI。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention</div>
<div class="meta-line">Authors: Yang Yu, Zhuangzhuang Chen, Siqi Wang, Lanqing Li, Xiaomeng Li</div>
<div class="meta-line">First: 2025-12-11T08:27:02+00:00 · Latest: 2025-12-11T08:27:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10414v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10414v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL-based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing studies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ignore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the diversity of responses. In this paper, we propose Selective-adversarial Entropy Intervention, namely SaEI, which enhances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the entropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formulates the entropy of sampled responses as an adversarial objective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger answer space during RL sampling. Then, we propose token-selective entropy computation (TsEC) to maximize the effectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于选择性对抗熵干预的RL增强视觉推理</div>
<div class="mono" style="margin-top:8px">近年来，强化学习（RL）已成为提升视觉语言模型（VLMs）推理能力的常见选择。考虑到现有的基于RL的微调方法，通过干预熵可以有效提高探索能力，从而改善策略性能。值得注意的是，大多数现有研究在RL策略优化过程中仅通过控制特定标记的更新来干预熵，而忽略了在RL采样过程中干预熵可以提升GRPO性能，通过提高响应的多样性。在本文中，我们提出了一种选择性对抗熵干预方法，即SaEI，通过使用从采样响应的熵中来的标记选择性对抗目标来扭曲视觉输入，从而增强策略熵。具体而言，我们首先提出了基于熵引导的对抗采样（EgAS），将采样响应的熵作为对抗目标进行建模。然后，相应的对抗梯度可以用于攻击视觉输入以生成对抗样本，使策略模型在RL采样过程中探索更大的答案空间。接着，我们提出了标记选择性熵计算（TsEC），以在EgAS中最大化对抗攻击的有效性，而不扭曲VLMs中的事实知识。在领域内和领域外数据集上的广泛实验表明，我们提出的方法可以通过熵干预大大增强策略探索，从而提升推理能力。论文被接受后将发布代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to enhance the reasoning capabilities of vision-language models using reinforcement learning (RL) by proposing Selective-adversarial Entropy Intervention (SaEI). The method introduces entropy-guided adversarial sampling (EgAS) to improve the diversity of responses during RL sampling and token-selective entropy computation (TsEC) to avoid distorting factual knowledge. Experiments show that SaEI significantly boosts policy exploration and reasoning capabilities on both in-domain and out-of-domain datasets.</div>
<div class="mono" style="margin-top:8px">本文提出了一种选择性对抗熵干预（SaEI）方法，以增强基于强化学习（RL）的视觉推理能力。该方法通过引入熵引导的对抗采样（EgAS）来提高RL采样期间响应的多样性，并通过令牌选择性熵计算（TsEC）确保有效的对抗攻击而不扭曲VLM中的事实知识。实验结果表明，SaEI在领域内和领域外数据集上显著提升了策略探索和推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</div>
<div class="meta-line">Authors: Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-30T04:57:26+00:00 · Latest: 2025-12-11T08:10:13+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00090v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00090v3">PDF</a> · <a href="https://github.com/UnicomAI/LeMiCa">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LeMiCa：基于字典序最小最大路径缓存的高效扩散式视频生成加速框架</div>
<div class="mono" style="margin-top:8px">我们提出了LeMiCa，一种无需训练且高效的加速框架，用于扩散式视频生成。现有的缓存策略主要侧重于减少局部启发式错误，但往往忽视了全局错误的累积，导致加速后的视频与原始视频之间存在明显的内容降级。为了解决这一问题，我们将缓存调度形式化为带有加权边的有向图，并引入了一种字典序最小最大路径优化策略，该策略明确界定了最坏情况路径误差。这种方法显著提高了生成帧中全局内容和风格的一致性。在多个文本到视频基准上的大量实验表明，LeMiCa在推理速度和生成质量上都取得了双重改进。值得注意的是，我们的方法在Latte模型上实现了2.9倍的加速，并在Open-Sora上达到了LPIPS分数0.05，优于先前的缓存技术。重要的是，这些改进伴随着最小的感知质量下降，使LeMiCa成为加速扩散式视频生成的稳健且通用的范式。我们认为这种方法可以作为未来高效可靠视频合成研究的强大基础。我们的代码可在：https://github.com/UnicomAI/LeMiCa 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LeMiCa is a training-free acceleration framework for diffusion-based video generation that addresses the issue of global error accumulation in existing caching strategies. By formulating cache scheduling as a directed graph and using Lexicographic Minimax Path Optimization, LeMiCa ensures consistent global content and style across frames. Experiments show that LeMiCa improves both inference speed and generation quality, achieving a 2.9x speedup and an LPIPS score of 0.05, outperforming previous methods with minimal perceptual quality loss.</div>
<div class="mono" style="margin-top:8px">LeMiCa 是一个无需训练的加速框架，用于基于扩散的视频生成，解决了现有缓存策略中全局误差累积的问题。通过将缓存调度形式化为有向图，并使用 Lexicographic Minimax 路径优化，LeMiCa 确保了生成帧中全局内容和风格的一致性。实验表明，LeMiCa 在提高推理速度和生成质量方面表现出色，实现了 2.9 倍的加速，并在 Open-Sora 上达到了 LPIPS 分数 0.05，同时保持了最小的感知质量下降。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking Ahead: Foresight Intelligence in MLLMs and World Models</div>
<div class="meta-line">Authors: Zhantao Gong, Liaoyuan Fan, Qing Guo, Xun Xu, Xulei Yang, Shijie Li</div>
<div class="meta-line">First: 2025-11-24T04:04:59+00:00 · Latest: 2025-12-11T08:02:01+00:00</div>
<div class="meta-line">Comments: 25 pages, 27 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18735v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>前瞻智能：在MLLMs和世界模型中的预见能力</div>
<div class="mono" style="margin-top:8px">在本研究中，我们将前瞻智能定义为预见和解释未来事件的能力——这种能力对于自动驾驶等应用至关重要，但现有研究却对其关注不足。为弥补这一差距，我们引入了FSU-QA，这是一个新的视觉问答（VQA）数据集，专门设计用于激发和评估前瞻智能。通过FSU-QA，我们首次对最先进的视觉语言模型（VLMs）在前瞻任务下的表现进行了全面研究，发现当前模型在推理未来情况方面仍然存在困难。除了作为基准测试之外，FSU-QA 还能够通过测量其生成预测的语义连贯性来评估世界模型，这种连贯性通过VLMs与这些输出结合后的性能提升来量化。我们的实验进一步证明，即使是对FSU-QA进行微调的小型VLMs也能显著超越更大、更先进的模型。这些发现共同将FSU-QA定位为开发下一代能够真正预见和理解未来事件的模型的原理性基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces Foresight Intelligence as the ability to anticipate future events, crucial for applications like autonomous driving. The authors developed FSU-QA, a new VQA dataset, to evaluate this capability in Vision-Language Models (VLMs). Experiments show that current VLMs struggle with foresight reasoning, but even small models fine-tuned on FSU-QA outperform larger, more advanced models. FSU-QA thus serves as a benchmark for developing models that can better predict future scenarios.</div>
<div class="mono" style="margin-top:8px">本文提出了预见智能，即预见未来事件的能力，这对于自动驾驶等应用至关重要，但在现有研究中却鲜有涉及。作者开发了FSU-QA，这是一个新的VQA数据集，用于评估Vision-Language模型（VLM）的预见能力。实验表明，当前的VLM在预见推理方面存在困难，但即使是经过FSU-QA微调的小型模型也超过了更大、更先进的模型。FSU-QA还通过测量生成预测的语义连贯性来增强世界模型的评估。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies</div>
<div class="meta-line">Authors: Cong Pang, Hongtao Yu, Zixuan Chen, Lewei Lu, Xin Lou</div>
<div class="meta-line">First: 2025-12-11T07:48:34+00:00 · Latest: 2025-12-11T07:48:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10384v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10384v1">PDF</a> · <a href="https://github.com/pc-inno/FROW">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model&#x27;s category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型视觉语言模型精细粒度识别：基准与优化策略</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）取得了显著进展，使复杂的视觉语言交互和对话应用成为可能。然而，现有的基准主要集中在推理任务上，往往忽视了精细粒度识别，这对于实际应用场景至关重要。为解决这一问题，我们引入了精细粒度识别开放世界（FROW）基准，旨在通过GPT-4o对LVLMs进行详细评估。在此基础上，我们从数据构建和训练过程两个方面提出了新的优化策略，以提高LVLMs的性能。我们的数据集包括拼接数据，将多个短答案响应结合在一起，以及来自真实世界问题和答案的开放世界数据，使用GPT-4o生成，从而为评估LVLMs的精细粒度识别提供了一个全面框架。实验表明，拼接数据可将类别识别准确性提高1%，开放世界数据可将FROW基准准确性提高10%-20%，内容准确性提高6%-12%。同时，将精细粒度数据纳入预训练阶段可将模型的类别识别准确性提高多达10%。基准将可在https://github.com/pc-inno/FROW/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing benchmarks that primarily focus on reasoning tasks and neglect fine-grained recognition. It introduces the FROW benchmark using GPT-4o for evaluating LVLMs and proposes an optimization strategy from data construction and training process perspectives. The study shows that mosaic data improves category recognition accuracy by 1%, open-world data boosts FROW benchmark accuracy by 10%-20% and content accuracy by 6%-12%, and incorporating fine-grained data into pre-training can enhance category recognition accuracy by up to 10%.</div>
<div class="mono" style="margin-top:8px">本文针对现有基准主要关注推理任务而忽视精细识别的问题，提出了精细识别开放世界（FROW）基准并使用GPT-4o进行评估。研究提出了一种从数据构建和训练过程两个角度优化的方法。研究表明，使用拼接数据可以提高类别识别准确率1%，使用开放世界数据可以提高FROW基准准确率10%-20%和内容准确率6%-12%，将精细识别数据纳入预训练阶段可以提高类别识别准确率高达10%。</div>
</details>
</div>
<div class="card">
<div class="title">Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos</div>
<div class="meta-line">Authors: Mingyu Jeon, Jisoo Yang, Sungjin Han, Jinkwon Hwang, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</div>
<div class="meta-line">First: 2025-12-11T07:25:48+00:00 · Latest: 2025-12-11T07:25:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10363v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a &#x27;Search-then-Refine&#x27; approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a &#x27;search&#x27; phase candidate explosion, and (2) the &#x27;refine&#x27; phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient &#x27;search&#x27; and costly &#x27;refine&#x27; phases. P2S overcomes these challenges with two key innovations: an &#x27;Adaptive Span Generator&#x27; to prevent the search phase candidate explosion, and &#x27;Query Decomposition&#x27; to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>点到区间：导航未见过的时长视频的零样本时刻检索</div>
<div class="mono" style="margin-top:8px">零样本长视频时刻检索（ZLVMR）是指使用自然语言查询在无任务特定训练的情况下识别时长视频中的时间片段的任务。LVMR的核心技术挑战源于一次性处理整个长视频的计算不可行性。这一限制已经确立了“搜索-然后细化”的方法，其中候选者迅速缩小，仅分析那些部分，成为LVMR的主要范式。然而，现有方法对此范式面临严重限制。传统的监督学习在可扩展性和泛化能力方面存在局限性，尽管消耗了大量资源。然而，现有的零样本方法也失败了，面临双重挑战：（1）其启发式策略导致“搜索”阶段候选者爆炸，（2）“细化”阶段，由于语义差异易受攻击，需要高成本的VLM验证，导致显著的计算开销。我们提出了**P**oint-**to**-**S**pan（P2S），一种新的无需训练的框架，以克服这一低效的“搜索”和昂贵的“细化”阶段的挑战。P2S通过两个关键创新克服了这些挑战：一个“自适应区间生成器”以防止搜索阶段候选者爆炸，以及“查询分解”以在不依赖高成本VLM验证的情况下细化候选者。据我们所知，P2S是第一个能够在时长视频中实现时间定位的零样本框架，显著优于监督下的最新方法（例如，在MAD上的R5@0.1上提高了3.7%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of zero-shot long video moment retrieval (ZLVMR) by proposing a novel framework called Point-to-Span (P2S). Motivated by the inefficiencies in existing &#x27;Search-then-Refine&#x27; approaches, P2S introduces an &#x27;Adaptive Span Generator&#x27; to reduce the number of candidates in the search phase and &#x27;Query Decomposition&#x27; to refine candidates without relying on high-cost VLM verification. Experimental results show that P2S outperforms supervised state-of-the-art methods by 3.7% on R5@0.1 on MAD, demonstrating its effectiveness in handling hour-long videos.</div>
<div class="mono" style="margin-top:8px">论文提出了一个训练免费框架Point-to-Span (P2S)，通过使用自适应跨度生成器减少搜索阶段的候选数量，并使用查询分解在不依赖昂贵的VLM验证的情况下精简候选。该方法在处理小时长视频和自然语言查询方面优于监督下的最新方法，例如在MAD上的R5@0.1上提高了3.7%的表现。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
