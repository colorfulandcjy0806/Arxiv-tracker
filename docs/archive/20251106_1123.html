<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-06 11:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251106_1123</div>
    <div class="row"><div class="card">
<div class="title">Image Super-Resolution with Guarantees via Conformalized Generative   Models</div>
<div class="meta-line">Authors: Eduardo Adame, Daniel Csillag, Guilherme Tegoni Goedert</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-12T13:14:57+00:00 · Latest: 2025-11-04T17:06:44+00:00</div>
<div class="meta-line">Comments: To appear at NeurIPS 2025. 17 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.09664v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.09664v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of generative ML foundation models for image restoration
tasks such as super-resolution calls for robust and interpretable uncertainty
quantification methods. We address this need by presenting a novel approach
based on conformal prediction techniques to create a &#x27;confidence mask&#x27; capable
of reliably and intuitively communicating where the generated image can be
trusted. Our method is adaptable to any black-box generative model, including
those locked behind an opaque API, requires only easily attainable data for
calibration, and is highly customizable via the choice of a local image
similarity metric. We prove strong theoretical guarantees for our method that
span fidelity error control (according to our local image similarity metric),
reconstruction quality, and robustness in the face of data leakage. Finally, we
empirically evaluate these results and establish our method&#x27;s solid
performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过符合化生成模型的图像超分辨率保证</div>
<div class="mono" style="margin-top:8px">生成式机器学习基础模型在图像恢复任务（如超分辨率）中的日益使用，呼唤稳健且可解释的不确定性量化方法。我们通过提出一种基于符合预测技术的新方法来满足这一需求，创建一个能够可靠且直观地传达生成图像可信度的“置信掩码”。我们的方法适用于任何黑箱生成模型，包括那些被封闭API锁定的模型，仅需易于获取的数据进行校准，并且可以通过选择局部图像相似性度量进行高度定制。我们证明了我们的方法在保真度误差控制（根据我们的局部图像相似性度量）、重建质量和面对数据泄漏的稳健性方面具有强大的理论保证。最后，我们对这些结果进行了实证评估，并确立了我们方法的稳固性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the need for robust and interpretable uncertainty quantification methods in generative machine learning models used for image restoration tasks like super-resolution. The authors propose a novel approach utilizing conformal prediction techniques to develop a &#x27;confidence mask&#x27; that effectively indicates the reliability of generated images. Experimental results demonstrate strong theoretical guarantees regarding fidelity error control, reconstruction quality, and robustness against data leakage, alongside empirical evaluations confirming the method&#x27;s solid performance across various scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机是需要在用于图像恢复任务（如超分辨率）的生成机器学习模型中实现稳健和可解释的不确定性量化方法。作者提出了一种新方法，利用符合预测技术开发“置信掩码”，以指示生成图像的可靠性。该方法可适应各种黑箱生成模型，仅需少量校准数据，并允许通过局部图像相似性度量进行定制，展示了在保真度误差控制、重建质量和对数据泄露的稳健性方面的强理论保证，并通过实证评估验证了这些结果。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal</div>
<div class="meta-line">Authors: Rongxin Liao, Feng Li, Yanyan Wei, Zenglin Shi, Le Zhang, Huihui Bai, Meng Wang</div>
<div class="meta-line">First: 2025-03-12T03:03:06+00:00 · Latest: 2025-11-04T15:59:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.09013v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.09013v2">PDF</a> · <a href="https://github.com/RongxinL/CyclicPrompt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal adverse weather removal (UAWR) seeks to address various weather
degradations within a unified framework. Recent methods are inspired by prompt
learning using pre-trained vision-language models (e.g., CLIP), leveraging
degradation-aware prompts to facilitate weather-free image restoration,
yielding significant improvements. In this work, we propose CyclicPrompt, an
innovative cyclic prompt approach designed to enhance the effectiveness,
adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key
components: 1) a composite context prompt that integrates weather-related
information and context-aware representations into the network to guide
restoration. This prompt differs from previous methods by marrying learnable
input-conditional vectors with weather-specific knowledge, thereby improving
adaptability across various degradations. 2) The erase-and-paste mechanism,
after the initial guided restoration, substitutes weather-specific knowledge
with constrained restoration priors, inducing high-quality weather-free
concepts into the composite prompt to further fine-tune the restoration
process. Therefore, we can form a cyclic &quot;Prompt-Restore-Prompt&quot; pipeline that
adeptly harnesses weather-specific knowledge, textual contexts, and reliable
textures. Extensive experiments on synthetic and real-world datasets validate
the superior performance of CyclicPrompt. The code is available at:
https://github.com/RongxinL/CyclicPrompt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示恢复，恢复提示：用于通用不良天气去除的循环提示</div>
<div class="mono" style="margin-top:8px">通用不良天气去除（UAWR）旨在在统一框架内解决各种天气退化。最近的方法受到使用预训练视觉-语言模型（如CLIP）的提示学习的启发，利用退化感知提示促进无天气图像恢复，取得了显著的改进。在这项工作中，我们提出了CyclicPrompt，一种创新的循环提示方法，旨在增强UAWR的有效性、适应性和通用性。CyclicPrompt包括两个关键组件：1）复合上下文提示，将与天气相关的信息和上下文感知表示集成到网络中以指导恢复。该提示与以前的方法不同，通过将可学习的输入条件向量与特定天气知识结合，从而提高了在各种退化中的适应性。2）擦除和粘贴机制，在初始引导恢复后，用受限恢复先验替代特定天气知识，将高质量的无天气概念引入复合提示，以进一步微调恢复过程。因此，我们可以形成一个循环的“提示-恢复-提示”管道，巧妙地利用特定天气知识、文本上下文和可靠的纹理。在合成和真实世界数据集上的广泛实验验证了CyclicPrompt的优越性能。代码可在以下网址获取：https://github.com/RongxinL/CyclicPrompt。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve universal adverse weather removal (UAWR) by leveraging advancements in prompt learning with pre-trained vision-language models. The authors propose a novel method called CyclicPrompt, which incorporates a composite context prompt and an erase-and-paste mechanism to enhance the adaptability and generalizability of weather-free image restoration. Experimental results demonstrate that CyclicPrompt significantly outperforms existing methods on both synthetic and real-world datasets, effectively integrating weather-specific knowledge and improving restoration quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过利用提示学习技术来改善通用不良天气去除（UAWR），以增强在各种天气条件下的图像恢复。作者提出了CyclicPrompt，它由一个复合上下文提示组成，该提示将与天气相关的信息与上下文感知表示相结合，以及一个擦除和粘贴机制，通过用恢复先验替代天气特定知识来优化恢复过程。实验结果表明，CyclicPrompt在合成和真实世界数据集上显著优于现有方法，展示了其在恢复受不良天气影响的图像方面的有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Super-Resolution: Towards Diffusion-Based Methods   without Motion Alignment</div>
<div class="meta-line">Authors: Zhihao Zhan, Wang Pang, Xiang Zhu, Yechao Bai</div>
<div class="meta-line">First: 2025-03-05T10:37:51+00:00 · Latest: 2025-11-04T13:48:25+00:00</div>
<div class="meta-line">Comments: ICSPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03355v5">Abs</a> · <a href="http://arxiv.org/pdf/2503.03355v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we rethink the approach to video super-resolution by
introducing a method based on the Diffusion Posterior Sampling framework,
combined with an unconditional video diffusion transformer operating in latent
space. The video generation model, a diffusion transformer, functions as a
space-time model. We argue that a powerful model, which learns the physics of
the real world, can easily handle various kinds of motion patterns as prior
knowledge, thus eliminating the need for explicit estimation of optical flows
or motion parameters for pixel alignment. Furthermore, a single instance of the
proposed video diffusion transformer model can adapt to different sampling
conditions without re-training. Empirical results on synthetic and real-world
datasets illustrate the feasibility of diffusion-based, alignment-free video
super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考视频超分辨率：无运动对齐的扩散基础方法</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们通过引入基于扩散后验采样框架的方法，重新思考视频超分辨率的方式，结合在潜在空间中操作的无条件视频扩散变换器。视频生成模型，即扩散变换器，作为一个时空模型。我们认为，一个强大的模型，能够学习现实世界的物理规律，可以轻松处理各种运动模式作为先验知识，从而消除对光流或运动参数进行显式估计以实现像素对齐的需求。此外，所提出的视频扩散变换器模型的单个实例可以在不同的采样条件下适应，而无需重新训练。对合成和真实世界数据集的实证结果表明，基于扩散的无对齐视频超分辨率的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of traditional video super-resolution methods that rely on motion alignment by proposing a novel approach using the Diffusion Posterior Sampling framework alongside an unconditional video diffusion transformer in latent space. The method leverages a space-time model to learn the underlying physics of motion patterns, thereby removing the necessity for explicit optical flow estimation. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of this diffusion-based, alignment-free approach to video super-resolution.</div>
<div class="mono" style="margin-top:8px">本研究通过提出一种新方法，利用扩散后验采样框架和无条件视频扩散变换器在潜在空间中，解决了传统视频超分辨率方法依赖运动对齐的局限性。该方法利用扩散变换器作为时空模型，学习运动模式的基本物理特性，从而消除了对显式光流估计的需求。对合成和真实世界数据集的实验证明了这种无对齐视频超分辨率技术的有效性，展示了其在不同采样条件下无需重新训练的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-modal Diffusion Modelling for Super-resolved Spatial   Transcriptomics</div>
<div class="meta-line">Authors: Xiaofei Wang, Xingxu Huang, Stephen J. Price, Chao Li</div>
<div class="meta-line">First: 2024-04-19T16:01:00+00:00 · Latest: 2025-11-04T11:12:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2404.12973v3">Abs</a> · <a href="http://arxiv.org/pdf/2404.12973v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancement of spatial transcriptomics (ST) allows to characterize
spatial gene expression within tissue for discovery research. However, current
ST platforms suffer from low resolution, hindering in-depth understanding of
spatial gene expression. Super-resolution approaches promise to enhance ST maps
by integrating histology images with gene expressions of profiled tissue spots.
However, current super-resolution methods are limited by restoration
uncertainty and mode collapse. Although diffusion models have shown promise in
capturing complex interactions between multi-modal conditions, it remains a
challenge to integrate histology images and gene expression for super-resolved
ST maps. This paper proposes a cross-modal conditional diffusion model for
super-resolving ST maps with the guidance of histology images. Specifically, we
design a multi-modal disentangling network with cross-modal adaptive modulation
to utilize complementary information from histology images and spatial gene
expression. Moreover, we propose a dynamic cross-attention modelling strategy
to extract hierarchical cell-to-tissue information from histology images.
Lastly, we propose a co-expression-based gene-correlation graph network to
model the co-expression relationship of multiple genes. Experiments show that
our method outperforms other state-of-the-art methods in ST super-resolution on
three public datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨模态扩散建模用于超分辨率空间转录组学</div>
<div class="mono" style="margin-top:8px">最近空间转录组学（ST）的进展使得能够在组织中表征空间基因表达以进行发现研究。然而，当前的ST平台存在分辨率低的问题，阻碍了对空间基因表达的深入理解。超分辨率方法承诺通过将组织学图像与已分析组织点的基因表达相结合来增强ST图谱。然而，当前的超分辨率方法受到恢复不确定性和模式崩溃的限制。尽管扩散模型在捕捉多模态条件之间的复杂交互方面显示出潜力，但将组织学图像与基因表达集成以获得超分辨率ST图谱仍然是一个挑战。本文提出了一种跨模态条件扩散模型，以组织学图像为指导实现ST图谱的超分辨率。具体而言，我们设计了一个多模态解耦网络，采用跨模态自适应调制，以利用组织学图像和空间基因表达的互补信息。此外，我们提出了一种动态跨注意力建模策略，以从组织学图像中提取分层的细胞到组织信息。最后，我们提出了一种基于共表达的基因相关图网络，以建模多个基因的共表达关系。实验表明，我们的方法在三个公共数据集上超分辨率ST方面优于其他最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current spatial transcriptomics (ST) platforms, which suffer from low resolution that impedes a detailed understanding of spatial gene expression. The authors propose a cross-modal conditional diffusion model that integrates histology images with spatial gene expression to enhance ST maps. Key experimental findings indicate that this method, which employs a multi-modal disentangling network and a dynamic cross-attention modeling strategy, significantly outperforms existing state-of-the-art techniques in ST super-resolution across three public datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前空间转录组学（ST）平台的局限性，这些平台存在低分辨率的问题，妨碍了对空间基因表达的深入理解。作者提出了一种跨模态条件扩散模型，该模型将组织学图像与基因表达数据结合，以增强ST图谱。实验结果表明，该方法通过采用多模态解耦网络和动态交叉注意力建模策略，显著优于现有的最先进技术，在三个公共数据集上实现了ST图谱的超分辨率。</div>
</details>
</div>
<div class="card">
<div class="title">KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</div>
<div class="meta-line">Authors: Teerapong Panboonyuen</div>
<div class="meta-line">First: 2025-11-04T10:44:36+00:00 · Latest: 2025-11-04T10:44:36+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02462v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellite image inpainting is a crucial task in remote sensing, where
accurately restoring missing or occluded regions is essential for robust image
analysis. In this paper, we propose KAO, a novel framework that utilizes
Kernel-Adaptive Optimization within diffusion models for satellite image
inpainting. KAO is specifically designed to address the challenges posed by
very high-resolution (VHR) satellite datasets, such as DeepGlobe and the
Massachusetts Roads Dataset. Unlike existing methods that rely on
preconditioned models requiring extensive retraining or postconditioned models
with significant computational overhead, KAO introduces a Latent Space
Conditioning approach, optimizing a compact latent space to achieve efficient
and accurate inpainting. Furthermore, we incorporate Explicit Propagation into
the diffusion process, facilitating forward-backward fusion, which improves the
stability and precision of the method. Experimental results demonstrate that
KAO sets a new benchmark for VHR satellite image restoration, providing a
scalable, high-performance solution that balances the efficiency of
preconditioned models with the flexibility of postconditioned models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAO：卫星图像扩散中的内核自适应优化</div>
<div class="mono" style="margin-top:8px">卫星图像修复是遥感中的一项关键任务，准确恢复缺失或遮挡区域对于稳健的图像分析至关重要。本文提出了KAO，一个新颖的框架，利用内核自适应优化在卫星图像修复的扩散模型中。KAO专门设计用于应对非常高分辨率（VHR）卫星数据集（如DeepGlobe和马萨诸塞州道路数据集）带来的挑战。与依赖于需要大量重新训练的预处理模型或具有显著计算开销的后处理模型的现有方法不同，KAO引入了一种潜在空间条件化方法，优化紧凑的潜在空间以实现高效和准确的修复。此外，我们将显式传播纳入扩散过程，促进前向-后向融合，提高了方法的稳定性和精度。实验结果表明，KAO为VHR卫星图像恢复设定了新的基准，提供了一种可扩展的高性能解决方案，平衡了预处理模型的效率和后处理模型的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance satellite image inpainting, which is vital for effective remote sensing analysis, particularly in dealing with high-resolution datasets. The authors propose KAO, a framework that employs Kernel-Adaptive Optimization within diffusion models to tackle the challenges associated with very high-resolution satellite images. Experimental results indicate that KAO establishes a new benchmark for satellite image restoration, offering a scalable and high-performance solution that effectively combines the efficiency of preconditioned models with the flexibility of postconditioned models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于有效的卫星图像修复，以恢复缺失或遮挡区域，从而改善遥感中的图像分析。作者提出了KAO框架，该框架在扩散模型中采用内核自适应优化，专门解决超高分辨率卫星数据集面临的挑战。关键实验结果表明，KAO在超高分辨率卫星图像修复中设立了新的基准，提供了一种可扩展的高性能解决方案，结合了预处理模型的效率和后处理模型的灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">SatFusion: A Unified Framework for Enhancing Satellite IoT Images via   Multi-Temporal and Multi-Source Data Fusion</div>
<div class="meta-line">Authors: Yufei Tong, Guanjie Cheng, Peihan Wu, Yicheng Zhu, Kexu Lu, Feiyi Chen, Meng Xi, Junqin Huang, Xueqiang Yan, Junfan Wang, Shuiguang Deng</div>
<div class="meta-line">First: 2025-10-09T07:59:37+00:00 · Latest: 2025-11-04T07:20:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.07905v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.07905v2">PDF</a> · <a href="https://github.com/dllgyufei/SatFusion.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid advancement of the digital society, the proliferation of
satellites in the Satellite Internet of Things (Sat-IoT) has led to the
continuous accumulation of large-scale multi-temporal and multi-source images
across diverse application scenarios. However, existing methods fail to fully
exploit the complementary information embedded in both temporal and source
dimensions. For example, Multi-Image Super-Resolution (MISR) enhances
reconstruction quality by leveraging temporal complementarity across multiple
observations, yet the limited fine-grained texture details in input images
constrain its performance. Conversely, pansharpening integrates multi-source
images by injecting high-frequency spatial information from panchromatic data,
but typically relies on pre-interpolated low-resolution inputs and assumes
noise-free alignment, making it highly sensitive to noise and misregistration.
To address these issues, we propose SatFusion: A Unified Framework for
Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion.
Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF)
module to achieve deep feature alignment with the panchromatic image. Then, a
Multi-Source Image Fusion (MSIF) module injects fine-grained texture
information from the panchromatic data. Finally, a Fusion Composition module
adaptively integrates the complementary advantages of both modalities while
dynamically refining spectral consistency, supervised by a weighted combination
of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB,
and GF2 datasets demonstrate that SatFusion significantly improves fusion
quality, robustness under challenging conditions, and generalizability to
real-world Sat-IoT scenarios. The code is available at:
https://github.com/dllgyufei/SatFusion.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SatFusion：通过多时态和多源数据融合增强卫星物联网图像的统一框架</div>
<div class="mono" style="margin-top:8px">随着数字社会的快速发展，卫星物联网（Sat-IoT）中卫星的激增导致在多种应用场景中持续积累大规模的多时态和多源图像。然而，现有方法未能充分利用嵌入在时态和源维度中的互补信息。例如，多图像超分辨率（MISR）通过利用多个观测之间的时态互补性来增强重建质量，但输入图像中有限的细粒度纹理细节限制了其性能。相反，镶嵌技术通过从全色数据中注入高频空间信息来整合多源图像，但通常依赖于预插值的低分辨率输入，并假设无噪声对齐，使其对噪声和误配准高度敏感。为了解决这些问题，我们提出了SatFusion：通过多时态和多源数据融合增强卫星物联网图像的统一框架。具体而言，SatFusion首先采用多时态图像融合（MTIF）模块与全色图像实现深度特征对齐。然后，多源图像融合（MSIF）模块从全色数据中注入细粒度纹理信息。最后，融合组合模块自适应地整合两种模式的互补优势，同时动态优化光谱一致性，由多个损失函数的加权组合进行监督。在WorldStrat、WV3、QB和GF2数据集上的大量实验表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性以及对真实世界Sat-IoT场景的泛化能力。代码可在以下地址获取：https://github.com/dllgyufei/SatFusion.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the quality of satellite IoT images by effectively utilizing the abundant multi-temporal and multi-source data available in various applications. The authors propose a unified framework called SatFusion, which incorporates a Multi-Temporal Image Fusion (MTIF) module for deep feature alignment with panchromatic images, followed by a Multi-Source Image Fusion (MSIF) module that integrates fine-grained texture information. Experimental results on multiple datasets, including WorldStrat and WV3, indicate that SatFusion significantly improves fusion quality, robustness in challenging conditions, and generalizability to real-world satellite IoT scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效利用卫星生成的大量多时相和多源数据来提高卫星物联网图像的质量。作者提出了一种名为SatFusion的统一框架，该框架包括一个多时相图像融合（MTIF）模块，用于与全色图像进行深度特征对齐，随后是一个多源图像融合（MSIF）模块，结合了细粒度的纹理信息。对包括WorldStrat和WV3在内的多个数据集的实验结果表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性以及对现实世界应用的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Transformer meets Multi-level Wavelet Spectrum for Single   Image Super-Resolution</div>
<div class="meta-line">Authors: Peng Du, Hui Li, Han Xu, Paul Barom Jeon, Dongwook Lee, Daehyun Ji, Ran Yang, Feng Zhu</div>
<div class="meta-line">Venue: ICCV 2025 Oral</div>
<div class="meta-line">First: 2025-11-03T02:56:58+00:00 · Latest: 2025-11-04T05:16:07+00:00</div>
<div class="meta-line">Comments: ICCV 2025 Oral Paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01175v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.01175v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform to decompose images into wavelet spectra. A pyramid tokenization
method is proposed which embeds the spectra into a sequence of tokens for
transformer model, facilitating to capture features from both spatial and
frequency domain. A dual-decoder is designed elaborately to handle the distinct
variances in low-frequency and high-frequency sub-bands, without omitting their
alignment in image generation. Extensive experiments on multiple benchmark
datasets demonstrate the effectiveness of our method, with high performance on
both perception quality and fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器与多级小波谱结合用于单幅图像超分辨率</div>
<div class="mono" style="margin-top:8px">离散小波变换（DWT）已被广泛研究以增强图像超分辨率（SR）的性能。尽管一些基于DWT的方法通过捕捉细粒度频率信号来改善SR，但大多数现有方法忽视了多尺度频率子带之间的相互关系，导致重建图像中的不一致性和不自然伪影。为了解决这一挑战，我们提出了一种基于图像小波谱的扩散变换器模型用于SR（DTWSR）。DTWSR结合了扩散模型和变换器的优势，以捕捉多尺度频率子带之间的相互关系，从而生成更一致和真实的SR图像。具体而言，我们使用多级离散小波变换将图像分解为小波谱。我们提出了一种金字塔标记化方法，将谱嵌入到变换器模型的标记序列中，便于从空间和频率域捕捉特征。我们精心设计了一个双解码器，以处理低频和高频子带中的不同方差，而不忽略它们在图像生成中的对齐。对多个基准数据集的广泛实验表明我们的方法有效，在感知质量和保真度上表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve single image super-resolution (SR) by addressing the limitations of existing methods that often overlook the interrelations among multiscale frequency sub-bands, leading to artifacts in reconstructed images. The authors propose a Diffusion Transformer model based on image Wavelet spectra (DTWSR), which utilizes a Multi-level Discrete Wavelet Transform to decompose images and a pyramid tokenization method to embed the spectra into a sequence of tokens for the transformer model. Experimental results on multiple benchmark datasets show that DTWSR significantly enhances both perceptual quality and fidelity of SR images compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法中常常忽视多尺度频率子带之间相互关系的问题，从而改善单幅图像超分辨率（SR），避免重建图像中的伪影。作者提出了一种基于图像小波谱的扩散变换器模型（DTWSR），该模型利用多级离散小波变换对图像进行分解，并采用金字塔标记化方法将小波谱嵌入到变换器模型的标记序列中。对多个基准数据集的实验结果表明，DTWSR在感知质量和保真度方面显著优于传统方法。</div>
</details>
</div>
<div class="card">
<div class="title">High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a   Vision Transformer with Residual Feature Network</div>
<div class="meta-line">Authors: Abuobaida M. Khair, Wenjing Jiang, Yousuf Babiker M. Osman, Wenjun Xia, Xiaopeng Ma</div>
<div class="meta-line">Venue: Biomedical Signal Processing and Control 113 (2026) 108990</div>
<div class="meta-line">First: 2025-11-04T03:03:39+00:00 · Latest: 2025-11-04T03:03:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02212v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents a hybrid deep learning framework, the Vision Transformer
with Residual Feature Network (VRF-Net), for recovering high-resolution system
matrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from
downsampling and coil sensitivity variations. VRF-Net addresses these
challenges by combining transformer-based global attention with residual
convolutional refinement, enabling recovery of both large-scale structures and
fine details. To reflect realistic MPI conditions, the system matrix is
degraded using a dual-stage downsampling strategy. Training employed
paired-image super-resolution on the public Open MPI dataset and a simulated
dataset incorporating variable coil sensitivity profiles. For system matrix
recovery on the Open MPI dataset, VRF-Net achieved nRMSE = 0.403, pSNR = 39.08
dB, and SSIM = 0.835 at 2x scaling, and maintained strong performance even at
challenging scale 8x (pSNR = 31.06 dB, SSIM = 0.717). For the simulated
dataset, VRF-Net achieved nRMSE = 4.44, pSNR = 28.52 dB, and SSIM = 0.771 at 2x
scaling, with stable performance at higher scales. On average, it reduced nRMSE
by 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over
interpolation and CNN-based methods. In image reconstruction of Open MPI
phantoms, VRF-Net further reduced reconstruction error to nRMSE = 1.79 at 2x
scaling, while preserving structural fidelity (pSNR = 41.58 dB, SSIM = 0.960),
outperforming existing methods. These findings demonstrate that VRF-Net enables
sharper, artifact-free system matrix recovery and robust image reconstruction
across multiple scales, offering a promising direction for future in vivo
applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高分辨率磁粒子成像系统矩阵恢复：基于残差特征网络的视觉变换器</div>
<div class="mono" style="margin-top:8px">本研究提出了一种混合深度学习框架，即基于残差特征网络的视觉变换器（VRF-Net），用于恢复磁粒子成像（MPI）中的高分辨率系统矩阵。MPI分辨率常常受到下采样和线圈灵敏度变化的影响。VRF-Net通过结合基于变换器的全局注意力和残差卷积细化来解决这些挑战，使得能够恢复大规模结构和细节。为了反映真实的MPI条件，系统矩阵采用双阶段下采样策略进行降级。训练使用了公共的Open MPI数据集和一个包含可变线圈灵敏度特征的模拟数据集进行配对图像超分辨率。在Open MPI数据集上的系统矩阵恢复中，VRF-Net在2倍缩放时实现了nRMSE = 0.403，pSNR = 39.08 dB，SSIM = 0.835，并且在具有挑战性的8倍缩放下仍保持强劲表现（pSNR = 31.06 dB，SSIM = 0.717）。在模拟数据集上，VRF-Net在2倍缩放时实现了nRMSE = 4.44，pSNR = 28.52 dB，SSIM = 0.771，并在更高缩放下表现稳定。平均而言，它将nRMSE降低了88.2%，pSNR提高了44.7%，SSIM改善了34.3%，优于插值和基于CNN的方法。在Open MPI幻影的图像重建中，VRF-Net进一步将重建误差降低到nRMSE = 1.79（2倍缩放），同时保持结构保真度（pSNR = 41.58 dB，SSIM = 0.960），超越了现有方法。这些发现表明，VRF-Net能够实现更清晰、无伪影的系统矩阵恢复和跨多个尺度的稳健图像重建，为未来的体内应用提供了有希望的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of Magnetic Particle Imaging (MPI) resolution, which is affected by downsampling and coil sensitivity variations. The authors developed a hybrid deep learning framework called Vision Transformer with Residual Feature Network (VRF-Net) to recover high-resolution system matrices. Experimental results showed that VRF-Net achieved significant improvements in recovery metrics, with reductions in normalized root mean square error (nRMSE) by 88.2%, increases in peak signal-to-noise ratio (pSNR) by 44.7%, and enhancements in structural similarity index (SSIM) by 34.3% compared to traditional methods, demonstrating its effectiveness in producing sharper and artifact-free images across various scales.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高磁粒子成像（MPI）的分辨率，而MPI的分辨率常常受到下采样和线圈灵敏度变化的影响。作者开发了一种名为残差特征网络的视觉变换器（VRF-Net）的混合深度学习框架，该框架结合了基于变换器的全局注意力和残差卷积精细化，以恢复高分辨率系统矩阵。实验结果表明，VRF-Net在恢复指标上取得了显著改善，nRMSE减少了88.2%，pSNR增加了44.7%，SSIM提高了34.3%，证明了其在系统矩阵恢复和图像重建方面的有效性，适用于各种尺度。</div>
</details>
</div>
<div class="card">
<div class="title">Locally-Supervised Global Image Restoration</div>
<div class="meta-line">Authors: Benjamin Walder, Daniel Toader, Robert Nuster, Günther Paltauf, Peter Burgholzer, Gregor Langer, Lukas Krainer, Markus Haltmeier</div>
<div class="meta-line">First: 2025-11-03T19:12:25+00:00 · Latest: 2025-11-03T19:12:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01998v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address the problem of image reconstruction from incomplete measurements,
encompassing both upsampling and inpainting, within a learning-based framework.
Conventional supervised approaches require fully sampled ground truth data,
while self-supervised methods allow incomplete ground truth but typically rely
on random sampling that, in expectation, covers the entire image. In contrast,
we consider fixed, deterministic sampling patterns with inherently incomplete
coverage, even in expectation. To overcome this limitation, we exploit multiple
invariances of the underlying image distribution, which theoretically allows us
to achieve the same reconstruction performance as fully supervised approaches.
We validate our method on optical-resolution image upsampling in photoacoustic
microscopy (PAM), demonstrating competitive or superior results while requiring
substantially less ground truth data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部监督的全局图像恢复</div>
<div class="mono" style="margin-top:8px">我们解决了从不完整测量中重建图像的问题，涵盖了上采样和修补，基于学习的框架。传统的监督方法需要完全采样的真实数据，而自监督方法允许不完整的真实数据，但通常依赖于随机采样，期望覆盖整个图像。相反，我们考虑固定的、确定性的采样模式，固有地具有不完整的覆盖，即使在期望中。为了克服这一限制，我们利用了基础图像分布的多重不变性，这在理论上使我们能够实现与完全监督方法相同的重建性能。我们在光学分辨率图像上采样的光声显微镜（PAM）中验证了我们的方法，展示了具有竞争力或更优的结果，同时需要的真实数据显著减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of image reconstruction from incomplete measurements, focusing on both upsampling and inpainting within a learning-based framework. Unlike traditional supervised methods that depend on fully sampled ground truth data, and self-supervised methods that rely on random sampling, this study employs fixed, deterministic sampling patterns that inherently provide incomplete coverage. The findings indicate that by leveraging multiple invariances of the underlying image distribution, the proposed method achieves reconstruction performance comparable to fully supervised approaches, as validated through experiments on optical-resolution image upsampling in photoacoustic microscopy, yielding competitive or superior results with significantly less ground truth data required.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善从不完整测量中进行图像重建的能力，解决传统监督和自监督方法的局限性。作者提出了一种基于学习的框架，利用固定的确定性采样模式来处理图像的不完整覆盖。实验结果表明，他们的方法在光学分辨率图像上采样的光声显微镜中表现出竞争力或优越的性能，同时相比传统方法需要显著更少的真实数据。</div>
</details>
</div>
<div class="card">
<div class="title">RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution   of Rare-Earth Features</div>
<div class="meta-line">Authors: Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</div>
<div class="meta-line">First: 2025-10-27T19:56:43+00:00 · Latest: 2025-11-03T17:58:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23816v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23816v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow&#x27;s core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model&#x27;s
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RareFlow：面向物理的跨传感器稀土特征超分辨率流匹配</div>
<div class="mono" style="margin-top:8px">遥感图像的超分辨率（SR）在分布外（OOD）条件下常常失败，例如由不同传感器捕获的稀有地貌特征，产生视觉上合理但物理上不准确的结果。我们提出了RareFlow，一个旨在提高OOD鲁棒性的面向物理的SR框架。RareFlow的核心是一个双条件架构。门控控制网络从低分辨率输入中保留细粒度的几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上合理的输出，我们引入了一个多方面的损失函数，强制执行与传感器属性的光谱和辐射一致性。此外，该框架通过采用随机前向传播方法量化自身的预测不确定性；结果输出方差直接识别不熟悉的输入，减轻特征幻觉。我们在一个新的、精心策划的多传感器卫星图像基准上验证了RareFlow。在盲评中，地球物理专家将我们模型的输出评为接近真实图像的保真度，显著优于最先进的基线。这种定性优越性通过感知指标的定量提升得到了证实，包括FID减少近40%。RareFlow为数据稀缺的科学领域提供了一个高保真合成的强大框架，并为在严重领域转移下的受控生成提供了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of super-resolution (SR) techniques in remote sensing imagery, particularly under out-of-distribution conditions where rare geomorphic features are captured by various sensors. The authors propose RareFlow, a physics-aware SR framework that employs a dual-conditioning architecture, combining a Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Experimental results demonstrate that RareFlow significantly outperforms existing methods, with geophysical experts rating its outputs as nearly equivalent to ground truth imagery, and achieving a nearly 40% reduction in Fréchet Inception Distance (FID), indicating its effectiveness in producing high-fidelity results in challenging scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决遥感图像超分辨率（SR）技术在分布外条件下的局限性，特别是当通过不同传感器捕获稀有地貌特征时。作者提出了RareFlow，这是一种物理感知的SR框架，采用双重条件架构，结合了用于几何保真度的门控控制网络和用于语义指导的文本提示。实验结果表明，RareFlow显著优于最先进的方法，地球物理专家将其输出评估为几乎等同于真实图像，并在Fréchet Inception Distance（FID）上实现了近40%的减少，表明在数据稀缺场景中感知质量和鲁棒性得到了改善。</div>
</details>
</div>
<div class="card">
<div class="title">AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</div>
<div class="meta-line">Authors: Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</div>
<div class="meta-line">First: 2025-01-26T06:40:30+00:00 · Latest: 2025-11-03T16:38:43+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TASLP 2025. Demopage:
  https://amphionspace.github.io/anyenhance. Open-source implementation:
  https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.15417v3">Abs</a> · <a href="http://arxiv.org/pdf/2501.15417v3">PDF</a> · <a href="https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://amphionspace.github.io/anyenhance">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce AnyEnhance, a unified generative model for voice enhancement
that processes both speech and singing voices. Based on a masked generative
model, AnyEnhance is capable of handling both speech and singing voices,
supporting a wide range of enhancement tasks including denoising,
dereverberation, declipping, super-resolution, and target speaker extraction,
all simultaneously and without fine-tuning. AnyEnhance introduces a
prompt-guidance mechanism for in-context learning, which allows the model to
natively accept a reference speaker&#x27;s timbre. In this way, it could boost
enhancement performance when a reference audio is available and enable the
target speaker extraction task without altering the underlying architecture.
Moreover, we also introduce a self-critic mechanism into the generative process
for masked generative models, yielding higher-quality outputs through iterative
self-assessment and refinement. Extensive experiments on various enhancement
tasks demonstrate AnyEnhance outperforms existing methods in terms of both
objective metrics and subjective listening tests. Demo audios are publicly
available at https://amphionspace.github.io/anyenhance. An open-source
implementation is provided at
https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyEnhance：一种具有提示引导和自我批评的统一生成模型用于语音增强</div>
<div class="mono" style="margin-top:8px">我们介绍了AnyEnhance，这是一种用于语音增强的统一生成模型，能够处理语音和歌声。基于掩蔽生成模型，AnyEnhance能够同时处理语音和歌声，支持包括去噪、去混响、去剪辑、超分辨率和目标说话人提取在内的多种增强任务，且无需微调。AnyEnhance引入了一种提示引导机制用于上下文学习，使模型能够原生接受参考说话人的音色。这样，当有参考音频可用时，可以提升增强性能，并在不改变基础架构的情况下实现目标说话人提取任务。此外，我们还在掩蔽生成模型的生成过程中引入了一种自我批评机制，通过迭代自我评估和改进，产生更高质量的输出。在各种增强任务上的广泛实验表明，AnyEnhance在客观指标和主观听觉测试方面均优于现有方法。演示音频可在https://amphionspace.github.io/anyenhance获取。开源实现可在https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to develop a unified generative model for voice enhancement that can effectively process both speech and singing voices across various enhancement tasks. The method employed is AnyEnhance, a masked generative model that incorporates a prompt-guidance mechanism for in-context learning and a self-critic mechanism for iterative refinement. Experimental results indicate that AnyEnhance significantly outperforms existing methods in both objective metrics and subjective listening tests across multiple enhancement tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的生成模型，以有效处理语音和歌声的增强任务。所采用的方法是AnyEnhance，它结合了掩蔽生成模型、上下文学习的提示引导机制和用于迭代优化的自我评估机制。实验结果表明，AnyEnhance在多个增强任务中在客观指标和主观听感测试中显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive   Reinforcement Learning with IQA Reward</div>
<div class="meta-line">Authors: Xiaogang Xu, Ruihang Chu, Jian Wang, Kun Zhou, Wenjie Shu, Harry Yang, Ser-Nam Lim, Hao Chen, Liang Lin</div>
<div class="meta-line">First: 2025-11-03T14:57:57+00:00 · Latest: 2025-11-03T14:57:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01645v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01645v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth&#x27;s distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过基于困难自适应强化学习与图像质量评估奖励增强扩散基础的恢复模型</div>
<div class="mono" style="margin-top:8px">最近，强化学习（RL）被纳入扩散模型，例如文本到图像等任务。然而，直接将现有的RL方法应用于基于扩散的图像恢复模型并不理想，因为恢复的目标与纯生成的目标根本不同：它更强调保真度。本文研究如何有效地将RL集成到基于扩散的恢复模型中。首先，通过对各种奖励函数的广泛实验，我们发现有效的奖励可以来自图像质量评估（IQA）模型，而不是直观的基于真实值的监督，这在RL之前的监督微调（SFT）阶段已经进行了优化。此外，我们的策略专注于使用RL处理与真实值显著偏离的挑战样本，我们的RL方法创新性地使用基于MLLM的IQA模型来最初对齐与高质量图像的分布。随着样本接近真实值的分布，RL与SFT自适应结合以实现更细粒度的对齐。这个动态过程通过一种自动加权策略来促进，该策略根据训练样本的相对难度进行调整。我们的策略是即插即用的，可以无缝应用于基于扩散的恢复模型，提升其在各种恢复任务中的性能。多个基准的广泛实验证明了我们提出的RL框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance diffusion-based image restoration models by effectively integrating reinforcement learning (RL), as traditional RL methods are not optimal for restoration tasks that prioritize fidelity over generation. The authors propose a novel approach that utilizes an Image Quality Assessment (IQA) model to derive effective rewards, focusing on challenging samples that deviate significantly from the ground truth. Their method combines RL with supervised fine-tuning (SFT) in a dynamic manner, adjusting the weighting based on the difficulty of training samples, which leads to improved performance across various restoration tasks, as demonstrated by extensive experiments on multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效整合强化学习（RL）来提高扩散基础图像恢复模型的性能，因为传统的RL方法不适合优先考虑保真度而非生成的恢复任务。作者提出了一种新方法，利用图像质量评估（IQA）模型来推导有效的奖励，重点关注与真实值显著偏离的挑战样本。实验结果表明，该方法根据训练样本的难度自适应地将RL与监督微调（SFT）结合，增强了各种基准测试中的恢复任务性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251105_1119.html">20251105_1119</a>
<a href="archive/20251104_1119.html">20251104_1119</a>
<a href="archive/20251103_1128.html">20251103_1128</a>
<a href="archive/20251102_1121.html">20251102_1121</a>
<a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
