<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-02 11:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251102_1121</div>
    <div class="row"><div class="card">
<div class="title">ResMatching: Noise-Resilient Computational Super-Resolution via Guided   Conditional Flow Matching</div>
<div class="meta-line">Authors: Anirban Ray, Vera Galinova, Florian Jug</div>
<div class="meta-line">First: 2025-10-30T15:29:20+00:00 · Latest: 2025-10-30T15:29:20+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26601v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResMatching：通过引导条件流匹配实现抗噪声计算超分辨率</div>
<div class="mono" style="margin-top:8px">荧光显微镜中的计算超分辨率（CSR）尽管是一个病态问题，但有着悠久的历史。CSR的核心在于寻找一个先验，用于推断从未被成像显微镜成像的微图像中的频率。随着更好的数据驱动机器学习技术的出现，可以学习到更强的先验，从而使CSR能够获得更好的结果。在这里，我们提出了ResMatching，一种新颖的CSR方法，利用引导条件流匹配来学习改进的数据先验。我们在BioSR数据集上的4种不同生物结构上评估ResMatching，并将其结果与7个基线进行比较。ResMatching始终取得竞争力的结果，在所有情况下都展示了数据保真度与感知现实之间的最佳权衡。我们观察到，使用ResMatching的CSR在强先验难以学习的情况下特别有效，例如当给定的低分辨率图像包含大量噪声时。此外，我们还展示了ResMatching可以用于从隐式学习的后验分布中采样，并且该分布在所有测试用例中都经过校准，使我们的方法能够提供逐像素的数据不确定性项，指导未来用户拒绝不确定的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve computational super-resolution (CSR) in fluorescence microscopy, which is challenged by the ill-posed nature of the problem and the need for effective priors to extrapolate unobserved frequencies in micrographs. The authors introduce ResMatching, a novel CSR method that employs guided conditional flow matching to learn enhanced data-priors. Experimental results demonstrate that ResMatching outperforms seven baseline methods across four diverse biological structures from the BioSR dataset, achieving the best balance between data fidelity and perceptual realism, particularly in scenarios with high noise levels in low-resolution images, and providing a calibrated pixel-wise data-uncertainty term for better prediction reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善荧光显微镜中的计算超分辨率（CSR），该问题受到不适定性质的挑战，需要更好的数据驱动先验。作者提出了ResMatching，这是一种新颖的CSR方法，采用引导条件流匹配来学习增强的数据先验。在BioSR数据集中对四种不同生物结构的实验评估表明，ResMatching始终优于七种基线方法，在数据保真度和感知真实感之间实现了最佳平衡，特别是在低分辨率图像中噪声较高的情况下，并且还提供了经过校准的逐像素数据不确定性项，以指导未来的预测。</div>
</details>
</div>
<div class="card">
<div class="title">LinearSR: Unlocking Linear Attention for Stable and Efficient Image   Super-Resolution</div>
<div class="meta-line">Authors: Xiaohui Li, Shaobin Zhuang, Shuo Cao, Yang Yang, Yuandong Pu, Qi Qin, Siqi Luo, Bin Fu, Yihao Liu</div>
<div class="meta-line">First: 2025-10-09T19:41:51+00:00 · Latest: 2025-10-30T14:46:21+00:00</div>
<div class="meta-line">Comments: 19 pages, 9 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08771v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.08771v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models for Image Super-Resolution (SR) are increasingly powerful,
yet their reliance on self-attention&#x27;s quadratic complexity (O(N^2)) creates a
major computational bottleneck. Linear Attention offers an O(N) solution, but
its promise for photorealistic SR has remained largely untapped, historically
hindered by a cascade of interrelated and previously unsolved challenges. This
paper introduces LinearSR, a holistic framework that, for the first time,
systematically overcomes these critical hurdles. Specifically, we resolve a
fundamental, training instability that causes catastrophic model divergence
using our novel &quot;knee point&quot;-based Early-Stopping Guided Fine-tuning (ESGF)
strategy. Furthermore, we mitigate the classic perception-distortion trade-off
with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we
establish an effective and lightweight guidance paradigm, TAG, derived from our
&quot;precision-over-volume&quot; principle. Our resulting LinearSR model simultaneously
delivers state-of-the-art perceptual quality with exceptional efficiency. Its
core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its
overall multi-step inference time remains highly competitive. This work
provides the first robust methodology for applying Linear Attention in the
photorealistic SR domain, establishing a foundational paradigm for future
research in efficient generative super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinearSR：解锁线性注意力以实现稳定高效的图像超分辨率</div>
<div class="mono" style="margin-top:8px">图像超分辨率（SR）的生成模型越来越强大，但它们对自注意力的二次复杂性（O(N^2)）的依赖造成了主要的计算瓶颈。线性注意力提供了O(N)的解决方案，但其在照片级真实感SR中的潜力仍未得到充分利用，历史上受到一系列相互关联且未解决的挑战的阻碍。本文介绍了LinearSR，这是一个整体框架，首次系统性地克服了这些关键障碍。具体而言，我们通过新颖的“膝点”基础的早停引导微调（ESGF）策略解决了导致灾难性模型发散的基本训练不稳定性。此外，我们通过专门的基于信噪比（SNR）的专家混合（MoE）架构缓解了经典的感知失真权衡。最后，我们建立了一个有效且轻量的引导范式TAG，源自我们的“精度优于体积”原则。我们得到的LinearSR模型同时提供了卓越的感知质量和高效性。其核心扩散前向传播（1-NFE）实现了SOTA级别的速度，而其整体多步推理时间仍然具有高度竞争力。这项工作为在照片级真实感SR领域应用线性注意力提供了首个稳健的方法论，为未来高效生成超分辨率的研究奠定了基础范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the computational bottleneck caused by the quadratic complexity of self-attention in generative models for image super-resolution (SR). The authors introduce LinearSR, a framework that employs a novel Early-Stopping Guided Fine-tuning strategy to resolve training instability and a SNR-based Mixture of Experts architecture to tackle the perception-distortion trade-off. The experimental results demonstrate that LinearSR achieves state-of-the-art perceptual quality and efficiency, with a core diffusion forward pass that delivers competitive speed and a robust methodology for applying Linear Attention in photorealistic SR.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决生成模型在图像超分辨率（SR）中因自注意力的二次复杂性而导致的计算瓶颈。作者提出了LinearSR框架，利用O(N)复杂度的线性注意力克服实现逼真SR的挑战。主要实验结果包括引入早停引导微调策略以稳定训练，基于信噪比的专家混合架构以平衡感知与失真，以及增强效率的轻量级引导范式，从而使LinearSR模型在感知质量上达到最先进水平，并在推理时间上保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and   High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?</div>
<div class="meta-line">Authors: Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-30T10:46:28+00:00 · Latest: 2025-10-30T10:46:28+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures. Includes supplementary material. Under review as
  a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26339v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GLYPH-SR：我们能否通过VLM引导的潜在扩散模型实现高质量图像超分辨率和高保真文本恢复？</div>
<div class="mono" style="margin-top:8px">图像超分辨率（SR）是许多视觉系统的基础——从监控和自主驾驶到文档分析和零售分析——因为恢复高频细节，特别是场景文本，能够实现可靠的下游感知。场景文本，即嵌入自然图像中的文本，如标志、产品标签和店面，通常携带最具可操作性的信息；当字符模糊或幻觉时，即使图像的其余部分看起来清晰，光学字符识别（OCR）和后续决策也会失败。然而，以往的SR研究往往调优于失真（PSNR/SSIM）或学习的感知指标（LIPIS、MANIQA、CLIP-IQA、MUSIQ），这些指标对字符级错误的敏感性较低。此外，解决文本SR的研究通常集中在简化的基准上，孤立字符，忽视了复杂自然场景中文本的挑战。因此，场景文本实际上被视为通用纹理。为了使SR在实际部署中有效，因此必须明确优化文本可读性和感知质量。我们提出GLYPH-SR，一个视觉-语言引导的扩散框架，旨在共同实现这两个目标。GLYPH-SR利用由OCR数据引导的文本SR融合控制网络（TS-ControlNet），以及一个在文本和场景中心指导之间交替的乒乓调度器。为了实现针对性的文本恢复，我们在一个合成语料库上训练这些组件，同时保持主要SR分支不变。在SVT、SCUT-CTW1500和CUTE80的x4和x8上，GLYPH-SR在保持竞争力的MANIQA、CLIP-IQA和MUSIQ的同时，将OCR F1提高了多达+15.18个百分点（SVT x8，OpenOCR）。GLYPH-SR旨在同时满足这两个目标——高可读性和高视觉真实感——提供看起来正确且读取正确的SR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance image super-resolution (SR) for applications that require high-quality text recovery, as traditional methods often neglect the importance of legibility in scene-text. The authors propose GLYPH-SR, a vision-language-guided diffusion framework that integrates a Text-SR Fusion ControlNet (TS-ControlNet) driven by optical character recognition (OCR) data, employing a ping-pong scheduling method to alternate guidance between text and scene elements. Experimental results demonstrate that GLYPH-SR significantly improves OCR F1 scores by up to 15.18 percentage points compared to baseline methods while maintaining competitive performance in perceptual quality metrics across multiple datasets at various scaling factors.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高图像超分辨率（SR），特别是场景文本的恢复，这对监控和文档分析等多种应用至关重要。作者提出了GLYPH-SR，这是一种视觉-语言引导的扩散框架，结合了基于光学字符识别（OCR）数据的文本SR融合控制网络（TS-ControlNet），并采用乒乓调度方法在文本和场景元素之间交替引导。实验结果表明，GLYPH-SR在OCR F1分数上实现了显著提升，较基线方法提高了最多15.18个百分点，同时在感知质量指标如MANIQA、CLIP-IQA和MUSIQ中保持了竞争力的表现。</div>
</details>
</div>
<div class="card">
<div class="title">DOVE: Efficient One-Step Diffusion Model for Real-World Video   Super-Resolution</div>
<div class="meta-line">Authors: Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo, Yulun Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T05:16:45+00:00 · Latest: 2025-10-30T06:40:44+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025. Code is available at:
  https://github.com/zhengchen1999/DOVE</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16239v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16239v2">PDF</a> · <a href="https://github.com/zhengchen1999/DOVE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have demonstrated promising performance in real-world video
super-resolution (VSR). However, the dozens of sampling steps they require,
make inference extremely slow. Sampling acceleration techniques, particularly
single-step, provide a potential solution. Nonetheless, achieving one step in
VSR remains challenging, due to the high training overhead on video data and
stringent fidelity demands. To tackle the above issues, we propose DOVE, an
efficient one-step diffusion model for real-world VSR. DOVE is obtained by
fine-tuning a pretrained video diffusion model (i.e., CogVideoX). To
effectively train DOVE, we introduce the latent-pixel training strategy. The
strategy employs a two-stage scheme to gradually adapt the model to the video
super-resolution task. Meanwhile, we design a video processing pipeline to
construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning
on this dataset further enhances the restoration capability of DOVE. Extensive
experiments show that DOVE exhibits comparable or superior performance to
multi-step diffusion-based VSR methods. It also offers outstanding inference
efficiency, achieving up to a 28$\times$ speed-up over existing methods such as
MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DOVE：高效的一步扩散模型用于真实世界视频超分辨率</div>
<div class="mono" style="margin-top:8px">扩散模型在真实世界视频超分辨率（VSR）中表现出良好的性能。然而，它们所需的数十个采样步骤使得推理极其缓慢。采样加速技术，特别是单步采样，提供了潜在的解决方案。然而，由于视频数据的高训练开销和严格的保真度要求，在VSR中实现一步仍然具有挑战性。为了解决上述问题，我们提出了DOVE，一种高效的一步扩散模型用于真实世界VSR。DOVE是通过微调预训练的视频扩散模型（即CogVideoX）获得的。为了有效训练DOVE，我们引入了潜在像素训练策略。该策略采用两阶段方案逐步使模型适应视频超分辨率任务。同时，我们设计了一个视频处理管道，以构建一个针对VSR量身定制的高质量数据集，称为HQ-VSR。在该数据集上的微调进一步增强了DOVE的恢复能力。大量实验表明，DOVE的性能与基于多步扩散的VSR方法相当或更优。它还提供了出色的推理效率，相较于现有方法如MGLD-VSR，速度提升可达28倍。代码可在：https://github.com/zhengchen1999/DOVE获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of video super-resolution (VSR) using diffusion models, which typically require numerous sampling steps that slow down inference. The authors propose DOVE, an efficient one-step diffusion model, which is fine-tuned from a pretrained video diffusion model called CogVideoX. They introduce a latent-pixel training strategy and a specialized video processing pipeline to create a high-quality dataset for VSR, named HQ-VSR. Experimental results demonstrate that DOVE achieves comparable or superior performance to multi-step diffusion methods while significantly enhancing inference speed, offering up to a 28-fold acceleration over existing techniques like MGLD-VSR.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高扩散模型在视频超分辨率（VSR）中的效率，因为这些模型通常需要多个采样步骤，导致推理速度缓慢。作者提出了DOVE，这是一种高效的一步扩散模型，通过微调预训练的视频扩散模型CogVideoX，并引入了一种潜在像素训练策略，该策略采用两阶段方案以更好地适应VSR任务。实验结果表明，DOVE不仅在性能上与多步扩散基础的VSR方法相当或更优，而且显著提高了推理速度，相比现有技术如MGLD-VSR实现了高达28倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and   Enhanced Motion Compensation</div>
<div class="meta-line">Authors: Wei Shang, Wanying Zhang, Shuhang Gu, Pengfei Zhu, Qinghua Hu, Dongwei Ren</div>
<div class="meta-line">First: 2025-10-30T05:08:45+00:00 · Latest: 2025-10-30T05:08:45+00:00</div>
<div class="meta-line">Comments: 13 pages, 10 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26149v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26149v1">PDF</a> · <a href="https://github.com/shangwei5/BasicAVSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BasicAVSR：通过图像先验和增强运动补偿实现任意尺度视频超分辨率</div>
<div class="mono" style="margin-top:8px">任意尺度视频超分辨率（AVSR）旨在提高视频帧的分辨率，可能在不同的缩放因子下，这带来了空间细节再现、时间一致性和计算复杂性等多个挑战。本文提出了一个强基线BasicAVSR，通过整合四个关键组件：1）从图像拉普拉斯金字塔生成的自适应多尺度频率先验，2）一个流引导传播单元，用于聚合相邻帧的时空信息，3）一个二阶运动补偿单元，以更准确地对齐相邻帧的空间，4）一个超上采样单元，以生成尺度感知和内容无关的上采样核。为了满足多样化的应用需求，我们实例化了三种传播变体：（i）用于严格在线推理的单向RNN单元，（ii）具有有限前瞻的单向RNN单元，允许小的输出延迟，以及（iii）为离线任务设计的双向RNN单元，其中计算资源的限制较小。实验结果证明了我们模型在这些不同场景中的有效性和适应性。通过广泛的实验，我们表明BasicAVSR在超分辨率质量、泛化能力和推理速度方面显著优于现有方法。我们的工作不仅推动了AVSR的最新进展，还将其核心组件扩展到多个框架以适应不同场景。代码可在https://github.com/shangwei5/BasicAVSR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of arbitrary-scale video super-resolution (AVSR), which include enhancing spatial detail, maintaining temporal consistency, and managing computational complexity. The authors propose a baseline model called BasicAVSR, which integrates adaptive multi-scale frequency priors, a flow-guided propagation unit, a second-order motion compensation unit, and a hyper-upsampling unit. Experimental results indicate that BasicAVSR significantly outperforms existing methods in super-resolution quality, generalization ability, and inference speed across various application scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决任意尺度视频超分辨率（AVSR）中的挑战，特别是在增强空间细节、保持时间一致性和管理计算复杂性方面。作者提出了一种名为BasicAVSR的基线模型，集成了自适应多尺度频率先验、流引导传播单元、二阶运动补偿单元和超高采样单元。实验结果表明，BasicAVSR在超分辨率质量、泛化能力和推理速度方面显著优于现有方法，展示了其有效性和适应性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
