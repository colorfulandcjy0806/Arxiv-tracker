<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-07 11:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251107_1117</div>
    <div class="row"><div class="card">
<div class="title">Transformer-Progressive Mamba Network for Lightweight Image   Super-Resolution</div>
<div class="meta-line">Authors: Sichen Guo, Wenjie Li, Yuanyang Liu, Guangwei Gao, Jian Yang, Chia-Wen Lin</div>
<div class="meta-line">First: 2025-11-05T06:46:17+00:00 · Latest: 2025-11-05T06:46:17+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03232v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Mamba-based super-resolution (SR) methods have demonstrated the
ability to capture global receptive fields with linear complexity, addressing
the quadratic computational cost of Transformer-based SR approaches. However,
existing Mamba-based methods lack fine-grained transitions across different
modeling scales, which limits the efficiency of feature representation. In this
paper, we propose T-PMambaSR, a lightweight SR framework that integrates
window-based self-attention with Progressive Mamba. By enabling interactions
among receptive fields of different scales, our method establishes a
fine-grained modeling paradigm that progressively enhances feature
representation with linear complexity. Furthermore, we introduce an Adaptive
High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
during Transformer and Mamba processing. Extensive experiments demonstrate that
T-PMambaSR progressively enhances the model&#x27;s receptive field and
expressiveness, yielding better performance than recent Transformer- or
Mamba-based methods while incurring lower computational cost. Our codes will be
released after acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>轻量级图像超分辨率的变压器-渐进式曼巴网络</div>
<div class="mono" style="margin-top:8px">最近，基于曼巴的超分辨率（SR）方法展示了以线性复杂度捕捉全局感受野的能力，解决了基于变压器的SR方法的二次计算成本。然而，现有的基于曼巴的方法在不同建模尺度之间缺乏细粒度的过渡，限制了特征表示的效率。本文提出了T-PMambaSR，一个轻量级的SR框架，结合了基于窗口的自注意力和渐进式曼巴。通过实现不同尺度感受野之间的交互，我们的方法建立了一个细粒度建模范式，逐步增强特征表示，保持线性复杂度。此外，我们引入了自适应高频细化模块（AHFRM），以恢复在变压器和曼巴处理过程中丢失的高频细节。大量实验表明，T-PMambaSR逐步增强了模型的感受野和表现力，性能优于最近的变压器或曼巴方法，同时计算成本更低。我们的代码将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the efficiency of Mamba-based super-resolution methods, which currently struggle with fine-grained transitions across different modeling scales. The authors propose T-PMambaSR, a lightweight framework that combines window-based self-attention with Progressive Mamba to facilitate interactions among receptive fields of varying scales, thus enhancing feature representation while maintaining linear complexity. Experimental results show that T-PMambaSR significantly improves the model&#x27;s receptive field and expressiveness, outperforming recent Transformer- and Mamba-based methods with a lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高图像超分辨率(SR)方法的效率，特别是解决现有Mamba基础方法在不同建模尺度之间缺乏细粒度过渡的问题。作者提出了T-PMambaSR，这是一种轻量级的SR框架，结合了基于窗口的自注意力和渐进式Mamba，以促进不同尺度感受野之间的交互，从而在保持线性复杂度的同时增强特征表示。实验结果表明，T-PMambaSR显著提高了模型的感受野和表现力，且在计算成本较低的情况下优于近期的Transformer和Mamba基础方法。</div>
</details>
</div>
<div class="card">
<div class="title">Image Super-Resolution with Guarantees via Conformalized Generative   Models</div>
<div class="meta-line">Authors: Eduardo Adame, Daniel Csillag, Guilherme Tegoni Goedert</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-12T13:14:57+00:00 · Latest: 2025-11-04T17:06:44+00:00</div>
<div class="meta-line">Comments: To appear at NeurIPS 2025. 17 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.09664v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.09664v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of generative ML foundation models for image restoration
tasks such as super-resolution calls for robust and interpretable uncertainty
quantification methods. We address this need by presenting a novel approach
based on conformal prediction techniques to create a &#x27;confidence mask&#x27; capable
of reliably and intuitively communicating where the generated image can be
trusted. Our method is adaptable to any black-box generative model, including
those locked behind an opaque API, requires only easily attainable data for
calibration, and is highly customizable via the choice of a local image
similarity metric. We prove strong theoretical guarantees for our method that
span fidelity error control (according to our local image similarity metric),
reconstruction quality, and robustness in the face of data leakage. Finally, we
empirically evaluate these results and establish our method&#x27;s solid
performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过符合化生成模型进行图像超分辨率的保证</div>
<div class="mono" style="margin-top:8px">生成式机器学习基础模型在图像恢复任务（如超分辨率）中的日益使用，呼唤稳健且可解释的不确定性量化方法。我们通过提出一种基于符合预测技术的新方法来满足这一需求，创建一个能够可靠且直观地传达生成图像可信度的“置信掩码”。我们的方法适用于任何黑箱生成模型，包括那些被封闭API锁定的模型，仅需易于获取的数据进行校准，并且可以通过选择局部图像相似性度量进行高度自定义。我们证明了我们的方法在保真度误差控制（根据我们的局部图像相似性度量）、重建质量和面对数据泄漏的稳健性方面具有强大的理论保证。最后，我们对这些结果进行了实证评估，并确立了我们方法的稳固性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is the growing need for reliable uncertainty quantification methods in generative machine learning models used for image restoration tasks like super-resolution. The authors propose a novel approach utilizing conformal prediction techniques to develop a &#x27;confidence mask&#x27; that effectively indicates the trustworthiness of generated images. Experimental results demonstrate that the method provides strong theoretical guarantees regarding fidelity error control, reconstruction quality, and robustness against data leakage, alongside solid empirical performance across various scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于需要在用于图像恢复任务（如超分辨率）的生成机器学习模型中实现稳健且可解释的不确定性量化方法。作者提出了一种新方法，利用符合预测技术开发了一个“置信掩码”，用于指示生成图像的可靠性。该方法适用于任何黑箱生成模型，仅需易于获取的校准数据，并允许通过局部图像相似性度量进行定制，展示了在保真度误差控制、重建质量和抵御数据泄漏方面的强理论保证，并通过实证评估显示出稳固的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal</div>
<div class="meta-line">Authors: Rongxin Liao, Feng Li, Yanyan Wei, Zenglin Shi, Le Zhang, Huihui Bai, Meng Wang</div>
<div class="meta-line">First: 2025-03-12T03:03:06+00:00 · Latest: 2025-11-04T15:59:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.09013v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.09013v2">PDF</a> · <a href="https://github.com/RongxinL/CyclicPrompt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal adverse weather removal (UAWR) seeks to address various weather
degradations within a unified framework. Recent methods are inspired by prompt
learning using pre-trained vision-language models (e.g., CLIP), leveraging
degradation-aware prompts to facilitate weather-free image restoration,
yielding significant improvements. In this work, we propose CyclicPrompt, an
innovative cyclic prompt approach designed to enhance the effectiveness,
adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key
components: 1) a composite context prompt that integrates weather-related
information and context-aware representations into the network to guide
restoration. This prompt differs from previous methods by marrying learnable
input-conditional vectors with weather-specific knowledge, thereby improving
adaptability across various degradations. 2) The erase-and-paste mechanism,
after the initial guided restoration, substitutes weather-specific knowledge
with constrained restoration priors, inducing high-quality weather-free
concepts into the composite prompt to further fine-tune the restoration
process. Therefore, we can form a cyclic &quot;Prompt-Restore-Prompt&quot; pipeline that
adeptly harnesses weather-specific knowledge, textual contexts, and reliable
textures. Extensive experiments on synthetic and real-world datasets validate
the superior performance of CyclicPrompt. The code is available at:
https://github.com/RongxinL/CyclicPrompt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示恢复，恢复提示：用于通用不良天气去除的循环提示</div>
<div class="mono" style="margin-top:8px">通用不良天气去除（UAWR）旨在在统一框架内解决各种天气退化。最近的方法受到使用预训练视觉-语言模型（如CLIP）的提示学习的启发，利用退化感知提示促进无天气图像恢复，取得了显著的改进。在这项工作中，我们提出了CyclicPrompt，一种创新的循环提示方法，旨在增强UAWR的有效性、适应性和通用性。CyclicPrompt包含两个关键组件：1）复合上下文提示，将与天气相关的信息和上下文感知表示集成到网络中以指导恢复。该提示与以前的方法不同，通过将可学习的输入条件向量与特定天气知识结合，从而提高了在各种退化中的适应性。2）擦除和粘贴机制，在初始引导恢复后，用受限恢复先验替代特定天气知识，将高质量的无天气概念引入复合提示，以进一步微调恢复过程。因此，我们可以形成一个循环的“提示-恢复-提示”管道，巧妙地利用特定天气知识、文本上下文和可靠的纹理。在合成和真实世界数据集上的大量实验验证了CyclicPrompt的优越性能。代码可在以下网址获取：https://github.com/RongxinL/CyclicPrompt。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve universal adverse weather removal (UAWR) by leveraging prompt learning techniques from pre-trained vision-language models. The authors propose a novel method called CyclicPrompt, which incorporates a composite context prompt and an erase-and-paste mechanism to enhance the adaptability and effectiveness of weather-free image restoration. Experimental results demonstrate that CyclicPrompt significantly outperforms existing methods on both synthetic and real-world datasets, showcasing its ability to effectively utilize weather-specific knowledge and improve restoration quality.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用最近在预训练视觉-语言模型中的提示学习进展来改善普遍不良天气去除（UAWR）。作者提出了一种名为CyclicPrompt的新方法，该方法包括一个复合上下文提示，将与天气相关的信息整合到网络中，以及一种擦除和粘贴机制，以增强恢复过程。实验结果表明，CyclicPrompt在合成和真实世界数据集上显著优于现有方法，展示了其在去除各种天气退化方面的有效性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Super-Resolution: Towards Diffusion-Based Methods   without Motion Alignment</div>
<div class="meta-line">Authors: Zhihao Zhan, Wang Pang, Xiang Zhu, Yechao Bai</div>
<div class="meta-line">First: 2025-03-05T10:37:51+00:00 · Latest: 2025-11-04T13:48:25+00:00</div>
<div class="meta-line">Comments: ICSPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03355v5">Abs</a> · <a href="http://arxiv.org/pdf/2503.03355v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we rethink the approach to video super-resolution by
introducing a method based on the Diffusion Posterior Sampling framework,
combined with an unconditional video diffusion transformer operating in latent
space. The video generation model, a diffusion transformer, functions as a
space-time model. We argue that a powerful model, which learns the physics of
the real world, can easily handle various kinds of motion patterns as prior
knowledge, thus eliminating the need for explicit estimation of optical flows
or motion parameters for pixel alignment. Furthermore, a single instance of the
proposed video diffusion transformer model can adapt to different sampling
conditions without re-training. Empirical results on synthetic and real-world
datasets illustrate the feasibility of diffusion-based, alignment-free video
super-resolution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考视频超分辨率：朝着无运动对齐的扩散基础方法</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们通过引入基于扩散后验采样框架的方法，重新思考视频超分辨率的方式，结合在潜在空间中操作的无条件视频扩散变换器。视频生成模型，即扩散变换器，作为一个时空模型。我们认为，一个强大的模型，能够学习现实世界的物理规律，可以轻松处理各种运动模式作为先验知识，从而消除对光流或运动参数进行显式估计以实现像素对齐的需求。此外，所提出的视频扩散变换器模型的单个实例可以在不同的采样条件下适应，而无需重新训练。对合成和真实世界数据集的实证结果表明，基于扩散的无对齐视频超分辨率的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of traditional video super-resolution methods that rely on motion alignment by proposing a novel approach using the Diffusion Posterior Sampling framework along with an unconditional video diffusion transformer in latent space. The method leverages a space-time model to learn the physics of real-world motion patterns, thereby removing the need for explicit optical flow estimation. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of this diffusion-based, alignment-free approach to video super-resolution.</div>
<div class="mono" style="margin-top:8px">本研究针对传统视频超分辨率方法依赖运动对齐的局限性，提出了一种基于扩散后验采样框架的新方法。该方法利用在潜在空间中操作的无条件视频扩散变换器，作为一个空间-时间模型，能够学习真实世界运动模式的物理特性。对合成和真实世界数据集的实验结果表明，这种基于扩散的方法在不需要显式运动估计或对齐的情况下，能够有效实现视频超分辨率，并且可以在不同采样条件下无需重新训练而适应。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-modal Diffusion Modelling for Super-resolved Spatial   Transcriptomics</div>
<div class="meta-line">Authors: Xiaofei Wang, Xingxu Huang, Stephen J. Price, Chao Li</div>
<div class="meta-line">First: 2024-04-19T16:01:00+00:00 · Latest: 2025-11-04T11:12:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2404.12973v3">Abs</a> · <a href="http://arxiv.org/pdf/2404.12973v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancement of spatial transcriptomics (ST) allows to characterize
spatial gene expression within tissue for discovery research. However, current
ST platforms suffer from low resolution, hindering in-depth understanding of
spatial gene expression. Super-resolution approaches promise to enhance ST maps
by integrating histology images with gene expressions of profiled tissue spots.
However, current super-resolution methods are limited by restoration
uncertainty and mode collapse. Although diffusion models have shown promise in
capturing complex interactions between multi-modal conditions, it remains a
challenge to integrate histology images and gene expression for super-resolved
ST maps. This paper proposes a cross-modal conditional diffusion model for
super-resolving ST maps with the guidance of histology images. Specifically, we
design a multi-modal disentangling network with cross-modal adaptive modulation
to utilize complementary information from histology images and spatial gene
expression. Moreover, we propose a dynamic cross-attention modelling strategy
to extract hierarchical cell-to-tissue information from histology images.
Lastly, we propose a co-expression-based gene-correlation graph network to
model the co-expression relationship of multiple genes. Experiments show that
our method outperforms other state-of-the-art methods in ST super-resolution on
three public datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨模态扩散建模用于超分辨率空间转录组学</div>
<div class="mono" style="margin-top:8px">最近空间转录组学（ST）的进展使得能够在组织中表征空间基因表达以进行发现研究。然而，当前的ST平台存在分辨率低的问题，阻碍了对空间基因表达的深入理解。超分辨率方法承诺通过将组织学图像与已分析组织点的基因表达相结合来增强ST图谱。然而，当前的超分辨率方法受到恢复不确定性和模式崩溃的限制。尽管扩散模型在捕捉多模态条件之间的复杂交互方面显示出潜力，但将组织学图像与基因表达结合以获得超分辨率ST图谱仍然是一个挑战。本文提出了一种跨模态条件扩散模型，以组织学图像为指导实现ST图谱的超分辨率。具体而言，我们设计了一个多模态解耦网络，采用跨模态自适应调制，以利用组织学图像和空间基因表达的互补信息。此外，我们提出了一种动态跨注意力建模策略，以从组织学图像中提取层次细胞到组织的信息。最后，我们提出了一种基于共表达的基因相关图网络，以建模多个基因的共表达关系。实验表明，我们的方法在三个公共数据集上超分辨率ST方面优于其他最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current spatial transcriptomics (ST) platforms, which suffer from low resolution that impedes a detailed understanding of spatial gene expression. The authors propose a cross-modal conditional diffusion model that integrates histology images with gene expression data to enhance ST maps. Experimental results demonstrate that this method significantly outperforms existing state-of-the-art techniques in super-resolving ST maps across three public datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前空间转录组学（ST）平台的局限性，这些平台由于分辨率低而妨碍了对空间基因表达的全面理解。作者提出了一种跨模态条件扩散模型，将组织学图像与基因表达数据结合，以增强ST图谱。关键实验结果表明，该方法在三个公共数据集上超分辨率ST图谱的表现优于现有的最先进技术，有效捕捉了多模态条件之间的复杂交互，并提高了恢复精度。</div>
</details>
</div>
<div class="card">
<div class="title">KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</div>
<div class="meta-line">Authors: Teerapong Panboonyuen</div>
<div class="meta-line">First: 2025-11-04T10:44:36+00:00 · Latest: 2025-11-04T10:44:36+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02462v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellite image inpainting is a crucial task in remote sensing, where
accurately restoring missing or occluded regions is essential for robust image
analysis. In this paper, we propose KAO, a novel framework that utilizes
Kernel-Adaptive Optimization within diffusion models for satellite image
inpainting. KAO is specifically designed to address the challenges posed by
very high-resolution (VHR) satellite datasets, such as DeepGlobe and the
Massachusetts Roads Dataset. Unlike existing methods that rely on
preconditioned models requiring extensive retraining or postconditioned models
with significant computational overhead, KAO introduces a Latent Space
Conditioning approach, optimizing a compact latent space to achieve efficient
and accurate inpainting. Furthermore, we incorporate Explicit Propagation into
the diffusion process, facilitating forward-backward fusion, which improves the
stability and precision of the method. Experimental results demonstrate that
KAO sets a new benchmark for VHR satellite image restoration, providing a
scalable, high-performance solution that balances the efficiency of
preconditioned models with the flexibility of postconditioned models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAO：卫星图像扩散中的内核自适应优化</div>
<div class="mono" style="margin-top:8px">卫星图像修复是遥感中的一项关键任务，准确恢复缺失或遮挡区域对于稳健的图像分析至关重要。本文提出了KAO，一个新颖的框架，利用内核自适应优化在卫星图像修复的扩散模型中。KAO专门设计用于应对非常高分辨率（VHR）卫星数据集（如DeepGlobe和马萨诸塞州道路数据集）带来的挑战。与依赖于需要大量重新训练的预处理模型或具有显著计算开销的后处理模型的现有方法不同，KAO引入了一种潜在空间条件化方法，优化紧凑的潜在空间以实现高效和准确的修复。此外，我们将显式传播纳入扩散过程中，促进前向-后向融合，提高了方法的稳定性和精度。实验结果表明，KAO为VHR卫星图像恢复设定了新的基准，提供了一种可扩展的高性能解决方案，平衡了预处理模型的效率和后处理模型的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve satellite image inpainting, which is vital for effective remote sensing analysis, particularly for very high-resolution datasets. The authors propose KAO, a framework that employs Kernel-Adaptive Optimization within diffusion models, specifically addressing the limitations of existing methods that require extensive retraining or incur high computational costs. Experimental results indicate that KAO establishes a new benchmark for restoring VHR satellite images, offering a scalable and efficient solution that combines the advantages of both preconditioned and postconditioned models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改善卫星图像修复，这对于有效的遥感分析至关重要，尤其是针对超高分辨率数据集。作者提出了KAO，一个在扩散模型中采用核自适应优化的框架，专门设计用于应对高分辨率卫星图像的挑战。实验结果表明，KAO在超高分辨率卫星图像修复方面建立了新的基准，提供了一种可扩展且高效的解决方案，结合了预处理模型和后处理模型的优点。</div>
</details>
</div>
<div class="card">
<div class="title">SatFusion: A Unified Framework for Enhancing Satellite IoT Images via   Multi-Temporal and Multi-Source Data Fusion</div>
<div class="meta-line">Authors: Yufei Tong, Guanjie Cheng, Peihan Wu, Yicheng Zhu, Kexu Lu, Feiyi Chen, Meng Xi, Junqin Huang, Xueqiang Yan, Junfan Wang, Shuiguang Deng</div>
<div class="meta-line">First: 2025-10-09T07:59:37+00:00 · Latest: 2025-11-04T07:20:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.07905v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.07905v2">PDF</a> · <a href="https://github.com/dllgyufei/SatFusion.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid advancement of the digital society, the proliferation of
satellites in the Satellite Internet of Things (Sat-IoT) has led to the
continuous accumulation of large-scale multi-temporal and multi-source images
across diverse application scenarios. However, existing methods fail to fully
exploit the complementary information embedded in both temporal and source
dimensions. For example, Multi-Image Super-Resolution (MISR) enhances
reconstruction quality by leveraging temporal complementarity across multiple
observations, yet the limited fine-grained texture details in input images
constrain its performance. Conversely, pansharpening integrates multi-source
images by injecting high-frequency spatial information from panchromatic data,
but typically relies on pre-interpolated low-resolution inputs and assumes
noise-free alignment, making it highly sensitive to noise and misregistration.
To address these issues, we propose SatFusion: A Unified Framework for
Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion.
Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF)
module to achieve deep feature alignment with the panchromatic image. Then, a
Multi-Source Image Fusion (MSIF) module injects fine-grained texture
information from the panchromatic data. Finally, a Fusion Composition module
adaptively integrates the complementary advantages of both modalities while
dynamically refining spectral consistency, supervised by a weighted combination
of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB,
and GF2 datasets demonstrate that SatFusion significantly improves fusion
quality, robustness under challenging conditions, and generalizability to
real-world Sat-IoT scenarios. The code is available at:
https://github.com/dllgyufei/SatFusion.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SatFusion：通过多时相和多源数据融合增强卫星物联网图像的统一框架</div>
<div class="mono" style="margin-top:8px">随着数字社会的快速发展，卫星互联网物联网（Sat-IoT）中卫星的激增导致在多种应用场景中持续积累大规模的多时相和多源图像。然而，现有方法未能充分利用嵌入在时间和源维度中的互补信息。例如，多图像超分辨率（MISR）通过利用多个观测之间的时间互补性来提高重建质量，但输入图像中有限的细粒度纹理细节限制了其性能。相反，镶嵌技术通过从全色数据中注入高频空间信息来整合多源图像，但通常依赖于预插值的低分辨率输入，并假设无噪声对齐，使其对噪声和错位高度敏感。为了解决这些问题，我们提出了SatFusion：通过多时相和多源数据融合增强卫星物联网图像的统一框架。具体而言，SatFusion首先采用多时相图像融合（MTIF）模块与全色图像实现深度特征对齐。然后，多源图像融合（MSIF）模块从全色数据中注入细粒度纹理信息。最后，融合组合模块自适应地整合两种模态的互补优势，同时动态优化光谱一致性，由多个损失函数的加权组合进行监督。在WorldStrat、WV3、QB和GF2数据集上的大量实验表明，SatFusion显著提高了融合质量、在挑战条件下的鲁棒性以及对真实世界Sat-IoT场景的泛化能力。代码可在：https://github.com/dllgyufei/SatFusion.git获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the quality of satellite IoT images, which are increasingly generated due to the proliferation of satellites in the digital society, while existing methods do not fully utilize the complementary information from multi-temporal and multi-source data. The authors propose SatFusion, a unified framework that incorporates a Multi-Temporal Image Fusion (MTIF) module for deep feature alignment with panchromatic images, followed by a Multi-Source Image Fusion (MSIF) module that integrates fine-grained texture information. Experimental results on various datasets, including WorldStrat and WV3, show that SatFusion significantly improves fusion quality, robustness in challenging conditions, and generalizability to real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过有效利用多时态和多源数据的互补信息来提高卫星物联网图像的质量，因为现有方法未能充分解决这些维度。作者提出了SatFusion，一个统一框架，包括一个多时态图像融合模块，用于与全色图像进行深度特征对齐，随后是一个多源图像融合模块，结合了细粒度的纹理细节。在多个数据集（包括WorldStrat和WV3）上的实验结果表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性以及对现实世界应用的普适性。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Transformer meets Multi-level Wavelet Spectrum for Single   Image Super-Resolution</div>
<div class="meta-line">Authors: Peng Du, Hui Li, Han Xu, Paul Barom Jeon, Dongwook Lee, Daehyun Ji, Ran Yang, Feng Zhu</div>
<div class="meta-line">Venue: ICCV 2025 Oral</div>
<div class="meta-line">First: 2025-11-03T02:56:58+00:00 · Latest: 2025-11-04T05:16:07+00:00</div>
<div class="meta-line">Comments: ICCV 2025 Oral Paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01175v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.01175v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform to decompose images into wavelet spectra. A pyramid tokenization
method is proposed which embeds the spectra into a sequence of tokens for
transformer model, facilitating to capture features from both spatial and
frequency domain. A dual-decoder is designed elaborately to handle the distinct
variances in low-frequency and high-frequency sub-bands, without omitting their
alignment in image generation. Extensive experiments on multiple benchmark
datasets demonstrate the effectiveness of our method, with high performance on
both perception quality and fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器与多级小波谱结合用于单幅图像超分辨率</div>
<div class="mono" style="margin-top:8px">离散小波变换（DWT）已被广泛研究以增强图像超分辨率（SR）的性能。尽管一些基于DWT的方法通过捕捉细粒度频率信号来改善SR，但大多数现有方法忽视了多尺度频率子带之间的相互关系，导致重建图像中的不一致性和不自然伪影。为了解决这一挑战，我们提出了一种基于图像小波谱的扩散变换器模型用于SR（DTWSR）。DTWSR结合了扩散模型和变换器的优势，以捕捉多尺度频率子带之间的相互关系，从而生成更一致和真实的SR图像。具体而言，我们使用多级离散小波变换将图像分解为小波谱。我们提出了一种金字塔标记化方法，将谱嵌入到变换器模型的标记序列中，便于从空间和频率域捕捉特征。我们精心设计了一个双解码器，以处理低频和高频子带中的不同方差，而不忽略它们在图像生成中的对齐。对多个基准数据集的广泛实验表明我们的方法有效，在感知质量和保真度上表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve single image super-resolution (SR) by addressing the limitations of existing methods that overlook the interrelations among multiscale frequency sub-bands, which can lead to artifacts in reconstructed images. The authors propose a Diffusion Transformer model based on image Wavelet spectra (DTWSR), which utilizes a Multi-level Discrete Wavelet Transform to decompose images and a pyramid tokenization method to embed these spectra into a sequence of tokens for the transformer model. Experimental results on various benchmark datasets show that DTWSR significantly enhances both perceptual quality and fidelity of SR images compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有方法忽视多尺度频率子带之间相互关系的问题，来改善单幅图像超分辨率（SR），这通常会导致重建图像中的伪影。作者提出了一种基于图像小波谱的扩散变换器模型（DTWSR），该模型利用多级离散小波变换对图像进行分解，并采用金字塔标记化方法将这些谱嵌入到变换器模型的标记序列中。多个基准数据集上的实验结果表明，DTWSR在感知质量和保真度方面的性能优于传统方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251106_1123.html">20251106_1123</a>
<a href="archive/20251105_1119.html">20251105_1119</a>
<a href="archive/20251104_1119.html">20251104_1119</a>
<a href="archive/20251103_1128.html">20251103_1128</a>
<a href="archive/20251102_1121.html">20251102_1121</a>
<a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
