<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-04 11:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251104_1119</div>
    <div class="row"><div class="card">
<div class="title">RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution   of Rare-Earth Features</div>
<div class="meta-line">Authors: Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</div>
<div class="meta-line">First: 2025-10-27T19:56:43+00:00 · Latest: 2025-11-03T17:58:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23816v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23816v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow&#x27;s core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model&#x27;s
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RareFlow：面向物理的跨传感器稀土特征超分辨率流匹配</div>
<div class="mono" style="margin-top:8px">遥感图像的超分辨率（SR）在分布外（OOD）条件下常常失败，例如由不同传感器捕获的稀有地貌特征，产生视觉上合理但物理上不准确的结果。我们提出了RareFlow，一个旨在提高OOD鲁棒性的面向物理的SR框架。RareFlow的核心是一个双条件架构。门控控制网络从低分辨率输入中保留细粒度的几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上合理的输出，我们引入了一个多方面的损失函数，强制执行与传感器特性的一致性，包括光谱和辐射一致性。此外，该框架通过采用随机前向传播方法量化自身的预测不确定性；结果输出方差直接识别不熟悉的输入，减轻特征幻觉。我们在一个新的、精心策划的多传感器卫星图像基准上验证了RareFlow。在盲评中，地球物理专家将我们模型的输出评为接近真实图像的保真度，显著优于最先进的基线。这种定性优越性通过感知指标的定量提升得到了证实，包括FID减少近40%。RareFlow为数据稀缺的科学领域提供了一个高保真合成的强大框架，并为在严重领域转移下的受控生成提供了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of super-resolution (SR) in remote sensing imagery, particularly under out-of-distribution conditions where rare geomorphic features are captured by various sensors, leading to inaccurate results. The authors developed RareFlow, a physics-aware SR framework that employs a dual-conditioning architecture, utilizing a Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Experimental results demonstrate that RareFlow significantly outperforms existing methods, with expert evaluations indicating that its outputs closely match ground truth imagery, supported by a nearly 40% reduction in Fréchet Inception Distance (FID), thus providing a robust solution for high-fidelity synthesis in data-scarce scientific applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决遥感图像超分辨率（SR）在分布外条件下的挑战，特别是当各种传感器捕捉到稀有地貌特征时，导致结果不准确。作者提出了RareFlow，这是一种物理感知的SR框架，采用双重条件架构，结合了用于几何保真度的门控控制网络和用于语义指导的文本提示。实验结果表明，RareFlow显著优于现有方法，专家评估表明其输出与真实图像非常接近，并且FID减少近40%，因此为数据稀缺的科学领域提供了一种高保真合成的稳健解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</div>
<div class="meta-line">Authors: Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</div>
<div class="meta-line">First: 2025-01-26T06:40:30+00:00 · Latest: 2025-11-03T16:38:43+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TASLP 2025. Demopage:
  https://amphionspace.github.io/anyenhance. Open-source implementation:
  https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.15417v3">Abs</a> · <a href="http://arxiv.org/pdf/2501.15417v3">PDF</a> · <a href="https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://amphionspace.github.io/anyenhance">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce AnyEnhance, a unified generative model for voice enhancement
that processes both speech and singing voices. Based on a masked generative
model, AnyEnhance is capable of handling both speech and singing voices,
supporting a wide range of enhancement tasks including denoising,
dereverberation, declipping, super-resolution, and target speaker extraction,
all simultaneously and without fine-tuning. AnyEnhance introduces a
prompt-guidance mechanism for in-context learning, which allows the model to
natively accept a reference speaker&#x27;s timbre. In this way, it could boost
enhancement performance when a reference audio is available and enable the
target speaker extraction task without altering the underlying architecture.
Moreover, we also introduce a self-critic mechanism into the generative process
for masked generative models, yielding higher-quality outputs through iterative
self-assessment and refinement. Extensive experiments on various enhancement
tasks demonstrate AnyEnhance outperforms existing methods in terms of both
objective metrics and subjective listening tests. Demo audios are publicly
available at https://amphionspace.github.io/anyenhance. An open-source
implementation is provided at
https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyEnhance：一种具有提示引导和自我批评的统一生成模型用于语音增强</div>
<div class="mono" style="margin-top:8px">我们介绍了AnyEnhance，一种用于语音增强的统一生成模型，能够处理语音和歌声。基于掩蔽生成模型，AnyEnhance能够同时处理语音和歌声，支持包括去噪、去混响、去剪辑、超分辨率和目标说话人提取在内的多种增强任务，且无需微调。AnyEnhance引入了一种提示引导机制用于上下文学习，使模型能够原生接受参考说话人的音色。这样，当有参考音频可用时，可以提升增强性能，并在不改变基础架构的情况下实现目标说话人提取任务。此外，我们还在掩蔽生成模型的生成过程中引入了一种自我批评机制，通过迭代自我评估和优化，产生更高质量的输出。在各种增强任务上的广泛实验表明，AnyEnhance在客观指标和主观听觉测试方面均优于现有方法。演示音频可在https://amphionspace.github.io/anyenhance获取。开源实现可在https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to develop a unified generative model for voice enhancement that can effectively process both speech and singing voices across various enhancement tasks. The authors propose AnyEnhance, which utilizes a masked generative model combined with a prompt-guidance mechanism for in-context learning and a self-critic mechanism for iterative refinement. Experimental results indicate that AnyEnhance significantly outperforms existing methods in both objective metrics and subjective listening tests across multiple enhancement tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的语音增强生成模型，能够有效处理语音和歌声，并适用于多种增强任务。所采用的方法是AnyEnhance，这是一种掩蔽生成模型，结合了上下文学习的提示引导机制和迭代优化的自我评估机制。实验结果表明，AnyEnhance在多个增强任务中在客观指标和主观听觉测试中显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Res-Bench: Benchmarking the Robustness of Multimodal Large Language   Models to Dynamic Resolution Input</div>
<div class="meta-line">Authors: Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, Xiang Wang</div>
<div class="meta-line">First: 2025-10-19T16:53:01+00:00 · Latest: 2025-11-02T13:48:12+00:00</div>
<div class="meta-line">Comments: The authors have discovered a significant error in the paper
  subsequent to submission, and are withdrawing the manuscript for substantial
  correction</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16926v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.16926v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman&#x27;s correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Res-Bench：多模态大型语言模型对动态分辨率输入的鲁棒性基准测试</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）越来越支持动态图像分辨率。然而，目前的评估范式主要评估语义性能，忽视了分辨率鲁棒性这一关键问题——即性能在不同输入分辨率下是否保持稳定。为了解决这一空白，我们引入了\textbf{Res-Bench}，这是一个综合基准，包含12个分辨率级别和6个核心能力维度的14,400个样本。我们设计了一个新的评估框架，超越传统的准确性指标，以捕捉性能稳定性。该框架引入了多个鲁棒性指标：斯皮尔曼相关系数用于评估分辨率-性能趋势，绝对/相对连续误差（ACE/RCE）用于测量性能波动。利用这些指标，我们对领先的MLLMs进行了大规模评估。我们的分析包括：（1）以模型为中心和以任务为中心的鲁棒性检查，（2）对包括填充和超分辨率在内的预处理策略的调查，以及（3）对稳定性增强的微调探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the robustness of Multimodal Large Language Models (MLLMs) to varying input resolutions, an aspect often overlooked in current evaluation paradigms that focus mainly on semantic performance. The authors introduced Res-Bench, a benchmark consisting of 14,400 samples across 12 resolution levels and six core capability dimensions, along with a novel evaluation framework that incorporates multiple robustness metrics such as Spearman&#x27;s correlation and Absolute/Relative Continuous Error (ACE/RCE) to assess performance stability. The findings reveal insights into model-centric and task-centric robustness, the impact of preprocessing strategies like padding and super-resolution, and the potential for fine-tuning to enhance stability across different resolutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估多模态大型语言模型（MLLMs）在不同图像分辨率下的鲁棒性，因为当前的评估主要集中在语义性能上。为了解决这一问题，作者开发了Res-Bench，一个包含14,400个样本、12个分辨率级别和六个核心能力维度的基准，以及一个新的评估框架，该框架包括斯皮尔曼相关性和绝对/相对连续误差（ACE/RCE）等鲁棒性指标。研究结果揭示了模型中心和任务中心的鲁棒性、填充和超分辨率等预处理策略的影响，以及微调在不同分辨率下增强稳定性的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Efficiency and Quality: MoEISR for Arbitrary-Scale Image   Super-Resolution</div>
<div class="meta-line">Authors: Young Jae Oh, Jihun Kim, Jihoon Nam, Tae Hyun Kim</div>
<div class="meta-line">First: 2023-11-20T05:34:36+00:00 · Latest: 2025-11-02T05:26:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2311.12077v2">Abs</a> · <a href="http://arxiv.org/pdf/2311.12077v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arbitrary-scale image super-resolution employing implicit neural functions
has gained significant attention lately due to its capability to upscale images
across diverse scales utilizing only a single model. Nevertheless, these
methodologies have imposed substantial computational demands as they involve
querying every target pixel to a single resource-intensive decoder. In this
paper, we introduce a novel and efficient framework, the Mixture-of-Experts
Implicit Super-Resolution (MoEISR), which enables super-resolution at arbitrary
scales with significantly increased computational efficiency without
sacrificing reconstruction quality. MoEISR dynamically allocates the most
suitable decoding expert to each pixel using a lightweight mapper module,
allowing experts with varying capacities to reconstruct pixels across regions
with diverse complexities. Our experiments demonstrate that MoEISR successfully
reduces significant amount of floating point operations (FLOPs) while
delivering comparable or superior peak signal-to-noise ratio (PSNR).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平衡效率与质量：用于任意尺度图像超分辨率的专家混合隐式超分辨率（MoEISR）</div>
<div class="mono" style="margin-top:8px">采用隐式神经函数的任意尺度图像超分辨率因其能够仅使用单一模型在不同尺度上放大图像而受到广泛关注。然而，这些方法对计算资源的需求很大，因为它们需要对每个目标像素进行查询到一个资源密集型解码器。在本文中，我们提出了一种新颖且高效的框架——专家混合隐式超分辨率（MoEISR），该框架在不牺牲重建质量的情况下，显著提高了任意尺度超分辨率的计算效率。MoEISR通过轻量级映射模块动态分配最合适的解码专家给每个像素，使得具有不同能力的专家能够在复杂性各异的区域重建像素。我们的实验表明，MoEISR成功减少了大量浮点运算（FLOPs），同时提供了可比或更优的峰值信噪比（PSNR）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational demands of arbitrary-scale image super-resolution using implicit neural functions, which typically require querying every target pixel with a resource-intensive decoder. The authors propose a new framework called Mixture-of-Experts Implicit Super-Resolution (MoEISR), which enhances computational efficiency by dynamically allocating the most suitable decoding expert to each pixel through a lightweight mapper module. Experimental results indicate that MoEISR significantly reduces the number of floating point operations (FLOPs) while achieving comparable or superior peak signal-to-noise ratio (PSNR) compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用隐式神经函数进行任意尺度图像超分辨率时高计算需求的问题，这通常需要对每个目标像素进行资源密集型解码器的查询。作者提出了一种新的框架，称为专家混合隐式超分辨率（MoEISR），通过轻量级映射模块动态分配最合适的解码专家到每个像素，从而提高计算效率。实验结果表明，MoEISR显著减少了浮点运算（FLOPs）的数量，同时在峰值信噪比（PSNR）方面达到了与现有方法相当或更优的效果。</div>
</details>
</div>
<div class="card">
<div class="title">A Study in Dataset Distillation for Image Super-Resolution</div>
<div class="meta-line">Authors: Tobias Dietz, Brian B. Moser, Tobias Nauen, Federico Raue, Stanislav Frolov, Andreas Dengel</div>
<div class="meta-line">First: 2025-02-05T22:34:49+00:00 · Latest: 2025-11-01T20:04:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.03656v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.03656v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dataset distillation aims to compress large datasets into compact yet highly
informative subsets that preserve the training behavior of the original data.
While this concept has gained traction in classification, its potential for
image Super-Resolution (SR) remains largely untapped. In this work, we conduct
the first systematic study of dataset distillation for SR, evaluating both
pixel- and latent-space formulations. We show that a distilled dataset,
occupying only 8.88% of the original size, can train SR models that retain
nearly the same reconstruction fidelity as those trained on full datasets.
Furthermore, we analyze how initialization strategies and distillation
objectives affect efficiency, convergence, and visual quality. Our findings
highlight the feasibility of SR dataset distillation and establish foundational
insights for memory- and compute-efficient generative restoration models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像超分辨率的数据集蒸馏研究</div>
<div class="mono" style="margin-top:8px">数据集蒸馏旨在将大型数据集压缩为紧凑且高度信息化的子集，以保留原始数据的训练行为。尽管这一概念在分类中获得了关注，但其在图像超分辨率（SR）中的潜力仍然未被充分挖掘。在本研究中，我们首次系统地研究了SR的数据集蒸馏，评估了像素和潜在空间的公式。我们展示了一个蒸馏数据集，仅占原始大小的8.88%，可以训练出与全数据集训练的模型几乎相同的重建保真度。此外，我们分析了初始化策略和蒸馏目标如何影响效率、收敛性和视觉质量。我们的研究结果突显了SR数据集蒸馏的可行性，并为内存和计算高效的生成恢复模型奠定了基础性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this study is to explore the potential of dataset distillation for image Super-Resolution (SR), an area that has not been extensively researched despite its success in classification tasks. The authors systematically evaluate both pixel- and latent-space formulations of dataset distillation, demonstrating that a distilled dataset comprising only 8.88% of the original size can effectively train SR models with reconstruction fidelity comparable to those trained on full datasets. Additionally, the study investigates the impact of different initialization strategies and distillation objectives on the efficiency, convergence, and visual quality of the models, providing foundational insights for developing memory- and compute-efficient generative restoration models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索数据集蒸馏在图像超分辨率（SR）中的潜力，这是一个尽管在分类任务中取得成功但尚未得到广泛研究的领域。作者对数据集蒸馏方法进行了系统调查，重点关注像素和潜在空间的公式。他们发现，蒸馏数据集仅占原始数据集的8.88%，能够有效训练SR模型，达到与全数据集训练的模型几乎相同的重建保真度，同时还分析了初始化策略和蒸馏目标对效率和视觉质量的影响。</div>
</details>
</div>
<div class="card">
<div class="title">A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image   Super-Resolution with Transformers and TaylorShift</div>
<div class="meta-line">Authors: Sanath Budakegowdanadoddi Nagaraju, Brian Bernhard Moser, Tobias Christian Nauen, Stanislav Frolov, Federico Raue, Andreas Dengel</div>
<div class="meta-line">First: 2024-11-15T14:43:58+00:00 · Latest: 2025-11-01T12:57:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2411.10231v2">Abs</a> · <a href="http://arxiv.org/pdf/2411.10231v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based architectures have recently advanced the image
reconstruction quality of super-resolution (SR) models. Yet, their scalability
remains limited by quadratic attention costs and coarse patch embeddings that
weaken pixel-level fidelity. We propose TaylorIR, a plug-and-play framework
that enforces 1x1 patch embeddings for true pixel-wise reasoning and replaces
conventional self-attention with TaylorShift, a Taylor-series-based attention
mechanism enabling full token interactions with near-linear complexity. Across
multiple SR benchmarks, TaylorIR delivers state-of-the-art performance while
reducing memory consumption by up to 60%, effectively bridging the gap between
fine-grained detail restoration and efficient transformer scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低分辨率图像值1x1个词：利用变换器和TaylorShift实现精细图像超分辨率</div>
<div class="mono" style="margin-top:8px">基于变换器的架构最近提高了超分辨率（SR）模型的图像重建质量。然而，它们的可扩展性仍然受到二次注意力成本和削弱像素级保真度的粗糙补丁嵌入的限制。我们提出了TaylorIR，一个即插即用的框架，强制执行1x1补丁嵌入以实现真正的像素级推理，并用TaylorShift替代传统的自注意力，这是一种基于泰勒级数的注意力机制，能够以接近线性的复杂度实现完整的标记交互。在多个SR基准测试中，TaylorIR提供了最先进的性能，同时将内存消耗减少了多达60%，有效地弥合了细粒度细节恢复与高效变换器扩展之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the scalability and pixel-level fidelity of transformer-based super-resolution models, which are currently limited by quadratic attention costs and coarse patch embeddings. The authors propose a novel framework called TaylorIR that utilizes 1x1 patch embeddings for precise pixel-wise reasoning and introduces a Taylor-series-based attention mechanism, TaylorShift, to facilitate full token interactions with near-linear complexity. Experimental results demonstrate that TaylorIR achieves state-of-the-art performance across various super-resolution benchmarks while reducing memory consumption by up to 60%, effectively enhancing fine-grained detail restoration and efficient transformer scaling.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高基于变换器的超分辨率模型的可扩展性和像素级保真度，而这些模型目前受到二次注意力成本和粗糙的补丁嵌入的限制。作者提出了TaylorIR，一个利用1x1补丁嵌入进行精确像素级推理的框架，并引入了基于泰勒级数的注意力机制TaylorShift，允许近线性复杂度的完整标记交互。实验结果表明，TaylorIR在多个超分辨率基准测试中实现了最先进的性能，同时将内存消耗减少了多达60%。</div>
</details>
</div>
<div class="card">
<div class="title">FIPER: Factorized Features for Robust Image Super-Resolution and   Compression</div>
<div class="meta-line">Authors: Yang-Che Sun, Cheng Yu Yeo, Ernie Chu, Jun-Cheng Chen, Yu-Lun Liu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-10-23T17:59:57+00:00 · Latest: 2025-11-01T10:51:33+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025. Project page: https://jayisaking.github.io/FIPER/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18083v4">Abs</a> · <a href="http://arxiv.org/pdf/2410.18083v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jayisaking.github.io/FIPER/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we propose using a unified representation, termed Factorized
Features, for low-level vision tasks, where we test on Single Image
Super-Resolution (SISR) and \textbf{Image Compression}. Motivated by the shared
principles between these tasks, they require recovering and preserving fine
image details, whether by enhancing resolution for SISR or reconstructing
compressed data for Image Compression. Unlike previous methods that mainly
focus on network architecture, our proposed approach utilizes a
basis-coefficient decomposition as well as an explicit formulation of
frequencies to capture structural components and multi-scale visual features in
images, which addresses the core challenges of both tasks. We replace the
representation of prior models from simple feature maps with Factorized
Features to validate the potential for broad generalizability. In addition, we
further optimize the compression pipeline by leveraging the mergeable-basis
property of our Factorized Features, which consolidates shared structures on
multi-frame compression. Extensive experiments show that our unified
representation delivers state-of-the-art performance, achieving an average
relative improvement of 204.4% in PSNR over the baseline in Super-Resolution
(SR) and 9.35% BD-rate reduction in Image Compression compared to the previous
SOTA. Project page: https://jayisaking.github.io/FIPER/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FIPER：用于鲁棒图像超分辨率和压缩的因子化特征</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们提出使用统一表示，称为因子化特征，针对低级视觉任务进行测试，包括单图像超分辨率（SISR）和图像压缩。受这些任务之间共享原则的启发，它们都需要恢复和保留细致的图像细节，无论是通过增强SISR的分辨率还是重建图像压缩的压缩数据。与主要关注网络架构的先前方法不同，我们提出的方法利用基系数分解以及频率的显式公式来捕捉图像中的结构组件和多尺度视觉特征，从而解决这两项任务的核心挑战。我们用因子化特征替代先前模型的简单特征图表示，以验证其广泛可推广性的潜力。此外，我们通过利用因子化特征的可合并基性质进一步优化压缩管道，从而在多帧压缩中整合共享结构。大量实验表明，我们的统一表示在超分辨率（SR）中相较于基线在PSNR上实现了204.4%的平均相对提升，在图像压缩中相比于之前的SOTA实现了9.35%的BD率降低。项目页面：https://jayisaking.github.io/FIPER/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need for effective methods in low-level vision tasks, specifically Single Image Super-Resolution (SISR) and Image Compression, which both require the recovery and preservation of fine image details. The authors propose a unified representation called Factorized Features, which employs basis-coefficient decomposition and an explicit frequency formulation to capture structural components and multi-scale visual features. Experimental results demonstrate that this approach achieves state-of-the-art performance, with an average relative improvement of 204.4% in PSNR for Super-Resolution and a 9.35% reduction in BD-rate for Image Compression compared to previous methods.</div>
<div class="mono" style="margin-top:8px">本研究通过提出一种称为因子化特征的统一表示，解决了低级视觉任务中的挑战，特别是单幅图像超分辨率（SISR）和图像压缩。该方法采用基底系数分解和显式频率公式，有效捕捉结构组件和多尺度视觉特征，超越了传统以网络架构为中心的方法。实验结果表明，这种统一表示在超分辨率中实现了204.4%的PSNR平均相对提升，并在图像压缩中相比于之前的方法减少了9.35%的BD率，达到了最先进的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251103_1128.html">20251103_1128</a>
<a href="archive/20251102_1121.html">20251102_1121</a>
<a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
