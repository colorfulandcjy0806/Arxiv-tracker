<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-10 11:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251110_1127</div>
    <div class="row"><div class="card">
<div class="title">Sharing the Learned Knowledge-base to Estimate Convolutional Filter   Parameters for Continual Image Restoration</div>
<div class="meta-line">Authors: Aupendu Kar, Krishnendu Ghosh, Prabir Kumar Biswas</div>
<div class="meta-line">First: 2025-11-07T16:52:42+00:00 · Latest: 2025-11-07T16:52:42+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to ACM ICVGIP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.05421v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.05421v1">PDF</a> · <a href="https://github.com/aupendu/continual-restore">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning is an emerging topic in the field of deep learning, where
a model is expected to learn continuously for new upcoming tasks without
forgetting previous experiences. This field has witnessed numerous
advancements, but few works have been attempted in the direction of image
restoration. Handling large image sizes and the divergent nature of various
degradation poses a unique challenge in the restoration domain. However,
existing works require heavily engineered architectural modifications for new
task adaptation, resulting in significant computational overhead.
Regularization-based methods are unsuitable for restoration, as different
restoration challenges require different kinds of feature processing. In this
direction, we propose a simple modification of the convolution layer to adapt
the knowledge from previous restoration tasks without touching the main
backbone architecture. Therefore, it can be seamlessly applied to any deep
architecture without any structural modifications. Unlike other approaches, we
demonstrate that our model can increase the number of trainable parameters
without significantly increasing computational overhead or inference time.
Experimental validation demonstrates that new restoration tasks can be
introduced without compromising the performance of existing tasks. We also show
that performance on new restoration tasks improves by adapting the knowledge
from the knowledge base created by previous restoration tasks. The code is
available at https://github.com/aupendu/continual-restore.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共享学习知识库以估计持续图像恢复的卷积滤波器参数</div>
<div class="mono" style="margin-top:8px">持续学习是深度学习领域的一个新兴主题，模型期望在不忘记先前经验的情况下，持续学习即将到来的新任务。该领域已经取得了许多进展，但在图像恢复方向的工作较少。处理大图像尺寸和各种退化的多样性在恢复领域提出了独特的挑战。然而，现有工作需要对新任务适应进行大量工程化的架构修改，导致显著的计算开销。基于正则化的方法不适合恢复，因为不同的恢复挑战需要不同类型的特征处理。在这方面，我们提出了一种简单的卷积层修改，以适应来自先前恢复任务的知识，而不触及主要的骨干架构。因此，它可以无缝应用于任何深度架构，而无需任何结构修改。与其他方法不同，我们证明我们的模型可以在不显著增加计算开销或推理时间的情况下增加可训练参数的数量。实验验证表明，可以引入新的恢复任务，而不会影响现有任务的性能。我们还展示了通过适应来自先前恢复任务创建的知识库的知识，新恢复任务的性能得到了改善。代码可在 https://github.com/aupendu/continual-restore 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of continual learning in image restoration, where models must adapt to new tasks without forgetting previous experiences while managing large image sizes and diverse degradation types. The authors propose a modification to the convolution layer that allows the model to leverage knowledge from prior restoration tasks without altering the main architecture, thus minimizing computational overhead. Experimental results indicate that the proposed method enables the introduction of new restoration tasks without degrading the performance of existing ones, and it enhances performance on new tasks by utilizing the knowledge base from earlier tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决图像修复中的持续学习挑战，即模型必须在不遗忘先前经验的情况下适应新任务，同时管理大图像尺寸和多样的退化类型。作者提出了一种对卷积层的修改，允许在不改变主架构的情况下整合先前修复任务的知识，从而最小化计算开销。实验结果表明，该模型能够成功地引入新修复任务，同时保持现有任务的性能，并且利用早期任务的知识库可以提高新挑战的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat   Fields</div>
<div class="meta-line">Authors: Alexander Becker, Rodrigo Caye Daudt, Dominik Narnhofer, Torben Peters, Nando Metzger, Jan Dirk Wegner, Konrad Schindler</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2025</div>
<div class="meta-line">First: 2023-11-29T14:01:28+00:00 · Latest: 2025-11-07T15:59:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2311.17643v4">Abs</a> · <a href="http://arxiv.org/pdf/2311.17643v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://therasr.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches to arbitrary-scale single image super-resolution (ASR) use
neural fields to represent continuous signals that can be sampled at arbitrary
resolutions. However, point-wise queries of neural fields do not naturally
match the point spread function (PSF) of pixels, which may cause aliasing in
the super-resolved image. Existing methods attempt to mitigate this by
approximating an integral version of the field at each scaling factor,
compromising both fidelity and generalization. In this work, we introduce
neural heat fields, a novel neural field formulation that inherently models a
physically exact PSF. Our formulation enables analytically correct
anti-aliasing at any desired output resolution, and -- unlike supersampling --
at no additional cost. Building on this foundation, we propose Thera, an
end-to-end ASR method that substantially outperforms existing approaches, while
being more parameter-efficient and offering strong theoretical guarantees. The
project page is at https://therasr.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Thera：无别名任意尺度超分辨率与神经热场</div>
<div class="mono" style="margin-top:8px">最近的任意尺度单图像超分辨率（ASR）方法使用神经场表示可以在任意分辨率下采样的连续信号。然而，神经场的逐点查询与像素的点扩散函数（PSF）不自然匹配，这可能导致超分辨率图像中的别名现象。现有方法试图通过在每个缩放因子下近似场的积分版本来缓解这一问题，但这会妥协保真度和泛化能力。在本研究中，我们引入了神经热场，这是一种新颖的神经场形式，固有地建模了物理上精确的PSF。我们的公式使得在任何期望输出分辨率下实现解析正确的抗别名成为可能，并且与超采样不同，且没有额外成本。在此基础上，我们提出了Thera，一种端到端的ASR方法，显著优于现有方法，同时更具参数效率并提供强有力的理论保证。项目页面为 https://therasr.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the issue of aliasing in arbitrary-scale single image super-resolution (ASR) caused by the mismatch between point-wise queries of neural fields and the point spread function (PSF) of pixels. The authors propose a novel approach called neural heat fields, which accurately models the PSF and allows for analytically correct anti-aliasing at any output resolution without additional computational cost. The experimental results demonstrate that Thera, the proposed ASR method, significantly outperforms existing techniques in terms of fidelity and parameter efficiency while providing strong theoretical guarantees.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决任意尺度单幅图像超分辨率（ASR）中，由于神经场的点查询与像素的点扩散函数（PSF）不匹配而导致的混叠问题。作者提出了一种新的公式，称为神经热场，能够准确建模PSF，并允许在任何输出分辨率下进行解析正确的抗混叠，而无需额外的计算成本。实验结果表明，所提出的ASR方法Thera在保真度和参数效率方面显著优于现有技术，同时提供了强有力的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">Real-World Adverse Weather Image Restoration via Dual-Level   Reinforcement Learning with High-Quality Cold Start</div>
<div class="meta-line">Authors: Fuyang Liu, Jiaqi Xu, Xiaowei Hu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-07T09:22:53+00:00 · Latest: 2025-11-07T09:22:53+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.05095v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.05095v1">PDF</a> · <a href="https://github.com/xxclfy/AgentRL-Real-Weather">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过高质量冷启动的双层强化学习实现真实世界恶劣天气图像恢复</div>
<div class="mono" style="margin-top:8px">恶劣天气严重影响现实世界的视觉感知，而现有的在合成数据上训练的视觉模型由于固定参数难以适应复杂的退化。为了解决这个问题，我们首先构建了HFLS-Weather，一个基于物理驱动的高保真数据集，模拟多种天气现象，然后设计了一个以HFLS-Weather为基础的双层强化学习框架用于冷启动训练。在这个框架中，在局部层面，特定天气的恢复模型通过扰动驱动的图像质量优化进行精细化，使得基于奖励的学习无需配对监督；在全局层面，元控制器根据场景退化动态协调模型选择和执行顺序。该框架能够持续适应现实世界条件，并在广泛的恶劣天气场景中实现了最先进的性能。代码可在https://github.com/xxclfy/AgentRL-Real-Weather获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of visual perception under adverse weather conditions, where existing models fail to generalize due to reliance on synthetic data. To overcome this, the authors developed HFLS-Weather, a high-fidelity dataset that simulates various weather phenomena, and introduced a dual-level reinforcement learning framework for cold-start training. This framework includes local weather-specific restoration models optimized through perturbation-driven learning and a global meta-controller that manages model selection based on scene degradation, resulting in state-of-the-art performance across diverse adverse weather scenarios.</div>
<div class="mono" style="margin-top:8px">本研究解决了恶劣天气条件下视觉感知的挑战，因为现有的基于合成数据训练的模型无法有效泛化。作者开发了一个高保真数据集HFLS-Weather，模拟了各种天气现象，并引入了一个用于冷启动训练的双层强化学习框架。该框架包括通过扰动驱动的图像质量优化来优化局部天气特定的恢复模型，以及一个根据场景退化管理模型选择的全局元控制器，从而在多种恶劣天气场景中实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Consistency Trajectory Matching for One-Step Generative Super-Resolution</div>
<div class="meta-line">Authors: Weiyi You, Mingyang Zhang, Leheng Zhang, Xingyu Zhou, Kexuan Shi, Shuhang Gu</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-03-26T09:20:42+00:00 · Latest: 2025-11-07T06:30:13+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.20349v5">Abs</a> · <a href="http://arxiv.org/pdf/2503.20349v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current diffusion-based super-resolution (SR) approaches achieve commendable
performance at the cost of high inference overhead. Therefore, distillation
techniques are utilized to accelerate the multi-step teacher model into
one-step student model. Nevertheless, these methods significantly raise
training costs and constrain the performance of the student model by the
teacher model. To overcome these tough challenges, we propose Consistency
Trajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy
that is able to generate photo-realistic SR results in one step. Concretely, we
first formulate a Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory to establish a deterministic mapping from low-resolution (LR) images
with noise to high-resolution (HR) images. Then we apply the Consistency
Training (CT) strategy to directly learn the mapping in one step, eliminating
the necessity of pre-trained diffusion model. To further enhance the
performance and better leverage the ground-truth during the training process,
we aim to align the distribution of SR results more closely with that of the
natural images. To this end, we propose to minimize the discrepancy between
their respective PF-ODE trajectories from the LR image distribution by our
meticulously designed Distribution Trajectory Matching (DTM) loss, resulting in
improved realism of our recovered HR images. Comprehensive experimental results
demonstrate that the proposed methods can attain comparable or even superior
capabilities on both synthetic and real datasets while maintaining minimal
inference latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步生成超分辨率的一致性轨迹匹配</div>
<div class="mono" style="margin-top:8px">当前基于扩散的超分辨率(SR)方法在高推理开销的情况下取得了可观的性能。因此，采用蒸馏技术将多步教师模型加速为一步学生模型。然而，这些方法显著提高了训练成本，并限制了学生模型的性能。为克服这些挑战，我们提出了一种无蒸馏策略——超分辨率一致性轨迹匹配(CTMSR)，能够一步生成照片级真实的SR结果。具体而言，我们首先构建一个概率流常微分方程(PF-ODE)轨迹，以建立从带噪声的低分辨率(LR)图像到高分辨率(HR)图像的确定性映射。然后，我们应用一致性训练(CT)策略直接学习该映射，消除预训练扩散模型的必要性。为了进一步提升性能并更好地利用训练过程中的真实数据，我们旨在使SR结果的分布与自然图像的分布更紧密对齐。为此，我们提出通过精心设计的分布轨迹匹配(DTM)损失最小化LR图像分布的PF-ODE轨迹之间的差异，从而提高恢复的HR图像的真实感。全面的实验结果表明，所提方法在合成和真实数据集上均能达到可比甚至更优的能力，同时保持最小的推理延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high inference overhead associated with current diffusion-based super-resolution (SR) methods while also reducing training costs. The authors propose a novel approach called Consistency Trajectory Matching for Super-Resolution (CTMSR), which utilizes a Probability Flow Ordinary Differential Equation (PF-ODE) to create a direct mapping from low-resolution images to high-resolution images without relying on a pre-trained diffusion model. Experimental results show that CTMSR achieves photo-realistic SR results in one step, demonstrating comparable or superior performance on both synthetic and real datasets while significantly reducing inference latency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高超分辨率（SR）技术的效率，目前依赖于扩散模型的技术存在高推理成本。作者提出了一种新方法，称为超分辨率一致性轨迹匹配（CTMSR），通过使用概率流常微分方程（PF-ODE）直接建立低分辨率到高分辨率图像的映射，从而消除了对蒸馏的需求。实验结果表明，CTMSR在一步内实现了逼真的SR效果，在合成和真实数据集上表现出可比或更优的性能，同时显著降低了推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">UHDRes: Ultra-High-Definition Image Restoration via Dual-Domain   Decoupled Spectral Modulation</div>
<div class="meta-line">Authors: S. Zhao, W. Lu, B. Wang, T. Wang, K. Zhang, H. Zhao</div>
<div class="meta-line">First: 2025-11-07T06:28:30+00:00 · Latest: 2025-11-07T06:28:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.05009v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.05009v1">PDF</a> · <a href="https://github.com/Zhao0100/UHDRes">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultra-high-definition (UHD) images often suffer from severe degradations such
as blur, haze, rain, or low-light conditions, which pose significant challenges
for image restoration due to their high resolution and computational demands.
In this paper, we propose UHDRes, a novel lightweight dual-domain decoupled
spectral modulation framework for UHD image restoration. It explicitly models
the amplitude spectrum via lightweight spectrum-domain modulation, while
restoring phase implicitly through spatial-domain refinement. We introduce the
spatio-spectral fusion mechanism, which first employs a multi-scale context
aggregator to extract local and global spatial features, and then performs
spectral modulation in a decoupled manner. It explicitly enhances amplitude
features in the frequency domain while implicitly restoring phase information
through spatial refinement. Additionally, a shared gated feed-forward network
is designed to efficiently promote feature interaction through shared-parameter
convolutions and adaptive gating mechanisms. Extensive experimental comparisons
on five public UHD benchmarks demonstrate that our UHDRes achieves the
state-of-the-art restoration performance with only 400K parameters, while
significantly reducing inference latency and memory usage. The codes and models
are available at https://github.com/Zhao0100/UHDRes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UHDRes：通过双域解耦光谱调制进行超高分辨率图像恢复</div>
<div class="mono" style="margin-top:8px">超高分辨率（UHD）图像常常遭受模糊、雾霾、雨水或低光照条件等严重退化，这对图像恢复提出了重大挑战，因为其高分辨率和计算需求。本文提出了UHDRes，一种新颖的轻量级双域解耦光谱调制框架用于UHD图像恢复。它通过轻量级光谱域调制显式建模幅度光谱，同时通过空间域细化隐式恢复相位。我们引入了时空光谱融合机制，首先采用多尺度上下文聚合器提取局部和全局空间特征，然后以解耦方式进行光谱调制。它在频域中显式增强幅度特征，同时通过空间细化隐式恢复相位信息。此外，设计了一个共享门控前馈网络，通过共享参数卷积和自适应门控机制高效促进特征交互。在五个公共UHD基准上的广泛实验比较表明，我们的UHDRes以仅400K参数实现了最先进的恢复性能，同时显著降低了推理延迟和内存使用。代码和模型可在https://github.com/Zhao0100/UHDRes获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of restoring ultra-high-definition (UHD) images that suffer from various degradations such as blur and low-light conditions, which complicate restoration due to high resolution and computational demands. The authors propose a novel lightweight framework called UHDRes, which utilizes a dual-domain decoupled spectral modulation approach that models amplitude spectrum in the frequency domain while restoring phase information through spatial-domain refinement. Experimental results on five public UHD benchmarks indicate that UHDRes achieves state-of-the-art restoration performance with only 400K parameters, while also significantly reducing inference latency and memory usage.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决超高分辨率（UHD）图像在模糊和低光等各种退化情况下的恢复挑战，这些问题由于高分辨率和计算需求而使恢复过程复杂化。作者提出了一种名为UHDRes的新型轻量级框架，采用双域解耦光谱调制方法，明确建模幅度谱，同时通过空间域细化恢复相位信息。对五个公共UHD基准的实验结果表明，UHDRes以仅400K参数实现了最先进的恢复性能，同时显著降低了推理延迟和内存使用。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251109_1120.html">20251109_1120</a>
<a href="archive/20251108_1110.html">20251108_1110</a>
<a href="archive/20251107_1117.html">20251107_1117</a>
<a href="archive/20251106_1123.html">20251106_1123</a>
<a href="archive/20251105_1119.html">20251105_1119</a>
<a href="archive/20251104_1119.html">20251104_1119</a>
<a href="archive/20251103_1128.html">20251103_1128</a>
<a href="archive/20251102_1121.html">20251102_1121</a>
<a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
