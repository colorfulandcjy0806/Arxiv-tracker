<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-05 11:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251105_1119</div>
    <div class="row"><div class="card">
<div class="title">RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution   of Rare-Earth Features</div>
<div class="meta-line">Authors: Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</div>
<div class="meta-line">First: 2025-10-27T19:56:43+00:00 · Latest: 2025-11-03T17:58:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23816v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23816v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow&#x27;s core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model&#x27;s
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RareFlow：面向物理的跨传感器稀土特征超分辨率流匹配</div>
<div class="mono" style="margin-top:8px">遥感图像的超分辨率（SR）在分布外（OOD）条件下常常失败，例如由不同传感器捕获的稀有地貌特征，产生视觉上合理但物理上不准确的结果。我们提出了RareFlow，一个旨在提高OOD鲁棒性的面向物理的SR框架。RareFlow的核心是一个双条件架构。门控控制网络从低分辨率输入中保留细粒度的几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上合理的输出，我们引入了一个多方面的损失函数，强制执行与传感器属性的光谱和辐射一致性。此外，该框架通过采用随机前向传播方法量化自身的预测不确定性；结果输出方差直接识别不熟悉的输入，减轻特征幻觉。我们在一个新的、精心策划的多传感器卫星图像基准上验证了RareFlow。在盲评中，地球物理专家将我们模型的输出评为接近真实图像的保真度，显著优于最先进的基线。这种定性优越性通过感知指标的定量提升得到了证实，包括FID减少近40%。RareFlow为数据稀缺的科学领域提供了一个高保真合成的强大框架，并为在严重领域转移下的受控生成提供了新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of super-resolution (SR) in remote sensing imagery, particularly under out-of-distribution conditions where rare geomorphic features are captured by various sensors. The authors developed RareFlow, a physics-aware SR framework that employs a dual-conditioning architecture, combining a Gated ControlNet for geometric fidelity and textual prompts for semantic guidance. Experimental results demonstrate that RareFlow significantly outperforms state-of-the-art methods, with expert evaluations indicating that its outputs closely match ground truth imagery, supported by a nearly 40% reduction in FID, thus providing a robust solution for high-fidelity synthesis in data-scarce scientific fields.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决遥感图像超分辨率（SR）在分布外条件下的挑战，特别是当通过各种传感器捕获稀有地貌特征时，导致结果不准确。作者开发了RareFlow，这是一种物理感知的SR框架，采用双重条件架构，利用门控控制网络保持几何保真度，并通过文本提示提供语义指导。实验结果表明，RareFlow显著优于最先进的方法，专家评估表明其输出与真实图像非常接近，FID减少近40%，从而建立了一种在数据稀缺环境中实现高保真合成的稳健方法。</div>
</details>
</div>
<div class="card">
<div class="title">AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</div>
<div class="meta-line">Authors: Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</div>
<div class="meta-line">First: 2025-01-26T06:40:30+00:00 · Latest: 2025-11-03T16:38:43+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TASLP 2025. Demopage:
  https://amphionspace.github.io/anyenhance. Open-source implementation:
  https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.15417v3">Abs</a> · <a href="http://arxiv.org/pdf/2501.15417v3">PDF</a> · <a href="https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://amphionspace.github.io/anyenhance">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce AnyEnhance, a unified generative model for voice enhancement
that processes both speech and singing voices. Based on a masked generative
model, AnyEnhance is capable of handling both speech and singing voices,
supporting a wide range of enhancement tasks including denoising,
dereverberation, declipping, super-resolution, and target speaker extraction,
all simultaneously and without fine-tuning. AnyEnhance introduces a
prompt-guidance mechanism for in-context learning, which allows the model to
natively accept a reference speaker&#x27;s timbre. In this way, it could boost
enhancement performance when a reference audio is available and enable the
target speaker extraction task without altering the underlying architecture.
Moreover, we also introduce a self-critic mechanism into the generative process
for masked generative models, yielding higher-quality outputs through iterative
self-assessment and refinement. Extensive experiments on various enhancement
tasks demonstrate AnyEnhance outperforms existing methods in terms of both
objective metrics and subjective listening tests. Demo audios are publicly
available at https://amphionspace.github.io/anyenhance. An open-source
implementation is provided at
https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyEnhance：一种具有提示引导和自我批评的统一生成模型用于语音增强</div>
<div class="mono" style="margin-top:8px">我们介绍了AnyEnhance，这是一种用于语音增强的统一生成模型，能够处理语音和歌声。基于掩蔽生成模型，AnyEnhance能够同时处理语音和歌声，支持包括去噪、去混响、去剪辑、超分辨率和目标说话人提取在内的多种增强任务，且无需微调。AnyEnhance引入了一种提示引导机制用于上下文学习，使模型能够原生接受参考说话人的音色。这样，当有参考音频可用时，可以提升增强性能，并在不改变基础架构的情况下实现目标说话人提取任务。此外，我们还在掩蔽生成模型的生成过程中引入了一种自我批评机制，通过迭代自我评估和改进，产生更高质量的输出。在各种增强任务上的广泛实验表明，AnyEnhance在客观指标和主观听感测试方面均优于现有方法。演示音频可在https://amphionspace.github.io/anyenhance获取。开源实现可在https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the research is to develop a unified generative model for voice enhancement that can effectively process both speech and singing voices across various enhancement tasks. The method employed is AnyEnhance, a masked generative model that integrates a prompt-guidance mechanism for in-context learning and a self-critic mechanism for iterative refinement. Experimental results indicate that AnyEnhance significantly outperforms existing methods in both objective metrics and subjective listening tests across multiple enhancement tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的生成模型，用于有效处理语音和歌声的增强任务。所采用的方法是AnyEnhance，这是一种掩蔽生成模型，结合了上下文学习的提示引导机制和迭代优化的自我批评机制。实验结果表明，AnyEnhance在多个增强任务中在客观指标和主观听觉测试方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Res-Bench: Benchmarking the Robustness of Multimodal Large Language   Models to Dynamic Resolution Input</div>
<div class="meta-line">Authors: Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, Xiang Wang</div>
<div class="meta-line">First: 2025-10-19T16:53:01+00:00 · Latest: 2025-11-02T13:48:12+00:00</div>
<div class="meta-line">Comments: The authors have discovered a significant error in the paper
  subsequent to submission, and are withdrawing the manuscript for substantial
  correction</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16926v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.16926v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman&#x27;s correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Res-Bench：多模态大型语言模型对动态分辨率输入的鲁棒性基准测试</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）越来越支持动态图像分辨率。然而，目前的评估范式主要评估语义性能，忽视了分辨率鲁棒性这一关键问题——即性能在不同输入分辨率下是否保持稳定。为了解决这一空白，我们引入了\textbf{Res-Bench}，这是一个综合基准，包含12个分辨率级别和6个核心能力维度的14,400个样本。我们设计了一个新的评估框架，超越传统的准确性指标，以捕捉性能稳定性。该框架引入了多个鲁棒性指标：斯皮尔曼相关系数用于评估分辨率-性能趋势，绝对/相对连续误差（ACE/RCE）用于测量性能波动。利用这些指标，我们对领先的MLLMs进行了大规模评估。我们的分析包括：（1）以模型为中心和以任务为中心的鲁棒性检查，（2）对包括填充和超分辨率在内的预处理策略的调查，以及（3）对稳定性增强的微调探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the robustness of Multimodal Large Language Models (MLLMs) to varying image resolutions, an aspect often overlooked in current evaluation paradigms that focus primarily on semantic performance. The authors introduce Res-Bench, a benchmark consisting of 14,400 samples across 12 resolution levels and six core capability dimensions, along with a novel evaluation framework that incorporates multiple robustness metrics such as Spearman&#x27;s correlation and Absolute/Relative Continuous Error (ACE/RCE) to assess performance stability. The findings reveal insights into model-centric and task-centric robustness, the impact of preprocessing strategies like padding and super-resolution, and the potential for fine-tuning to enhance stability across different resolutions.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估多模态大型语言模型（MLLMs）对不同输入分辨率的鲁棒性，这是当前主要关注语义性能的评估范式中常被忽视的一个方面。作者引入了Res-Bench，一个包含14,400个样本、12个分辨率级别和六个核心能力维度的基准，以及一个新颖的评估框架，该框架结合了多个鲁棒性指标，如斯皮尔曼相关系数和绝对/相对连续误差（ACE/RCE），以评估性能稳定性。大规模评估的主要发现揭示了模型中心和任务中心的鲁棒性、填充和超分辨率等预处理策略的影响，以及微调在增强稳定性方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Balancing Efficiency and Quality: MoEISR for Arbitrary-Scale Image   Super-Resolution</div>
<div class="meta-line">Authors: Young Jae Oh, Jihun Kim, Jihoon Nam, Tae Hyun Kim</div>
<div class="meta-line">First: 2023-11-20T05:34:36+00:00 · Latest: 2025-11-02T05:26:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2311.12077v2">Abs</a> · <a href="http://arxiv.org/pdf/2311.12077v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arbitrary-scale image super-resolution employing implicit neural functions
has gained significant attention lately due to its capability to upscale images
across diverse scales utilizing only a single model. Nevertheless, these
methodologies have imposed substantial computational demands as they involve
querying every target pixel to a single resource-intensive decoder. In this
paper, we introduce a novel and efficient framework, the Mixture-of-Experts
Implicit Super-Resolution (MoEISR), which enables super-resolution at arbitrary
scales with significantly increased computational efficiency without
sacrificing reconstruction quality. MoEISR dynamically allocates the most
suitable decoding expert to each pixel using a lightweight mapper module,
allowing experts with varying capacities to reconstruct pixels across regions
with diverse complexities. Our experiments demonstrate that MoEISR successfully
reduces significant amount of floating point operations (FLOPs) while
delivering comparable or superior peak signal-to-noise ratio (PSNR).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平衡效率与质量：用于任意尺度图像超分辨率的专家混合隐式方法</div>
<div class="mono" style="margin-top:8px">采用隐式神经函数的任意尺度图像超分辨率最近受到广泛关注，因为它能够仅使用单一模型在不同尺度上放大图像。然而，这些方法对计算资源的需求很大，因为它们需要对每个目标像素进行查询到一个资源密集型解码器。本文介绍了一种新颖且高效的框架——专家混合隐式超分辨率（MoEISR），该框架在不牺牲重建质量的情况下，显著提高了任意尺度超分辨率的计算效率。MoEISR通过轻量级映射模块动态分配最合适的解码专家给每个像素，使得具有不同能力的专家能够在复杂性各异的区域重建像素。我们的实验表明，MoEISR成功减少了大量浮点运算（FLOPs），同时提供了可比或更优的峰值信噪比（PSNR）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the high computational demands of arbitrary-scale image super-resolution using implicit neural functions, which typically require querying every pixel with a single decoder. The authors propose a new framework called Mixture-of-Experts Implicit Super-Resolution (MoEISR), which enhances computational efficiency by dynamically assigning the most appropriate decoding expert to each pixel through a lightweight mapper module. Experimental results indicate that MoEISR significantly reduces the number of floating point operations (FLOPs) while achieving comparable or superior peak signal-to-noise ratio (PSNR) compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决使用隐式神经函数进行任意尺度图像超分辨率时的高计算需求，这通常需要对每个目标像素进行资源密集型解码器查询。作者提出了一种新的框架，称为专家混合隐式超分辨率（MoEISR），通过轻量级映射模块动态分配最合适的解码专家到每个像素，从而提高计算效率。实验结果表明，MoEISR显著减少了浮点运算（FLOPs）的数量，同时在峰值信噪比（PSNR）方面达到了与现有方法相当或更优的效果。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251104_1119.html">20251104_1119</a>
<a href="archive/20251103_1128.html">20251103_1128</a>
<a href="archive/20251102_1121.html">20251102_1121</a>
<a href="archive/20251101_1119.html">20251101_1119</a>
<a href="archive/20251031_1137.html">20251031_1137</a>
<a href="archive/20251031_1118.html">20251031_1118</a>
<a href="archive/20251030_1121.html">20251030_1121</a>
<a href="archive/20251029_1124.html">20251029_1124</a>
<a href="archive/20251029_1024.html">20251029_1024</a>
<a href="archive/20251028_2136.html">20251028_2136</a>
<a href="archive/20251028_2059.html">20251028_2059</a>
<a href="archive/20251028_2029.html">20251028_2029</a>
<a href="archive/20251028_1955.html">20251028_1955</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
