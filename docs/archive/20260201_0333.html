<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-01 03:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260201_0333</div>
    <div class="row"><div class="card">
<div class="title">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</div>
<div class="meta-line">Authors: Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-29T18:59:24+00:00 · Latest: 2026-01-29T18:59:24+00:00</div>
<div class="meta-line">Comments: 26 pages, 31 figures, 13 tables. Project Page: https://sites.google.com/view/vi-probe/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vi-probe/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) often answer classic visual illusions &quot;correctly&quot; on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLMs 是感知还是回忆？经典视觉错觉探究视觉感知与记忆</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）在原始图像上对经典视觉错觉通常会给出正确的回答，但在错觉因素反转后仍坚持相同的回答，尽管人类可以明显察觉视觉变化。这引发了一个基本问题：VLMs 是感知视觉变化还是仅仅回忆已记忆的模式？尽管已有几项研究注意到了这一现象，但其背后的成因仍不清楚。为了从观察转向系统理解，本文引入了VI-Probe，这是一种可控的视觉错觉框架，具有分级扰动和匹配的视觉对照（无错觉诱导器），以解开基于视觉的感知与语言驱动的回忆之间的关系。不同于以往工作主要关注平均准确率，我们使用极性反转一致性、模板固定指数和与匹配对照归一化的错觉乘数来衡量稳定性和敏感性。不同家族的实验表明，反应持久性源自多种原因而非单一机制。例如，GPT-5 表现出记忆覆盖，Claude-Opus-4.1 显示感知与记忆的竞争，而 Qwen 变体则表明视觉处理的限制。我们的发现挑战了单一成因的观点，并促使基于探针的评估，以衡量知识和对受控视觉变化的敏感性。数据和代码可在 https://sites.google.com/view/vi-probe/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether large vision-language models (VLMs) perceive visual changes or merely recall memorized patterns by using a controllable visual-illusion framework called VI-Probe. The study measures stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier. The experiments across different VLMs reveal that response persistence arises from various causes, challenging the notion of a single mechanism. The findings suggest that probing-based evaluation is necessary to measure both knowledge and sensitivity to controlled visual change.</div>
<div class="mono" style="margin-top:8px">该研究通过使用可控视觉错觉框架VI-Probe，探讨大型视觉语言模型（VLMs）是感知视觉变化还是仅回忆记忆模式。实验表明，VLMs响应持久的原因是异质的，包括记忆覆盖、感知-记忆竞争和视觉处理限制。该研究挑战了一因论，并强调需要进行基于探测的评估，以同时衡量模型对受控视觉变化的知识和敏感性。</div>
</details>
</div>
<div class="card">
<div class="title">SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</div>
<div class="meta-line">Authors: Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi</div>
<div class="meta-line">First: 2026-01-29T18:41:52+00:00 · Latest: 2026-01-29T18:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22114v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SINA：使用人工智能的电路原理图图像到网表生成器</div>
<div class="mono" style="margin-top:8px">当前将电路原理图图像转换为机器可读网表的方法在组件识别和连接推理方面存在困难。在本文中，我们介绍了SINA，这是一个开源的全自动电路原理图图像到网表生成器。SINA结合了深度学习进行准确的组件检测、连通组件标记（CCL）进行精确的连接提取、光学字符识别（OCR）进行组件参考标识符检索，并使用视觉语言模型（VLM）进行可靠的参考标识符分配。在我们的实验中，SINA的整体网表生成准确率为96.47%，比最先进的方法高出2.72倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of converting circuit schematic images into machine-readable netlists, particularly in component recognition and connectivity inference. SINA, an open-source system, uses deep learning for component detection, CCL for connectivity extraction, OCR for reference designator retrieval, and a VLM for reliable reference designator assignments. Experiments show SINA achieves 96.47% overall netlist-generation accuracy, surpassing state-of-the-art methods by 2.72 times.</div>
<div class="mono" style="margin-top:8px">SINA 是一种自动化的电路图图像到网表生成器，它利用深度学习进行组件检测、CCL 进行连接提取、OCR 进行参考标识符检索，以及 VLM 进行可靠的参考标识符分配。它的整体网表生成准确率为 96.47%，比现有方法高出 2.72 倍。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering</div>
<div class="meta-line">Authors: Dongxuan Zhu, Ly Tran Ho Khanh, Andy Yat-Ming Cheung, Man-Chung Yue, Viet Anh Nguyen</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T17:17:04+00:00 · Latest: 2026-01-29T17:17:04+00:00</div>
<div class="meta-line">Comments: 34 pages, 2 figures. Accepted for publication at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22010v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过推断时斯蒂费尔激活引导探索多样的生成路径</div>
<div class="mono" style="margin-top:8px">语言模型通常默认生成一组窄范围的高概率输出，导致生成路径同质化且容易发生模式崩溃。基于采样的策略虽然引入了随机性，但在多个并发生成运行中仍难以保证多样性。我们通过引入STARS（基于斯蒂费尔的激活引导以促进多样推理）来解决这一限制，这是一种无需训练的、在推断时进行干预的方法，将激活引导转化为探索引擎。在每个标记处，STARS 收集并发生成运行的隐藏激活，并在斯蒂费尔流形上联合优化多个附加的引导方向。STARS 最大化引导激活的几何体积，而斯蒂费尔流形诱导引导干预的正交性。这种表述明确地促进了并发生成运行的发散激活向量，并隐式地促进了发散的生成轨迹。这种流形优化表述可以通过黎曼梯度下降算法求解，该算法具有收敛保证，但该算法对于实时推断来说耗时过长。为了保证低延迟，我们进一步设计了一种轻量级的一步更新，采用激进的闭式步长。在测试案例生成和科学发现基准测试中，STARS 一致地优于标准采样方法，在不牺牲质量性能的情况下实现了更高的多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of homogeneity in language model outputs by introducing STARS, a method that optimizes activation steering at inference time. STARS uses the Stiefel manifold to ensure orthogonality of steering interventions, promoting divergent generation trajectories. Experiments show that STARS outperforms standard sampling methods in terms of diversity while maintaining qualitative performance.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决模式塌陷问题来增强语言模型输出的多样性。STARS 是一种无需训练的方法，在推理时引导激活向量，以促进多样化的生成路径。它在 Stiefel 流形上优化多个引导方向，确保正交性并最大化引导激活向量的几何体积。实验表明，STARS 在生成测试案例和科学发现方面优于标准采样方法，且不牺牲质量。</div>
</details>
</div>
<div class="card">
<div class="title">Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</div>
<div class="meta-line">Authors: Konstantinos P. Panousis, Diego Marcos</div>
<div class="meta-line">First: 2026-01-29T16:28:55+00:00 · Latest: 2026-01-29T16:28:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21944v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to &quot;induce interpretability&quot;. In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>清晰度：稀疏感知概念瓶颈模型中的灵活性-可解释性权衡</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在各领域的广泛应用加剧了人们对模型可解释性的担忧。令人不安的是，这些模型往往被视为黑箱，对其决策过程的研究极为有限或不存在。尽管存在众多事后和事前的可解释性方法，但对学习表示的系统性和客观性评估仍然有限，特别是对于那些越来越被认为“诱导可解释性”的稀疏感知方法。在本文中，我们专注于概念瓶颈模型，并探讨不同的建模决策如何影响生成的表示。我们引入了清晰度的概念，这是一个衡量指标，捕捉下游性能与概念表示的稀疏性和精度之间的相互作用，同时提出了一种使用具有真实概念注释的数据集进行可解释性评估的框架。我们考虑了基于VLM和属性预测器的概念瓶颈模型（CBMs），以及三种不同的稀疏诱导策略：每例$\ell_1$、$\ell_0$和伯努利形式。我们的实验揭示了在相似性能水平下灵活性和可解释性之间的关键权衡，其中给定的方法可能会表现出截然不同的行为。代码将在发表后公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the interpretability issue in Vision-Language Models (VLMs) by focusing on Concept Bottleneck Models. It introduces a measure called clarity to evaluate the balance between downstream performance and the sparsity and precision of concept representations. The study considers both VLM- and attribute predictor-based CBMs with three sparsity-inducing strategies and finds a trade-off between flexibility and interpretability, where different methods can behave differently even when performance is similar.</div>
<div class="mono" style="margin-top:8px">该研究关注Vision-Language Models (VLMs)的可解释性问题，通过聚焦Concept Bottleneck Models来解决。引入了一个称为清晰度的度量，用于评估下游性能与概念表示的稀疏性和精度之间的平衡。研究考虑了基于VLM和属性预测器的CBMs，并使用三种稀疏性诱导策略，发现灵活性和可解释性之间存在权衡，即使性能相似，不同方法的行为也可能不同。</div>
</details>
</div>
<div class="card">
<div class="title">A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances</div>
<div class="meta-line">Authors: Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel</div>
<div class="meta-line">First: 2025-05-23T12:18:34+00:00 · Latest: 2026-01-29T16:22:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17799v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17799v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>coreset 选择的coreset 选择文献综述：介绍与最新进展</div>
<div class="mono" style="margin-top:8px">coreset 选择旨在解决如何从大型数据集中找到一个小型、具有代表性的子集，以保留关键模式，从而有效进行机器学习的问题。尽管已有几篇综述研究了数据缩减策略，但大多数综述主要集中在经典几何方法或主动学习技术上。相比之下，本文综述提供了一个更全面的观点，通过将训练无监督、训练导向和标签无监督三种主要的coreset研究方法统一到一个分类体系中。我们介绍了现有工作中经常忽略的子领域，包括子模形式、 bilevel 优化以及无标签数据集中的伪标签进展。此外，我们探讨了剪枝策略如何影响泛化能力和神经网络的标度法则，提供了先前综述中未提及的新见解。最后，我们在不同的计算、鲁棒性和性能需求下比较了这些方法，并指出了未来研究中需要解决的开放挑战，如鲁棒性、异常值过滤以及将coreset选择适应基础模型等问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey aims to provide a comprehensive view of coreset selection, which involves selecting a small, representative subset of a large dataset to preserve essential patterns for machine learning. It unifies three major lines of research: training-free, training-oriented, and label-free approaches. The survey covers submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets, and examines how pruning strategies influence generalization and neural scaling laws. Key findings include new insights into how these methods perform under varying computational, robustness, and performance demands, and highlight open challenges such as robustness and outlier filtering for future research.</div>
<div class="mono" style="margin-top:8px">本文旨在提供一种全面的聚核选择方法概述，这些方法涉及从大型数据集中选择一个小的、具有代表性的子集以保留关键模式，从而支持机器学习。作者将三种主要的研究方向——无训练、有训练和无标签方法——统一到一个分类体系中。他们还探讨了子模形式、 bilevel优化以及无标签数据集的伪标签等子领域。关键发现包括剪枝策略如何影响泛化和神经网络缩放定律的见解，以及在各种计算、鲁棒性和性能需求下的方法比较，指出了未来研究中需要解决的开放挑战。</div>
</details>
</div>
<div class="card">
<div class="title">FreeFuse: Multi-Subject LoRA Fusion via Adaptive Token-Level Routing at Test Time</div>
<div class="meta-line">Authors: Yaoli Liu, Yao-Xiang Ding, Kun Zhou</div>
<div class="meta-line">First: 2025-10-27T16:54:08+00:00 · Latest: 2026-01-29T16:14:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23515v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23515v2">PDF</a> · <a href="https://github.com/yaoliliu/FreeFuse">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes FreeFuse, a training-free framework for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to prior studies that focus on retraining LoRA to alleviate feature conflicts, our analysis reveals that simply spatially confining the subject LoRA&#x27;s output to its target region and preventing other LoRAs from directly intruding into this area is sufficient for effective mitigation. Accordingly, we implement Adaptive Token-Level Routing during the inference phase. We introduce FreeFuseAttn, a mechanism that exploits the flow matching model&#x27;s intrinsic semantic alignment to dynamically match subject-specific tokens to their corresponding spatial regions at early denoising timesteps, thereby bypassing the need for external segmentors. FreeFuse distinguishes itself through high practicality: it necessitates no additional training, model modifications, or user-defined masks spatial conditions. Users need only provide subject activation words to achieve seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both identity preservation and compositional fidelity. Our code is available at https://github.com/yaoliliu/FreeFuse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeFuse：通过自适应令牌级路由在测试时进行多主题LoRA融合的无训练框架</div>
<div class="mono" style="margin-top:8px">本文提出FreeFuse，一种无需训练的多主题文本到图像生成框架，通过自动融合多个主题LoRA实现。与以往专注于重新训练LoRA以缓解特征冲突的研究不同，我们的分析表明，简单地将主题LoRA的输出空间限制在其目标区域，并防止其他LoRA直接侵入该区域就足以有效缓解冲突。因此，在推理阶段我们实现了自适应令牌级路由。我们引入了FreeFuseAttn机制，该机制利用流匹配模型固有的语义对齐，在早期去噪时间步动态匹配主题特定的令牌到其相应的空间区域，从而绕过了对外部分割器的需求。FreeFuse通过其高实用性脱颖而出：它不需要额外的训练、模型修改或用户定义的空间条件。用户只需提供主题激活词即可无缝集成到标准工作流程中。广泛的实验验证了FreeFuse在身份保留和组成保真度方面优于现有方法。我们的代码可在https://github.com/yaoliliu/FreeFuse获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FreeFuse is a training-free framework for multi-subject text-to-image generation that automatically fuses multiple subject LoRAs. It uses Adaptive Token-Level Routing during inference to spatially confine each subject&#x27;s output to its target region, preventing other LoRAs from intruding. Experiments show that FreeFuse outperforms existing methods in preserving identity and compositional fidelity without requiring additional training or user-defined masks. Users only need to provide subject activation words.</div>
<div class="mono" style="margin-top:8px">FreeFuse 是一个无需训练的多主题文本到图像生成框架，自动融合多个主题 LoRA。它通过将每个主题 LoRA 的输出限制在其目标区域内，并防止其他 LoRA 侵入来避免重新训练。FreeFuseAttn 机制在去噪早期动态匹配主题特定的标记到相应的空间区域，无需额外训练或用户定义的遮罩。实验表明，FreeFuse 在保留身份和组成保真度方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</div>
<div class="meta-line">Authors: Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</div>
<div class="meta-line">First: 2026-01-29T16:14:07+00:00 · Latest: 2026-01-29T16:14:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21922v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用视频扩散模型辅助的零样本视频恢复与增强</div>
<div class="mono" style="margin-top:8px">尽管基于扩散的零样本图像恢复和增强方法已经取得了巨大成功，但将其应用于视频恢复或增强会导致严重的时域闪烁。本文提出了一种新的框架，利用快速发展的视频扩散模型辅助基于图像的方法，以保持更佳的时域一致性。我们提出了同构潜在融合、异构潜在融合以及基于COT的融合比例策略，利用同构和异构文本到视频扩散模型来补充图像方法。此外，我们提出了时域增强后处理，利用图像到视频扩散模型进一步提高时域一致性。该方法无需训练，可以应用于任何基于扩散的图像恢复和增强方法。实验结果表明了所提出方法的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the issue of temporal flickering in zero-shot video restoration and enhancement using diffusion models. The authors propose a framework that leverages video diffusion models to assist image-based methods, ensuring better temporal consistency. Key techniques include homologous and heterogenous latents fusion, and a COT-based fusion ratio strategy. Additionally, temporal-strengthening post-processing is introduced to enhance temporal consistency further. The method is training-free and can be applied to any diffusion-based image restoration and enhancement methods, showing superior performance in experiments.</div>
<div class="mono" style="margin-top:8px">论文解决了使用扩散模型进行零样本视频修复和增强时出现的严重时间闪烁问题。提出了一种框架，利用视频扩散模型来提高时间一致性。方法包括同源和异源潜变量融合，以及基于COT的融合比例策略，并采用时间增强后处理。该方法无需训练，可以应用于任何基于扩散的图像修复和增强方法。实验结果表明，该方法在保持时间一致性方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</div>
<div class="meta-line">Authors: Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</div>
<div class="meta-line">First: 2026-01-29T15:49:31+00:00 · Latest: 2026-01-29T15:49:31+00:00</div>
<div class="meta-line">Comments: 24 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21892v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过流匹配中的流形投影改进无分类器引导</div>
<div class="mono" style="margin-top:8px">无分类器引导（CFG）是一种广泛用于扩散和基于流模型可控生成的技术。尽管在实践中取得了成功，但CFG依赖于敏感于引导尺度的启发式线性外推。在本文中，我们通过优化的角度为CFG提供了一个原理性的解释。我们证明流匹配中的速度场对应于一系列平滑距离函数的梯度，这引导潜在变量向缩放的目标图像集移动。这种视角揭示了标准的CFG公式是该梯度的近似，其中预测差距，即条件输出与无条件输出之间的差异，决定了引导的敏感性。利用这一洞察，我们将CFG采样重新表述为具有流形约束的同伦优化。这种表述需要一个流形投影步骤，我们在采样过程中通过增量梯度下降方案实现。为了提高计算效率和稳定性，我们进一步通过Anderson加速改进了这一迭代过程，而无需额外的模型评估。我们提出的方法是训练免费的，并且一致地提高了生成保真度、提示对齐和对引导尺度的鲁棒性。我们在多种基准上验证了其有效性，展示了在DiT-XL-2-256、Flux和Stable Diffusion 3.5等大型模型上取得了显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of classifier-free guidance (CFG) in diffusion and flow-based models by providing a principled interpretation through optimization. It reformulates CFG as a homotopy optimization with a manifold constraint, requiring a manifold projection step implemented via incremental gradient descent. The authors enhance this process with Anderson Acceleration for improved efficiency and stability. The proposed methods consistently improve generation fidelity, prompt alignment, and robustness to the guidance scale across various benchmarks, including DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</div>
<div class="mono" style="margin-top:8px">本文旨在通过优化方法对流基模型中的无分类器引导（CFG）进行改进，通过优化提供了一个原理性的解释。作者将CFG重新表述为具有流形约束的同伦优化，并通过增量梯度下降方案实现流形投影。为了提高计算效率和稳定性，他们应用了Anderson加速。所提出的方法能够一致地提高生成保真度、提示对齐和对引导比例的鲁棒性，在大型模型如DiT-XL-2-256、Flux和Stable Diffusion 3.5上显示出显著的改进效果。</div>
</details>
</div>
<div class="card">
<div class="title">Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents</div>
<div class="meta-line">Authors: Taewon Kang</div>
<div class="meta-line">First: 2026-01-29T15:28:48+00:00 · Latest: 2026-01-29T15:28:48+00:00</div>
<div class="meta-line">Comments: 47 pages, 36 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21857v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>轨迹引导扩散在多层文档中保留前景背景生成</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于扩散的文档中心背景生成框架，通过潜在空间设计而非显式约束实现前景保留和多页风格一致性。我们的方法重新解释了扩散，将其视为在结构化潜在空间中随机轨迹的演变。通过塑造初始噪声及其几何对齐，背景生成自然地避开指定的前景区域，使可读内容保持完整，无需辅助机制。为了解决跨页风格漂移的长期问题，我们将风格控制与文本条件分离，并引入缓存的风格方向作为潜在空间中的持久向量。一旦选定，这些方向将约束扩散轨迹到共享的风格子空间，确保跨页和编辑迭代的一致外观。这种表述消除了重复提示式风格指定的需要，并为多页生成提供了更稳定的基础。我们的框架具有几何和物理解释，其中扩散路径在由偏好方向塑造的潜在流形上演变，由于轨迹初始化而不是明确排除，前景区域很少被穿越。所提出的方法无需训练，与现有的扩散骨干兼容，并在复杂文档中生成视觉上连贯、保留前景的结果。通过将扩散重新构想为潜在空间中的轨迹设计，我们提供了一种原理性的方法来实现一致和结构化的生成建模。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints.</div>
<div class="mono" style="margin-top:8px">论文提出了一种基于扩散的框架，用于在多层文档中生成背景同时保留前景。该方法通过潜空间设计避免指定的前景区域，并引入缓存的风格方向以确保跨页面的一致性。该方法不需要显式的约束或辅助机制，能够在无需重复风格指定的情况下提供视觉上连贯的结果。</div>
</details>
</div>
<div class="card">
<div class="title">MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</div>
<div class="meta-line">Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu</div>
<div class="meta-line">First: 2026-01-29T15:07:28+00:00 · Latest: 2026-01-29T15:07:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a &quot;less is more&quot; phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMFineReason：通过开放数据为中心的方法缩小多模态推理差距</div>
<div class="mono" style="margin-top:8px">近期视觉语言模型（VLMs）的进展在视觉推理方面取得了显著进步。然而，开源VLMs仍落后于专有系统，主要原因是缺乏高质量的推理数据。现有数据集在涵盖诸如STEM图表和视觉谜题等具有挑战性的领域方面覆盖面有限，且缺乏用于激发强大推理能力的一致且长形式的推理链（CoT）注释。为弥合这一差距，我们引入了MMFineReason，这是一个包含180万样本和51亿个解决方案标记的大规模多模态推理数据集，这些标记是从Qwen3-VL-235B-A22B-Thinking中提炼出的高质量推理注释。该数据集通过系统性的三阶段管道建立：（1）大规模数据收集和标准化，（2）生成推理链（CoT）理由，（3）基于推理质量和难度意识的全面筛选。该数据集涵盖了STEM问题、视觉谜题、游戏和复杂图表，每个样本都标注了视觉支持的推理痕迹。我们对MMFineReason进行微调Qwen3-VL-Instruct，开发了MMFineReason-2B/4B/8B版本。我们的模型在相应规模类别中建立了新的最佳结果。值得注意的是，MMFineReason-4B成功超越了Qwen3-VL-8B-Thinking，而MMFineReason-8B甚至超过了Qwen3-VL-30B-A3B-Thinking，接近Qwen3-VL-32B-Thinking，展示了显著的参数效率。通过我们的难度意识筛选策略，我们发现了一个“少即是多”的现象：仅7%（12.3万样本）的子集就能达到与完整数据集相当的性能。此外，我们揭示了推理导向的数据组合具有协同效应，同时提升了通用能力。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-based Few-Shot Early Rumor Detection with Imitation Agent</div>
<div class="meta-line">Authors: Fengzhu Zeng, Qian Shao, Ling Cheng, Wei Gao, Shih-Fen Cheng, Jing Ma, Cheng Niu</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-12-20T12:42:27+00:00 · Latest: 2026-01-29T15:01:08+00:00</div>
<div class="meta-line">Comments: Accepted at KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18352v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.18352v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \textit{early time point determination}, while the LLM serves as a powerful \textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM的少样本早期谣言检测与模仿代理</div>
<div class="mono" style="margin-top:8px">早期谣言检测（EARD）旨在根据一系列社交媒体帖子，识别出一个声明可以被准确分类的最早时间点。这在数据稀缺的环境中尤其具有挑战性。尽管大型语言模型（LLMs）在少样本NLP任务中表现出色，但它们不适用于时间序列数据，并且在训练和推理时计算成本高昂。在本文中，我们提出了一种新颖的EARD框架，该框架结合了一个自主代理和一个基于LLM的检测模型，其中代理作为可靠的决策者负责确定\textit{早期时间点}，而LLM则作为强大的\textit{谣言检测器}。这种方法提供了第一个少样本EARD的解决方案，只需要训练一个轻量级的代理，而LLM则无需训练。在四个真实世界数据集上的广泛实验表明，我们的方法在LLMs上提升了性能，并在准确性和及时性方面超越了现有的EARD方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of early rumor detection in data-scarce settings by proposing a novel framework that integrates an autonomous agent and a Large Language Model (LLM). The agent determines the earliest time point for accurate classification, while the LLM detects rumors. This approach requires only training a lightweight agent and does not involve training the LLM, leading to improved performance across different LLMs and surpassing existing methods in both accuracy and timeliness on four real-world datasets.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种新颖的框架，将自主代理与大型语言模型（LLM）结合，以解决数据稀缺环境下的早期谣言检测挑战。代理确定最早的时间点进行准确分类，而LLM负责检测谣言。这种方法仅需训练一个轻量级代理，而不涉及训练LLM，从而在四个真实世界数据集上实现了更高的性能，并在准确性和及时性方面超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US</div>
<div class="meta-line">Authors: Seongchan Park, Jaehong Kim, Hyeonseung Kim, Heejin Bin, Sue Moon, Wonjae Lee</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2026-01-29T14:58:54+00:00 · Latest: 2026-01-29T14:58:54+00:00</div>
<div class="meta-line">Comments: Accepted at The Web Conference 2026. We release Korean and English multimodal moral emotion classifiers</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21815v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21815v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform&#x27;s multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>道德愤怒塑造超越注意力的承诺：韩国和美国YouTube上的多模态道德情绪</div>
<div class="mono" style="margin-top:8px">在注意力经济中，理解媒体修辞如何影响受众参与至关重要。本研究探讨了主流新闻频道在YouTube上以道德情感框架呈现内容如何影响韩国和美国用户的在线行为。为了捕捉平台的多模态特性，结合缩略图图像和视频标题，我们通过微调视觉语言模型开发了一个多模态道德情绪分类器。该模型在两种语言的人标注多模态数据集上进行训练，并应用于来自主要新闻机构的约40万条视频。我们分析了包括观看次数、点赞和评论在内的参与度水平，代表了不同程度的承诺。结果显示，其他谴责性道德愤怒表达，批评他人道德，无论在哪个文化中，都能一致地增加所有形式的参与度，从被动观看到积极评论。这些发现表明，道德愤怒是一种特别有效的心理策略，不仅能吸引注意力，还能激发积极的参与。我们讨论了其他谴责性修辞可能被滥用的问题，因为这种做法可能会通过强化群体内部和群体外部的分化来加深分歧。为了促进未来研究并确保可重复性，我们公开发布了韩语和英语的多模态道德情绪分类器。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</div>
<div class="meta-line">Authors: Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe</div>
<div class="meta-line">First: 2026-01-29T14:41:01+00:00 · Latest: 2026-01-29T14:41:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model&#x27;s output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识向量削弱：大型视觉-语言模型的高效无训练卸载方法</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）因其强大的多模态能力而被广泛采用，但它们引发了严重的隐私泄露和有害内容生成等问题。机器卸载已作为去除训练模型中特定数据影响的一种有前景的解决方案出现。然而，现有方法大多依赖于基于梯度的优化，对大规模LVLMs来说会带来巨大的计算成本。为解决这一局限，我们提出了一种名为知识向量削弱（KVW）的无训练卸载方法，该方法直接干预整个模型而不进行梯度计算。KVW 识别出在模型生成忘记集输出时被激活的知识向量，并逐步削弱它们的贡献，从而防止模型利用不希望的知识。在MLLMU和CLEAR基准上的实验表明，KVW 在稳定遗忘-保留权衡的同时，显著提高了计算效率，优于基于梯度和LoRA的卸载方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of privacy leakage and harmful content generation in large vision-language models (LVLMs) by proposing Knowledge Vector Weakening (KVW), a training-free unlearning method. KVW directly intervenes in the model without requiring gradient computation, identifying and weakening knowledge vectors activated during the model&#x27;s output generation on the forget set. Experiments show that KVW achieves a stable balance between forgetting and retaining information while being more computationally efficient than gradient-based and LoRA-based methods.</div>
<div class="mono" style="margin-top:8px">论文提出了一种训练-free 的遗忘方法 Knowledge Vector Weakening (KVW)，以解决大型视觉-语言模型 (LVLM) 中的隐私泄露和有害内容生成问题。KVW 不需要梯度计算，直接干预模型，在输出生成忘记集上的知识向量时进行识别和削弱，从而保持信息的遗忘和保留之间的稳定平衡，并且在计算效率上优于基于梯度和 LoRA 的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Error Amplification Limits ANN-to-SNN Conversion in Continuous Control</div>
<div class="meta-line">Authors: Zijie Xu, Zihan Huang, Yiting Dong, Kang Chen, Wenxuan Liu, Zhaofei Yu</div>
<div class="meta-line">First: 2026-01-29T14:28:00+00:00 · Latest: 2026-01-29T14:28:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) can achieve competitive performance by converting already existing well-trained Artificial Neural Networks (ANNs), avoiding further costly training. This property is particularly attractive in Reinforcement Learning (RL), where training through environment interaction is expensive and potentially unsafe. However, existing conversion methods perform poorly in continuous control, where suitable baselines are largely absent. We identify error amplification as the key cause: small action approximation errors become temporally correlated across decision steps, inducing cumulative state distribution shift and severe performance degradation. To address this issue, we propose Cross-Step Residual Potential Initialization (CRPI), a lightweight training-free mechanism that carries over residual membrane potentials across decision steps to suppress temporally correlated errors. Experiments on continuous control benchmarks with both vector and visual observations demonstrate that CRPI can be integrated into existing conversion pipelines and substantially recovers lost performance. Our results highlight continuous control as a critical and challenging benchmark for ANN-to-SNN conversion, where small errors can be strongly amplified and impact performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>误差放大会限制ANN到SNN转换在连续控制中的应用</div>
<div class="mono" style="margin-top:8px">通过将已经训练好的人工神经网络（ANN）转换为脉冲神经网络（SNN），SNN可以在不进行额外训练的情况下实现竞争性的性能，这在强化学习（RL）中尤其具有吸引力，因为环境交互训练既昂贵又可能不安全。然而，现有的转换方法在连续控制中表现不佳，因为缺乏合适的基线。我们发现误差放大会是主要原因：小的动作近似误差会在决策步骤之间变得时序相关，导致状态分布的累积变化和严重的性能下降。为了解决这个问题，我们提出了跨步骤残差膜电位初始化（CRPI），这是一种轻量级的无需训练机制，可以在决策步骤之间传递残差膜电位以抑制时序相关误差。在具有向量和视觉观察的连续控制基准测试中，CRPI可以集成到现有的转换管道中，并显著恢复了丢失的性能。我们的结果强调了连续控制是ANN到SNN转换的关键和具有挑战性的基准，其中小的误差可以被强烈放大并影响性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the poor performance of Spiking Neural Networks (SNNs) converted from Artificial Neural Networks (ANNs) in continuous control tasks, where small action errors can be amplified over time. The study introduces Cross-Step Residual Potential Initialization (CRPI), a lightweight mechanism that suppresses temporally correlated errors by carrying over residual membrane potentials across decision steps. Experiments show that CRPI can integrate into existing conversion pipelines and significantly improve performance in continuous control benchmarks with both vector and visual observations.</div>
<div class="mono" style="margin-top:8px">研究旨在解决从人工神经网络（ANN）转换而来的脉冲神经网络（SNN）在连续控制任务中的表现不佳问题，其中小的动作误差会随着时间累积放大，导致性能严重下降。研究提出了一种无训练的机制——跨步残余膜电位初始化（CRPI），通过在决策步骤之间传递残余膜电位来抑制这些累积误差。实验表明，CRPI可以在具有向量和视觉观察的连续控制基准测试中有效恢复性能，突显了在连续控制任务中对SNN转换进行误差缓解的迫切需求。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2025-06-03T12:48:54+00:00 · Latest: 2026-01-29T13:56:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.03229v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.03229v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVA, and GPT-4V, leveraging these models to replace time-consuming manual annotation and enable annotation-free training has become a promising research direction. This paper studies learning from noisy partial labels generated by pre-trained VLMs and proposes a collaborative consistency regularization (Co-Reg) framework. Unlike symmetric noise commonly assumed in traditional noisy label learning, VLM-generated noise is instance-dependent and reflects the intrinsic biases of pre-trained models, posing greater challenges. To address this issue, we jointly train two neural networks to perform collaborative label purification via a co-pseudo-labeling mechanism, while enforcing consistency regularization in both label and feature representation spaces. In addition, multiple anti-overfitting strategies are introduced, including alternating optimization of contrastive representations and pseudo-labels, as well as maintaining class prototypes in a shared feature space. The proposed method can further incorporate few-shot manually annotated labels for performance enhancement. Extensive experiments under various settings demonstrate the effectiveness of our approach and highlight the potential of integrating weakly supervised learning into the knowledge distillation of pre-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>弱监督学习与VLM精炼的桥梁：基于预训练VLM的嘈杂部分标签学习以实现高效下游适应</div>
<div class="mono" style="margin-top:8px">在嘈杂部分标签学习（NPLL）的背景下，每个训练样本都与多个嘈杂注释者标注的一组候选标签相关联。随着高性能预训练视觉-语言模型（VLMs）如CLIP、LLaVA和GPT-4V的出现，利用这些模型替代耗时的手动标注并实现无标注训练已成为一个有前景的研究方向。本文研究了从预训练VLM生成的嘈杂部分标签中学习，并提出了一种协作一致性正则化（Co-Reg）框架。与传统嘈杂标签学习中假设的对称噪声不同，VLM生成的噪声是实例相关的，并反映了预训练模型的固有偏差，提出了更大的挑战。为了解决这一问题，我们联合训练两个神经网络，通过共伪标签机制进行协作标签净化，同时在标签和特征表示空间中施加一致性正则化。此外，还引入了多种抗过拟合策略，包括对比表示和伪标签的交替优化，以及在共享特征空间中保持类原型。所提出的方法还可以进一步结合少量的手动标注标签以提高性能。在各种设置下的广泛实验表明了我们方法的有效性，并突显了将弱监督学习整合到预训练模型的知识精炼中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses noisy partial label learning (NPLL) where each training sample has multiple candidate labels from noisy annotators. It proposes a collaborative consistency regularization (Co-Reg) framework to purify labels using pre-trained vision-language models (VLMs) like CLIP and LLaVA. The method involves training two neural networks collaboratively and enforcing consistency in both label and feature spaces, while also introducing anti-overfitting strategies. Experiments show the effectiveness of this approach in enhancing downstream adaptation with fewer manual annotations.</div>
<div class="mono" style="margin-top:8px">本文针对预训练视觉-语言模型（VLM）生成的嘈杂部分标签的学习挑战，提出了一种协作一致性正则化（Co-Reg）框架。该方法联合训练两个神经网络进行标签净化，并在标签和特征表示空间中施加一致性正则化。此外，还引入了防止过拟合的策略，并可以结合少量的手动标注标签以提高性能。实验结果表明，在各种设置下该方法的有效性，并突出了将弱监督学习整合到预训练模型的知识蒸馏中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling</div>
<div class="meta-line">Authors: Xinglin Wang, Jiayi Shi, Shaoxiong Feng, Peiwen Yuan, Yiwei Li, Yueqi Zhang, Chuyi Tan, Ji Zhang, Boyuan Pan, Yao Hu, Kan Li</div>
<div class="meta-line">First: 2026-01-29T13:18:36+00:00 · Latest: 2026-01-29T13:18:36+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不要浪费你的部署：回收搜索经验以实现高效的测试时扩展</div>
<div class="mono" style="margin-top:8px">测试时扩展通过分配额外的推理计算资源来扩展大型语言模型的推理能力，从而扩大解决方案空间的探索范围。然而，现有的搜索策略通常将部署视为一次性样本，其中在每次试验后有价值的中间见解实际上被丢弃。这种系统性的记忆缺失导致了巨大的计算冗余，因为模型在广泛的尝试中反复重新推导出已发现的结论并重新访问已知的死胡同。为了弥合这一差距，我们提出了**回收搜索经验（RSE）**，这是一种无需训练的自我引导策略，将测试时搜索从一系列孤立的试验转变为累积过程。通过积极地从原始轨迹中提炼出共享的经验库，RSE 使中间结论的正向回收能够缩短冗余推导，并使失败模式的负向回收能够修剪遇到的死胡同。理论上，我们提供了一种分析，正式化了RSE的效率增益，并验证了它在解决复杂推理任务时相对于独立采样的优势。实验上，在HMMT24、HMMT25、IMO-Bench和HLE上进行的大量实验表明，RSE 以与强基线相当的计算成本实现了最先进的扩展效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiency of existing test-time scaling methods for Large Language Models, which discard valuable intermediate insights after each trial. It introduces Recycling Search Experience (RSE), a self-guided strategy that recycles search experiences to reduce redundancy. Empirically, RSE outperforms strong baselines on various reasoning tasks with similar computational costs, demonstrating superior scaling efficiency.</div>
<div class="mono" style="margin-top:8px">论文针对现有大型语言模型测试时扩展方法存在的问题，即每次试验后丢弃有价值的信息，导致重复计算。提出了一种自引导的Recycling Search Experience (RSE)策略，通过积累和重复使用中间结论和失败模式来提高推理效率。实验结果显示，RSE在HMMT24、HMMT25、IMO-Bench和HLE上的表现优于强基线，且具有相似的计算成本，实现了最先进的扩展效率。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Visual Surrogate Compression for Alzheimer&#x27;s Disease Classification</div>
<div class="meta-line">Authors: Dexuan Ding, Ciyuan Peng, Endrowednes Kuantama, Jingcai Guo, Jia Wu, Jian Yang, Amin Beheshti, Ming-Hsuan Yang, Yuankai Qi</div>
<div class="meta-line">First: 2026-01-29T13:05:46+00:00 · Latest: 2026-01-29T13:05:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21673v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-dimensional structural MRI (sMRI) images are widely used for Alzheimer&#x27;s Disease (AD) diagnosis. Most existing methods for sMRI representation learning rely on 3D architectures (e.g., 3D CNNs), slice-wise feature extraction with late aggregation, or apply training-free feature extractions using 2D foundation models (e.g., DINO). However, these three paradigms suffer from high computational cost, loss of cross-slice relations, and limited ability to extract discriminative features, respectively. To address these challenges, we propose Multimodal Visual Surrogate Compression (MVSC). It learns to compress and adapt large 3D sMRI volumes into compact 2D features, termed as visual surrogates, which are better aligned with frozen 2D foundation models to extract powerful representations for final AD classification. MVSC has two key components: a Volume Context Encoder that captures global cross-slice context under textual guidance, and an Adaptive Slice Fusion module that aggregates slice-level information in a text-enhanced, patch-wise manner. Extensive experiments on three large-scale Alzheimer&#x27;s disease benchmarks demonstrate our MVSC performs favourably on both binary and multi-class classification tasks compared against state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阿尔茨海默病分类的多模态视觉代理压缩</div>
<div class="mono" style="margin-top:8px">高维结构磁共振成像（sMRI）图像广泛用于阿尔茨海默病（AD）诊断。现有的大多数sMRI表示学习方法依赖于3D架构（例如，3D CNNs）、切片级特征提取与后期聚合，或使用2D基础模型（例如，DINO）进行无训练特征提取。然而，这三种范式分别面临高计算成本、跨切片关系丢失和提取判别特征能力有限的问题。为解决这些挑战，我们提出了一种多模态视觉代理压缩（MVSC）。MVSC学习将大型3D sMRI体素压缩和适应为紧凑的2D特征，称为视觉代理，这些特征与冻结的2D基础模型更好地对齐，以提取用于最终AD分类的强大表示。MVSC有两个关键组件：一个体积上下文编码器，在文本引导下捕捉全局跨切片上下文，以及一个增强切片融合模块，在文本增强、块级方式下聚合切片级信息。在三个大规模阿尔茨海默病基准上的广泛实验表明，与最先进的方法相比，我们的MVSC在二分类和多分类任务中均表现优异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency and accuracy of Alzheimer&#x27;s Disease (AD) diagnosis using structural MRI (sMRI) images. MVSC is proposed to compress 3D sMRI volumes into 2D visual surrogates, which are better aligned with 2D foundation models for feature extraction. The method includes a Volume Context Encoder for capturing cross-slice context and an Adaptive Slice Fusion module for text-enhanced aggregation. Experiments show MVSC outperforms existing methods on both binary and multi-class AD classification tasks.</div>
<div class="mono" style="margin-top:8px">论文针对高维度结构MRI图像在阿尔茨海默病诊断中的挑战，如高计算成本和跨切片关系的丢失。提出了多模态视觉代理压缩（MVSC），将3D MRI体积压缩为2D视觉代理，以便更好地与2D基础模型对齐。MVSC包括体积上下文编码器和自适应切片融合模块。实验表明，MVSC在二分类和多分类任务上均优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching</div>
<div class="meta-line">Authors: Li Ju, Mayank Nautiyal, Andreas Hellander, Ekta Vats, Prashant Singh</div>
<div class="meta-line">First: 2026-01-29T12:58:42+00:00 · Latest: 2026-01-29T12:58:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21662v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are typically deterministic in nature and lack intrinsic mechanisms to quantify epistemic uncertainty, which reflects the model&#x27;s lack of knowledge or ignorance of its own representations. We theoretically motivate negative log-density of an embedding as a proxy for the epistemic uncertainty, where low-density regions signify model ignorance. The proposed method REPVLM computes the probability density on the hyperspherical manifold of the VLM embeddings using Riemannian Flow Matching. We empirically demonstrate that REPVLM achieves near-perfect correlation between uncertainty and prediction error, significantly outperforming existing baselines. Beyond classification, we also demonstrate that the model also provides a scalable metric for out-of-distribution detection and automated data curation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黎曼流匹配的预训练VLMs的 epistemic 不确定性量化</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通常具有确定性，缺乏内在机制来量化 epistemic 不确定性，这反映了模型对其自身表示的无知或知识不足。我们从理论上将嵌入的负对数密度作为 epistemic 不确定性的代理，其中低密度区域表示模型的无知。所提出的方法 REPVLM 使用黎曼流匹配计算 VLM 嵌入在超球面流形上的概率密度。我们实验证明，REPVLM 在不确定性与预测误差之间的相关性接近完美，显著优于现有基线。除了分类之外，我们还证明该模型还提供了一种可扩展的用于检测异常分布和自动数据整理的度量标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the lack of epistemic uncertainty quantification in Vision-Language Models (VLMs) by proposing REPVLM, which computes the probability density on the hyperspherical manifold of VLM embeddings using Riemannian Flow Matching. The method correlates uncertainty with prediction error effectively and outperforms existing baselines. Additionally, REPVLM is shown to be useful for out-of-distribution detection and data curation.</div>
<div class="mono" style="margin-top:8px">论文通过理论分析将负对数密度作为模型无知的代理，解决了Vision-Language模型（VLM）中缺乏表征不确定性量化的问题。方法REPVLM使用黎曼流匹配计算VLM嵌入在超球面流形上的概率密度。实验结果表明，REPVLM在不确定性与预测误差之间的相关性上接近完美，优于现有方法。此外，该模型还提供了一种可扩展的用于异常分布检测和自动数据整理的度量标准。</div>
</details>
</div>
<div class="card">
<div class="title">OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</div>
<div class="meta-line">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</div>
<div class="meta-line">First: 2026-01-29T12:43:02+00:00 · Latest: 2026-01-29T12:43:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21639v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCRVerse：端到端视觉语言模型中的全方位OCR</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型的发展推动了对管理和应用大量多模态数据的需求，使得从视觉图像中提取信息的OCR技术越来越受欢迎。然而，现有的OCR方法主要集中在识别图像或扫描文档中的文本元素（文本中心的OCR），忽视了从视觉信息密集型图像源（视觉中心的OCR）中识别视觉元素，如图表、网页和科学图表。实际上，这些视觉信息密集型图像在互联网上广泛存在，并具有重要的现实应用价值，如数据可视化和网页分析。在本技术报告中，我们提出了OCRVerse，这是一种端到端的全方位OCR方法，能够统一处理文本中心的OCR和视觉中心的OCR。为此，我们构建了全面的数据工程，涵盖了广泛的文本中心文档，如报纸、杂志和书籍，以及视觉中心的渲染组合，包括图表、网页和科学图表。此外，我们为OCRVerse提出了两阶段的SFT-RL多域训练方法。SFT直接混合跨域数据进行训练和建立初始领域知识，而RL则专注于为每个领域的特性设计个性化的奖励策略。具体来说，由于不同领域需要不同的输出格式和预期输出，我们在RL阶段提供了足够的灵活性，为每个领域定制灵活的奖励信号，从而提高跨域融合并避免数据冲突。实验结果表明，OCRVerse的有效性，其在文本中心和视觉中心数据类型上的表现与大规模开源和闭源模型相当甚至更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces OCRVerse, a holistic OCR method that integrates text-centric and vision-centric OCR in an end-to-end vision-language model. It addresses the limitation of existing OCR methods by constructing a comprehensive dataset and proposing a two-stage SFT-RL training method. The experimental results show that OCRVerse performs competitively across different types of data, even matching the performance of large-scale models.</div>
<div class="mono" style="margin-top:8px">OCRVerse 是一种端到端的综合 OCR 方法，能够同时处理文本中心和视觉中心的 OCR。它通过全面的数据工程和两阶段 SFT-RL 训练方法来处理各种视觉和文本数据。实验结果表明，OCRVerse 在不同类型的数据显示出竞争力，甚至能够与大规模开源和闭源模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization</div>
<div class="meta-line">Authors: Songhan Jiang, Fengchun Liu, Ziyue Wang, Linghan Cai, Yongbing Zhang</div>
<div class="meta-line">First: 2026-01-29T12:21:16+00:00 · Latest: 2026-01-29T12:21:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21617v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21617v1">PDF</a> · <a href="https://github.com/cyclexfy/PathReasoner-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PathReasoner-R1: 通过知识引导的策略优化将结构化推理融入病理视觉语言模型</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）正在推动计算病理学的发展，具备卓越的视觉理解能力。然而，当前系统往往直接输出结论而缺乏可验证的证据链推理，这严重限制了临床信任并阻碍了专家错误的纠正。为解决这些问题，我们构建了PathReasoner，这是首个大规模的全切片图像（WSI）推理数据集。不同于以往依赖未经验证的蒸馏工作，我们开发了一个严格的知识引导生成管道。通过利用医学知识图谱，我们明确地将结构化的病理发现和临床推理与诊断对齐，生成了超过20000个高质量的指导样本。基于该数据库，我们提出了PathReasoner-R1，该模型结合了轨迹掩蔽监督微调与推理导向的强化学习，以植入结构化的推理链能力。为了确保医学严谨性，我们设计了一个知识感知的多粒度奖励函数，其中包括严格与知识图谱对齐的实体奖励机制。这有效地引导模型优化逻辑一致性而非仅仅匹配结果，从而增强其鲁棒性。大量实验表明，PathReasoner-R1 在PathReasoner 和公共基准测试中均实现了最先进的性能，为病理模型提供了透明且临床相关的推理能力。数据集和代码可在 https://github.com/cyclexfy/PathReasoner-R1 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the reasoning capabilities of Vision-Language Models (VLMs) in computational pathology by providing verifiable evidence-linked reasoning. To achieve this, the authors developed PathReasoner, a large-scale dataset of whole-slide image reasoning, and PathReasoner-R1, which combines trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning. The model uses a knowledge-aware reward function to ensure logical consistency, leading to state-of-the-art performance on both PathReasoner and public benchmarks. This work equips pathology models with transparent, clinically grounded reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提供可验证的证据链推理来增强视觉-语言模型（VLM）在计算病理学中的推理能力。为此，作者开发了PathReasoner，这是一个大规模的全切片图像推理数据集，以及PathReasoner-R1，该模型结合了轨迹掩蔽监督微调与推理导向的强化学习。模型使用知识感知的奖励函数来确保逻辑一致性，从而在PathReasoner和公共基准测试上实现了最先进的性能。这种方法使病理模型具备透明且临床相关的推理能力，提高了临床信任和专家错误纠正的能力。</div>
</details>
</div>
<div class="card">
<div class="title">WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</div>
<div class="meta-line">Authors: Zijin Yang, Yu Sun, Kejiang Chen, Jiawei Zhao, Jun Jiang, Weiming Zhang, Nenghai Yu</div>
<div class="meta-line">First: 2026-01-29T12:14:32+00:00 · Latest: 2026-01-29T12:14:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21610v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WMVLM：通过视觉语言模型评估扩散模型图像水印</div>
<div class="mono" style="margin-top:8px">数字水印对于保护来自扩散模型的生成图像至关重要。准确的水印评估对于算法开发至关重要，但现有方法存在显著局限性：缺乏统一框架处理残差和语义水印，结果缺乏可解释性，忽视了全面的安全考虑，且经常使用不合适的语义水印度量标准。为解决这些差距，我们提出了WMVLM，这是首个通过视觉语言模型（VLMs）统一且可解释的扩散模型图像水印评估框架。我们重新定义了每种水印类型的质量和安全性度量标准：残差水印通过艺术强度和擦除抗性进行评估，而语义水印则通过潜在分布偏移进行评估。此外，我们引入了三阶段训练策略，逐步使模型实现分类、评分和可解释的文本生成。实验表明，WMVLM在不同数据集、扩散模型和水印方法上的泛化能力优于最先进的VLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a unified and interpretable framework for evaluating digital watermarks in images generated by diffusion models. WMVLM uses vision-language models to assess both residual and semantic watermarks by defining new quality and security metrics. The framework demonstrates superior performance and strong generalization across various datasets, diffusion models, and watermarking methods through experimental validation.</div>
<div class="mono" style="margin-top:8px">研究旨在利用视觉语言模型开发一种统一且可解释的图像水印评估框架。方法提出了针对残差和语义水印的新质量和安全性指标，并采用了三阶段训练策略。关键发现表明，WMVLM在各种数据集、扩散模型和水印技术上的泛化能力优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</div>
<div class="meta-line">Authors: Sangoh Lee, Sangwoo Mo, Wook-Shin Han</div>
<div class="meta-line">First: 2025-12-23T03:13:39+00:00 · Latest: 2026-01-29T12:02:16+00:00</div>
<div class="meta-line">Comments: Project page with videos and code: https://vap-project.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20014v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20014v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vap-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as &quot;bring my cup,&quot; where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bring My Cup！使用视觉注意提示个性化视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作（VLA）模型在通用指令上表现出色，但在处理个性化命令如“bring my cup”时却遇到困难，其中机器人必须在视觉上相似的对象中执行特定实例的操作。我们研究了操作个人物品的场景，在这种场景中，VLA 必须使用少量参考图像识别并控制训练期间未见过的用户特定对象。为了解决这一挑战，我们提出了视觉注意提示（VAP），这是一种简单而有效的无需训练的感知适配器，为冻结的VLA提供自上而下的选择性注意。VAP 将参考图像视为非参数视觉记忆，通过开放式词汇检测和基于嵌入的匹配将个人对象定位在场景中，然后通过突出显示对象并重写指令将这种定位作为视觉提示注入。我们构建了两个模拟基准 Personalized-SIMPLER 和 Personalized-VLABench，以及一个真实世界的桌面基准，以评估跨多个机器人和任务的个性化操作。实验表明，VAP 在成功率和正确对象操作方面始终优于通用策略和标记学习基线，有助于弥合语义理解和实例级控制之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of personalizing vision-language-action models to handle specific commands like &#x27;bring my cup,&#x27; where the robot must identify and manipulate a particular object among similar ones. The authors propose Visual Attentive Prompting (VAP), a training-free method that uses reference images as a visual memory and injects this information as a visual prompt to guide the robot&#x27;s actions. Experiments show that VAP improves success rates and correct-object manipulation compared to generic policies and token-learning baselines.</div>
<div class="mono" style="margin-top:8px">研究解决了VLA模型处理个性化命令如“拿我的杯子”的问题，其中机器人需要在相似物体中识别并操作特定物体。研究引入了Visual Attentive Prompting (VAP)，这是一种无需训练的方法，通过参考图像增强冻结的VLA模型，赋予其自上而下的选择性注意力。VAP在模拟和真实世界基准测试中提高了成功率和正确物体的操作，展示了其在语义理解和实例级控制之间的桥梁作用。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</div>
<div class="meta-line">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</div>
<div class="meta-line">First: 2026-01-29T12:01:53+00:00 · Latest: 2026-01-29T12:01:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21590v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21590v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model&#x27;s generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展的功率采样：通过分布锐化解锁LLMs无需训练的高效推理</div>
<div class="mono" style="margin-top:8px">强化学习（RL）后训练是提高大型语言模型（LLMs）推理性能的主要方法，但越来越多的证据表明，其收益主要来自分布锐化而非新能力的获取。最近的研究表明，使用马尔可夫链蒙特卡洛（MCMC）从LLMs的幂分布中采样可以恢复与RL后训练相当的性能，且无需依赖外部奖励；然而，MCMC的高计算成本使其在广泛应用中不可行。在本文中，我们提出了一种理论依据的方法，以消除迭代MCMC的需要。我们推导出一种新的公式，表明全局幂分布可以近似为一个按令牌缩放的低温分布，其中缩放因子捕捉未来轨迹的质量。利用这一洞察，我们引入了一种无需训练和验证者的算法，以自回归方式锐化基础模型的生成分布。实验上，我们在四个LLMs上对数学、问答和代码任务进行了评估，结果显示我们的方法在不依赖任何外部奖励的情况下，能够匹配或超越单次GRPO，同时将推理延迟降低超过10倍，相比基于MCMC的采样方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the high computational cost of using Markov chain Monte Carlo (MCMC) for distribution sharpening in large language models (LLMs), proposing a novel method that avoids iterative MCMC. The method approximates the global power distribution with a token-level scaled low-temperature distribution, which is used to sharpen the base model&#x27;s generative distribution. Experiments on math, QA, and code tasks across four LLMs demonstrate that this approach matches or outperforms one-shot GRPO without external rewards and reduces inference latency by over 10x compared to MCMC-based sampling.</div>
<div class="mono" style="margin-top:8px">本文旨在解决使用马尔可夫链蒙特卡洛（MCMC）进行大规模语言模型（LLM）分布锐化时的高计算成本问题，提出了一种无训练和无验证者的新型方法。通过将全局功率分布近似为一个按令牌缩放的低温度分布，该方法以自回归方式锐化生成分布。实验表明，该方法在数学、问答和代码任务上能够匹配或超越单次GRPO，且无需外部奖励，同时将推理延迟降低了超过10倍，相比基于MCMC的采样方法。</div>
</details>
</div>
<div class="card">
<div class="title">DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</div>
<div class="meta-line">Authors: Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</div>
<div class="meta-line">First: 2025-10-23T17:42:14+00:00 · Latest: 2026-01-29T11:37:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20766v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20766v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://noamissachar.github.io/DyPE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism&#x27;s quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model&#x27;s positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyPE：动态位置外推在超高清扩散中的应用</div>
<div class="mono" style="margin-top:8px">扩散变换器模型能够生成具有非凡保真度和细节的图像，但由于自注意力机制与图像标记数量的平方级扩展，训练它们在超高清分辨率下仍然极其昂贵。在本文中，我们提出了动态位置外推（DyPE），这是一种无需训练的新颖方法，使预训练的扩散变换器能够在其训练数据之外生成高得多分辨率的图像，且无需额外的采样成本。DyPE 利用了扩散过程中固有的频谱进展，其中低频结构早期收敛，高频则需要更多步骤来解决。具体而言，DyPE 在每次扩散步骤中动态调整模型的位置编码，使其频谱与生成过程的当前阶段相匹配。这种方法使我们能够在远超训练分辨率的分辨率下生成图像，例如，使用 FLUX 生成 16 百万像素的图像。在多个基准测试中，DyPE 一致地提高了性能，并在超高清图像生成中达到了最先进的保真度，尤其是在更高分辨率下，性能提升更为显著。项目页面可在 https://noamissachar.github.io/DyPE/ 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high computational cost of training diffusion models at ultra-high resolutions. DyPE, a training-free method, enables pre-trained diffusion transformers to generate images at resolutions far beyond their training data. By dynamically adjusting the model&#x27;s positional encoding, DyPE matches the frequency spectrum with the current stage of the generative process, allowing for the generation of images at resolutions up to 16 million pixels. The method consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, especially at higher resolutions.</div>
<div class="mono" style="margin-top:8px">DyPE 是一种无需训练的方法，通过在每个扩散步骤中动态调整模型的位置编码，使预训练的扩散变换器能够在超高清分辨率下生成图像。这种方法利用了扩散过程中的频谱进展，使模型的频谱与当前的生成阶段相匹配。DyPE 显著提高了性能，并在超高清分辨率图像生成中达到了最先进的保真度，尤其是在更高分辨率下效果更为显著，且无需额外的采样成本。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</div>
<div class="meta-line">Authors: Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</div>
<div class="meta-line">First: 2026-01-29T10:59:36+00:00 · Latest: 2026-01-29T10:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21542v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21542v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors&#x27;&#x27; and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双锚点内插求解器加速生成建模</div>
<div class="mono" style="margin-top:8px">流动匹配（FM）模型已成为高保真合成的领先范式。然而，它们依赖于迭代常微分方程（ODE）求解，这造成了显著的延迟瓶颈。现有解决方案面临两难境地：无训练求解器在低神经网络评估次数（NFEs）时性能严重下降，而基于训练的一或几步生成方法则会带来高昂的训练成本，并缺乏即插即用的灵活性。为弥合这一差距，我们提出了双锚点内插求解器（BA-求解器）。BA-求解器保留了标准无训练求解器的灵活性，同时通过引入一个轻量级的SideNet（1-2%主干网络大小）与冻结的主干网络结合，实现了显著加速。具体而言，我们的方法基于两个协同组件：1）双向时间感知，其中SideNet学习在不重新训练重的主干网络的情况下，近似未来和历史速度；2）双锚点速度集成，利用SideNet与两个锚点速度来高效近似批量高阶积分的中间速度。通过利用主干网络建立高精度的“锚点”，并利用SideNet细化轨迹，BA-求解器能够在最小化误差的情况下使用大时间间隔。在ImageNet-256²上的实验结果表明，BA-求解器仅在10次NFEs时就能达到与100+ NFEs欧拉求解器相当的生成质量，并且在仅5次NFEs时仍能保持高保真度，且几乎不产生训练成本。此外，BA-求解器确保了与现有生成管道的无缝集成，便于下游任务如图像编辑的实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the latency issue in Flow Matching (FM) models by proposing the Bi-Anchor Interpolation Solver (BA-solver). BA-solver combines a lightweight SideNet with a frozen backbone to achieve significant acceleration. The method uses bidirectional temporal perception to approximate velocities and bi-anchor velocity integration to efficiently compute intermediate velocities. Experimental results on ImageNet-256^2 show that BA-solver can match the quality of a 100+ NFEs Euler solver with only 10 NFEs and maintain high fidelity with as few as 5 NFEs, without incurring significant training costs. Additionally, BA-solver is plug-and-play, enabling seamless integration with existing generative pipelines for tasks like image editing.</div>
<div class="mono" style="margin-top:8px">论文提出了一种双向锚点插值求解器（BA-solver），以解决Flow Matching (FM)模型中的延迟问题。该方法结合了一个轻量级的SideNet和一个冻结的主干网络，以实现显著的加速。BA-solver利用双向时间感知来近似未来和历史速度，并利用双锚点速度集成来高效地近似中间速度。实验结果表明，BA-solver可以在仅10个NFEs的情况下达到与100+ NFEs欧拉求解器相当的生成质量，并且在最少5个NFEs时仍能保持高保真度，且训练成本可以忽略不计，并且能够无缝集成到现有的生成性流水线中，支持诸如图像编辑等下游任务。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression</div>
<div class="meta-line">Authors: Xinwei Zhang, Hangcheng Liu, Li Bai, Hao Wang, Qingqing Ye, Tianwei Zhang, Haibo Hu</div>
<div class="meta-line">First: 2026-01-29T10:47:21+00:00 · Latest: 2026-01-29T10:47:21+00:00</div>
<div class="meta-line">Comments: Under Review, 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21531v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21531v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模视觉-语言模型在视觉标记压缩下的对抗鲁棒性研究</div>
<div class="mono" style="margin-top:8px">视觉标记压缩被广泛用于通过剪枝或合并视觉标记来加速大规模视觉-语言模型（LVLMs），但其对抗鲁棒性尚未被探索。我们表明，现有的基于编码器的攻击会显著高估压缩LVLMs的鲁棒性，这是因为优化与推理之间的不匹配：扰动是在完整标记表示上进行优化，而推理则是通过标记压缩瓶颈进行的。为了解决这一差距，我们提出了压缩对齐攻击（CAGE），该攻击在不假设访问部署的压缩机制或其标记预算的情况下，将扰动优化与压缩推理对齐。CAGE 结合了（i）期望特征破坏，将扰动集中在那些在可能的预算范围内可能存活的标记上，以及（ii）排名失真对齐，主动将标记扰动与排名分数对齐，以促进高失真证据的保留。在多种代表性的插即用压缩机制和数据集上，我们的结果表明，CAGE 一致地实现了比基线更低的鲁棒准确性。这项工作强调了忽略压缩的鲁棒性评估可能会过于乐观，呼吁对高效LVLMs进行压缩感知的安全评估和防御。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the adversarial robustness of large vision-language models (LVLMs) under visual token compression, addressing the mismatch between perturbation optimization and inference through a compression bottleneck. It introduces the Compression-AliGnEd (CAGE) attack, which aligns perturbation optimization with compression inference. Experiments across various compression mechanisms and datasets demonstrate that CAGE consistently achieves lower robust accuracy than the baseline, indicating that robustness assessments without considering compression can be overly optimistic.</div>
<div class="mono" style="margin-top:8px">研究探讨了视觉语言模型（LVLMs）在视觉标记压缩下的对抗鲁棒性，解决了优化扰动与通过压缩瓶颈进行推理之间的不匹配问题。引入了压缩对齐攻击（CAGE），使扰动优化与压缩推理对齐。实验表明，CAGE在各种压缩机制和数据集上的一致性鲁棒准确率低于基线，表明在不考虑压缩的情况下进行鲁棒性评估可能会过于乐观。</div>
</details>
</div>
<div class="card">
<div class="title">ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment</div>
<div class="meta-line">Authors: Xiuyu Li, Jinkai Zhang, Mingyang Yi, Yu Li, Longqiang Wang, Yue Wang, Ju Fan</div>
<div class="meta-line">First: 2026-01-29T10:06:52+00:00 · Latest: 2026-01-29T10:06:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21484v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ETS：能量引导的测试时缩放以实现无需训练的RL对齐</div>
<div class="mono" style="margin-top:8px">语言模型的强化学习（RL）后训练对齐在实践中有效，但代价高昂且不稳定，因为其复杂的训练过程。为了解决这个问题，我们提出了一种无需训练的推理方法，可以直接从最优RL策略中采样。应用于掩码语言模型（MLM）的转换概率包括一个参考策略模型和一个能量项。在此基础上，我们的算法能量引导的测试时缩放（ETS）通过在线蒙特卡洛估计关键的能量项，具有可证明的收敛速率。此外，为了确保实际效率，ETS 利用现代加速框架和定制的重要性采样估计器，大幅减少了推理延迟，同时可证明地保持了采样质量。在包括自回归模型和扩散语言模型的MLM（涵盖推理、编码和科学基准）实验中，我们的ETS始终提高了生成质量，验证了其有效性和设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the cost and instability issues of RL post-training alignment for language models by proposing a training-free inference method called Energy-Guided Test-Time Scaling (ETS). ETS uses a reference policy model and an energy term to estimate the transition probability for Masked Language Modeling, with convergence guaranteed. It also employs modern acceleration frameworks and importance sampling estimators to reduce inference latency while maintaining sampling quality. Experiments show that ETS improves generation quality across various benchmarks, validating its effectiveness.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出一种训练-free 推断方法——能量引导的测试时缩放（ETS），解决语言模型的 RL 后训练对齐成本高和不稳定的问题。ETS 使用参考策略模型和能量项来估计 Masked Language Modeling 的转移概率，并通过在线蒙特卡洛实现收敛，使用重要性采样估计器提高效率。实验表明，ETS 能够在各种基准上提高生成质量，验证了其有效性和设计。</div>
</details>
</div>
<div class="card">
<div class="title">NOSA: Native and Offloadable Sparse Attention</div>
<div class="meta-line">Authors: Yuxiang Huang, Pengjie Wang, Jicheng Han, Weilin Zhao, Zhou Su, Ao Sun, Hongya Lyu, Hengyu Zhao, Yudong Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu</div>
<div class="meta-line">First: 2025-10-15T14:33:16+00:00 · Latest: 2026-01-29T08:26:24+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.13602v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.13602v2">PDF</a> · <a href="https://github.com/thunlp/NOSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decoding throughput improvements from larger inference batches are limited by GPU memory, which is largely consumed by the key-value (KV) cache. Prior training-free KV cache offloading alleviates this by keeping redundant context on the CPU and fetching only a sparse subset for attention, but it often degrades long-generation quality due to training-inference mismatch on sparse patterns. Meanwhile, trainable sparse attention is incompatible with efficient offloading, as unconstrained KV accesses may force large CPU-to-GPU transfers and erase throughput gains. To this end, we propose NOSA, a trainable sparse attention mechanism natively designed for KV cache offloading. NOSA explicitly constrains the volume of CPU-GPU KV transfers, thereby achieving low communication overhead and high decoding throughput. We further build NOSI, a KV cache offloading inference system that fully unlocks NOSA&#x27;s efficiency. Empirical results on 1,3,8B LLMs demonstrate that NOSA outperforms KV cache offloading baselines on general, long-input, and long-generation tasks, while boosting decoding throughput by up to 5.04x, 1.92x, and 1.83x over FullAttn, InfLLMv2, and ShadowKV, respectively. We release our code at https://github.com/thunlp/NOSA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NOSA：原生可卸载的稀疏注意机制</div>
<div class="mono" style="margin-top:8px">从更大推理批次中获得的解码吞吐量提升受限于GPU内存，大部分被键值（KV）缓存消耗。先前的无训练KV缓存卸载通过在CPU上保留冗余上下文并仅获取稀疏子集来进行注意，从而缓解了这一问题，但往往会因稀疏模式的训练-推理不匹配而降低长生成质量。同时，可训练的稀疏注意机制与高效的卸载不兼容，因为不受约束的KV访问可能会强制进行大量CPU到GPU的数据传输，从而消除吞吐量的提升。为此，我们提出NOSA，一种原生设计用于KV缓存卸载的可训练稀疏注意机制。NOSA明确限制了CPU-GPU KV传输的体积，从而实现低通信开销和高解码吞吐量。我们进一步构建了NOSI，一个KV缓存卸载推理系统，完全释放了NOSA的效率。在1,3,8B大语言模型上的实验结果表明，NOSA在通用、长输入和长生成任务上优于KV缓存卸载基线，分别将解码吞吐量提升至FullAttn的5.04倍、InfLLMv2的1.92倍和ShadowKV的1.83倍。我们已在https://github.com/thunlp/NOSA/发布了我们的代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">NOSA is a trainable sparse attention mechanism designed for efficient key-value cache offloading, addressing the limitations of previous methods by constraining CPU-GPU data transfers and maintaining high decoding throughput. Experiments on 1.3B, 3B, and 8B language models show that NOSA outperforms existing baselines, achieving up to 5.04x, 1.92x, and 1.83x higher decoding throughput compared to FullAttn, InfLLMv2, and ShadowKV, respectively.</div>
<div class="mono" style="margin-top:8px">NOSA 是一种可训练的稀疏注意力机制，旨在高效地进行键值缓存卸载，通过限制 CPU-GPU 数据传输来解决先前方法的局限性，从而保持高解码吞吐量。实验结果表明，NOSA 在 1B、3B 和 8B 大型语言模型上的表现优于现有基线，分别比 FullAttn、InfLLMv2 和 ShadowKV 提高了 5.04 倍、1.92 倍和 1.83 倍的解码吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</div>
<div class="meta-line">Authors: Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi</div>
<div class="meta-line">First: 2025-10-08T16:46:57+00:00 · Latest: 2026-01-29T07:52:53+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026 (Main). Our code will be available at: https://github.com/ku-nlp/language-specific-dimensions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07213v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07213v2">PDF</a> · <a href="https://github.com/ku-nlp/language-specific-dimensions">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言存在于稀疏维度中：面向可解释和高效的多语言控制的大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型在有限的非英语数据暴露下表现出强大的多语言能力。先前的研究表明，以英语为中心的大语言模型在中间层将多语言内容映射到英语对齐的表示，然后在最终层将它们投影回目标语言的标记空间。从这一观察出发，我们假设这种跨语言过渡是由一组小而稀疏的维度控制的，这些维度在中间层到最终层的一致索引中出现。基于这一见解，我们提出了一种简单的、无需训练的方法来识别和操作这些维度，只需要少量（最多50句）平行或单语数据。在多语言生成控制任务上的实验揭示了这些维度的可解释性，表明在这些维度上的干预可以切换输出语言同时保留语义内容，并且在较低的成本下超过了先前基于神经元的方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the sparse dimensions in large language models that govern multilingual content representation. It proposes a training-free method to identify and manipulate these dimensions using as few as 50 sentences of data, which can switch output languages while preserving semantic content. Experiments show this method outperforms previous neuron-based approaches with lower costs.</div>
<div class="mono" style="margin-top:8px">研究探讨了大型语言模型中稀疏的维度如何控制多语言内容表示。研究识别了一组在各层中一致存在的小维度，可以通过少量数据操纵这些维度来控制语言输出。实验表明，在这些维度上的干预可以切换输出语言同时保持语义内容，并且在计算成本更低的情况下优于先前的神经元基方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</div>
<div class="meta-line">Authors: Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao</div>
<div class="meta-line">First: 2025-12-31T19:00:03+00:00 · Latest: 2026-01-29T06:04:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00065v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00065v2">PDF</a> · <a href="https://github.com/xz-liu/tokenforge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The open-weight language model ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single breaker token that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack sabotages the base model&#x27;s generation while leaving the donor&#x27;s utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and evades outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>词汇中的特洛伊木马：LLM 组合中的隐蔽破坏</div>
<div class="mono" style="margin-top:8px">开放权重语言模型生态系统越来越多地通过模型组合技术（如权重合并、推测解码和词汇扩展）来重新混搭来自不同来源的能力。在这些方法能够在不同模型家族之间应用之前，一个关键的前提是分词器移植，它将不兼容的词汇表对齐到共享的嵌入空间。我们证明了这一关键的互操作性步骤引入了一个供应链漏洞：我们设计了一个单一的破坏性标记，在捐赠模型中功能中立，但在移植到基础模型后可靠地重建为一个高相关性的恶意特征。通过利用系数重用的几何特性，我们的攻击破坏了基础模型的生成能力，同时让捐赠模型的功能统计上与正常行为无显著差异。我们将此问题形式化为一个双目标优化问题，并使用稀疏求解器实例化了攻击。实验表明，该攻击无需训练即可逃避异常检测，并且能够抵抗微调和权重合并的结构性持久性，突显了模块化人工智能组合管道中的隐藏风险。代码可在 https://github.com/xz-liu/tokenforge 获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security risks in model composition techniques for language models, focusing on the vulnerability introduced by tokenizer transplant. The method involves engineering a single breaker token that is harmless in the donor model but becomes a high-salience malicious feature in the base model after transplant. The key finding is that this attack can sabotage the base model&#x27;s generation without affecting the donor model&#x27;s performance, and it remains effective against fine-tuning and weight merging, indicating a hidden risk in modular AI composition processes.</div>
<div class="mono" style="margin-top:8px">论文探讨了语言模型生态系统中，由于tokenizer移植这一必要步骤引入的供应链风险。通过设计一个在供体模型中无害但在基础模型中成为高相关性恶意特征的单一破坏性标记，作者展示了这种隐蔽的破坏方法。该攻击破坏了基础模型的生成能力，但不会影响供体模型的性能，使其能够抵御微调和权重合并，从而揭示了模块化AI组合技术中的隐藏风险。攻击代码可在GitHub上获得。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
